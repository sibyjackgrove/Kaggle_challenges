{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LANL_Earthquake_Challenge_DNN_1D-CNN_LSTM_with_TPU.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sibyjackgrove/Kaggle_challenges/blob/master/LANL_Earthquake_Challenge_DNN_1D_CNN_LSTM_with_TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "PwAdMj-gZ_Gu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Notebook Content\n",
        "- Earthquake prediction background & helpful resources\n",
        "- Step 1 - Installing dependencies\n",
        "- Step 2 - Importing dataset\n",
        "- Step 3 - Exploratory data analysis\n",
        "- Step 4 - Feature engineering (statistical features added) and creating training datasets\n",
        "- Step 5 - Create and train models\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "uT-B0AyTbFVY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 1 - Install & Import Dependencies"
      ]
    },
    {
      "metadata": {
        "id": "QBzPAcuLoaew",
        "colab_type": "code",
        "outputId": "6f9f44ac-46c2-46b1-8bcc-71e510bb2e68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "cell_type": "code",
      "source": [
        "#to access kaggle datasets\n",
        "!pip install kaggle\n",
        "!pip install numpy==1.15.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2018.11.29)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.0.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Requirement already satisfied: Unidecode>=0.04.16 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.0.23)\n",
            "Requirement already satisfied: numpy==1.15.0 in /usr/local/lib/python3.6/dist-packages (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "reHWaDX-pGlx",
        "colab_type": "code",
        "outputId": "fbfaa6ee-6161-4cca-ba0e-a6defdea66d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "#Utilities\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "\n",
        "#data preprocessing\n",
        "import pandas as pd\n",
        "#math operations\n",
        "import numpy as np\n",
        "\n",
        "#data scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Widgets (if the work)\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interact_manual\n",
        "from IPython.display import display\n",
        "\n",
        "#TensorFlow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,BatchNormalization,Conv1D,MaxPooling1D,GlobalAveragePooling1D,Dropout, LSTM\n",
        "import tensorflow as tf\n",
        "print(tf.test.gpu_device_name())\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "1.13.0-rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MciLY7yq9ITC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "8325f890-2f5e-4e54-d948-f905ab3b1200"
      },
      "cell_type": "code",
      "source": [
        "IS_COLAB_BACKEND = 'COLAB_GPU' in os.environ  # this is always set on Colab, the value is 0 or 1 depending on GPU presence\n",
        "if IS_COLAB_BACKEND:\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user() # Authenticates the backend and also the TPU using your credentials so that they can access your private GCS buckets"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qhdz68Xm3Z4Z",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title visualization utilities [RUN ME]\n",
        "\"\"\"\n",
        "This cell contains helper functions used for visualization\n",
        "and downloads only. You can skip reading it. There is very\n",
        "little useful Keras/Tensorflow code here.\n",
        "\"\"\"\n",
        "\n",
        "# Matplotlib config\n",
        "plt.rc('image', cmap='gray_r')\n",
        "plt.rc('grid', linewidth=0)\n",
        "plt.rc('xtick', top=False, bottom=False, labelsize='large')\n",
        "plt.rc('ytick', left=False, right=False, labelsize='large')\n",
        "plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')\n",
        "plt.rc('text', color='a8151a')\n",
        "plt.rc('figure', facecolor='F0F0F0')# Matplotlib fonts\n",
        "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), \"mpl-data/fonts/ttf\")\n",
        "\n",
        "\n",
        "    \n",
        "# utility to display training and validation curves\n",
        "def display_training_curves(training, validation, title, subplot):\n",
        "  if subplot%10==1: # set up the subplots on the first call\n",
        "    plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
        "    plt.tight_layout()\n",
        "  ax = plt.subplot(subplot)\n",
        "  ax.grid(linewidth=1, color='white')\n",
        "  ax.plot(training)\n",
        "  ax.plot(validation)\n",
        "  ax.set_title('model '+ title)\n",
        "  ax.set_ylabel(title)\n",
        "  ax.set_xlabel('epoch')\n",
        "  ax.legend(['train', 'valid.'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QMtu1W7ebhRD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 2 - Import Dataset from Kaggle"
      ]
    },
    {
      "metadata": {
        "id": "KvuWAvb9obkg",
        "colab_type": "code",
        "outputId": "b4929ae7-711b-4b97-b6d0-5767625bc658",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "# Colab's file access feature\n",
        "from google.colab import files\n",
        "\n",
        "#retrieve uploaded file\n",
        "uploaded = files.upload()\n",
        "\n",
        "#print results\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ef698f5c-d92b-4d5c-ba9b-97b16bdc4e78\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-ef698f5c-d92b-4d5c-ba9b-97b16bdc4e78\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "User uploaded file \"kaggle.json\" with length 70 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sGcveM2up-04",
        "colab_type": "code",
        "outputId": "6663c14e-9186-4a21-97bc-27d42081c3d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "cell_type": "code",
      "source": [
        "#list competitions\n",
        "!kaggle competitions list"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ref                                            deadline             category            reward  teamCount  userHasEntered  \n",
            "---------------------------------------------  -------------------  ---------------  ---------  ---------  --------------  \n",
            "digit-recognizer                               2030-01-01 00:00:00  Getting Started  Knowledge       2506           False  \n",
            "titanic                                        2030-01-01 00:00:00  Getting Started  Knowledge       9951           False  \n",
            "house-prices-advanced-regression-techniques    2030-01-01 00:00:00  Getting Started  Knowledge       4081           False  \n",
            "imagenet-object-localization-challenge         2029-12-31 07:00:00  Research         Knowledge         35           False  \n",
            "competitive-data-science-predict-future-sales  2019-12-31 23:59:00  Playground           Kudos       2396           False  \n",
            "two-sigma-financial-news                       2019-07-15 23:59:00  Featured          $100,000       2927           False  \n",
            "LANL-Earthquake-Prediction                     2019-06-03 23:59:00  Research           $50,000       1331            True  \n",
            "tmdb-box-office-prediction                     2019-05-30 23:59:00  Playground       Knowledge        273           False  \n",
            "dont-overfit-ii                                2019-05-07 23:59:00  Playground            Swag        770           False  \n",
            "gendered-pronoun-resolution                    2019-04-22 23:59:00  Research           $25,000        247           False  \n",
            "santander-customer-transaction-prediction      2019-04-10 23:59:00  Featured           $65,000       2440           False  \n",
            "womens-machine-learning-competition-2019       2019-04-09 23:59:00  Featured           $25,000         90           False  \n",
            "mens-machine-learning-competition-2019         2019-04-08 23:59:00  Featured           $25,000        160           False  \n",
            "histopathologic-cancer-detection               2019-03-30 23:59:00  Playground       Knowledge        722           False  \n",
            "petfinder-adoption-prediction                  2019-03-28 23:59:00  Featured           $25,000       1307           False  \n",
            "vsb-power-line-fault-detection                 2019-03-21 23:59:00  Featured           $25,000       1048           False  \n",
            "microsoft-malware-prediction                   2019-03-13 23:59:00  Research           $25,000       1935           False  \n",
            "humpback-whale-identification                  2019-02-28 23:59:00  Featured           $25,000       2090           False  \n",
            "elo-merchant-category-recommendation           2019-02-26 23:59:00  Featured           $50,000       4174           False  \n",
            "ga-customer-revenue-prediction                 2019-02-21 20:04:00  Featured           $45,000       3611           False  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zHB6DdwrqAav",
        "colab_type": "code",
        "outputId": "32f7b140-48c7-45ec-dd22-1466b0d73c73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "cell_type": "code",
      "source": [
        "#download earthquake data, will take 30-60 seconds (will only work after accepting competition rules)\n",
        "!kaggle competitions download -c LANL-Earthquake-Prediction"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading sample_submission.csv to /content\n",
            "\r  0% 0.00/33.3k [00:00<?, ?B/s]\n",
            "100% 33.3k/33.3k [00:00<00:00, 28.5MB/s]\n",
            "Downloading test.zip to /content\n",
            " 97% 234M/242M [00:01<00:00, 147MB/s]\n",
            "100% 242M/242M [00:01<00:00, 135MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            "100% 2.02G/2.03G [00:36<00:00, 40.1MB/s]\n",
            "100% 2.03G/2.03G [00:36<00:00, 59.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oxOrB-axqCOw",
        "colab_type": "code",
        "outputId": "d3e3e28a-4f59-4b56-eb31-f1c54402e604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "#unzip training data for usage, will take about 5 minutes (its big)\n",
        "!ls\n",
        "!unzip train.csv.zip\n",
        "!ls"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  sample_data  sample_submission.csv  test.zip\ttrain.csv.zip\n",
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n",
            "adc.json     sample_submission.csv  train.csv\n",
            "sample_data  test.zip\t\t    train.csv.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "afvCmHVDcPe-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 3 - Exploratory Data Analysis"
      ]
    },
    {
      "metadata": {
        "id": "IPTChUFWpJKY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1924
        },
        "outputId": "e4ad8a5a-1a06-44f1-9280-846b8a8d72f6"
      },
      "cell_type": "code",
      "source": [
        "#Extract training data into a dataframe for further manipulation\n",
        "train = pd.read_csv('train.csv', nrows=4095*100, dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})#\n",
        "print('Data frame shape:{}'.format(train.shape))\n",
        "#print first n entries\n",
        "train.head(4100)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data frame shape:(409500, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acoustic_data</th>\n",
              "      <th>time_to_failure</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>8</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>9</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-5</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>5</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>3</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-1</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>5</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>6</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>4</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>5</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>4</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>6</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>7</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>7</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>8</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>14</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>9</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>4</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>7</td>\n",
              "      <td>1.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4070</th>\n",
              "      <td>-11</td>\n",
              "      <td>1.469096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4071</th>\n",
              "      <td>-9</td>\n",
              "      <td>1.469096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4072</th>\n",
              "      <td>-3</td>\n",
              "      <td>1.469096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4073</th>\n",
              "      <td>-1</td>\n",
              "      <td>1.469096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4074</th>\n",
              "      <td>10</td>\n",
              "      <td>1.469096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4075</th>\n",
              "      <td>19</td>\n",
              "      <td>1.469096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4076</th>\n",
              "      <td>29</td>\n",
              "      <td>1.469095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4077</th>\n",
              "      <td>34</td>\n",
              "      <td>1.469095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4078</th>\n",
              "      <td>35</td>\n",
              "      <td>1.469095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4079</th>\n",
              "      <td>31</td>\n",
              "      <td>1.469095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4080</th>\n",
              "      <td>19</td>\n",
              "      <td>1.469095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4081</th>\n",
              "      <td>8</td>\n",
              "      <td>1.469095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4082</th>\n",
              "      <td>-9</td>\n",
              "      <td>1.469095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4083</th>\n",
              "      <td>-16</td>\n",
              "      <td>1.469095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4084</th>\n",
              "      <td>-20</td>\n",
              "      <td>1.469095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4085</th>\n",
              "      <td>-22</td>\n",
              "      <td>1.469095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4086</th>\n",
              "      <td>-16</td>\n",
              "      <td>1.469095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4087</th>\n",
              "      <td>-7</td>\n",
              "      <td>1.469095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4088</th>\n",
              "      <td>3</td>\n",
              "      <td>1.469095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4089</th>\n",
              "      <td>7</td>\n",
              "      <td>1.469095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4090</th>\n",
              "      <td>10</td>\n",
              "      <td>1.469095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4091</th>\n",
              "      <td>12</td>\n",
              "      <td>1.469095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4092</th>\n",
              "      <td>8</td>\n",
              "      <td>1.469095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4093</th>\n",
              "      <td>6</td>\n",
              "      <td>1.469095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4094</th>\n",
              "      <td>3</td>\n",
              "      <td>1.469095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4095</th>\n",
              "      <td>-13</td>\n",
              "      <td>1.468100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4096</th>\n",
              "      <td>-15</td>\n",
              "      <td>1.468100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4097</th>\n",
              "      <td>-7</td>\n",
              "      <td>1.468100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4098</th>\n",
              "      <td>-7</td>\n",
              "      <td>1.468100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4099</th>\n",
              "      <td>-3</td>\n",
              "      <td>1.468100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4100 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      acoustic_data  time_to_failure\n",
              "0                12         1.469100\n",
              "1                 6         1.469100\n",
              "2                 8         1.469100\n",
              "3                 5         1.469100\n",
              "4                 8         1.469100\n",
              "5                 8         1.469100\n",
              "6                 9         1.469100\n",
              "7                 7         1.469100\n",
              "8                -5         1.469100\n",
              "9                 3         1.469100\n",
              "10                5         1.469100\n",
              "11                2         1.469100\n",
              "12                2         1.469100\n",
              "13                3         1.469100\n",
              "14               -1         1.469100\n",
              "15                5         1.469100\n",
              "16                6         1.469100\n",
              "17                4         1.469100\n",
              "18                3         1.469100\n",
              "19                5         1.469100\n",
              "20                4         1.469100\n",
              "21                2         1.469100\n",
              "22                6         1.469100\n",
              "23                7         1.469100\n",
              "24                7         1.469100\n",
              "25                8         1.469100\n",
              "26               14         1.469100\n",
              "27                9         1.469100\n",
              "28                4         1.469100\n",
              "29                7         1.469100\n",
              "...             ...              ...\n",
              "4070            -11         1.469096\n",
              "4071             -9         1.469096\n",
              "4072             -3         1.469096\n",
              "4073             -1         1.469096\n",
              "4074             10         1.469096\n",
              "4075             19         1.469096\n",
              "4076             29         1.469095\n",
              "4077             34         1.469095\n",
              "4078             35         1.469095\n",
              "4079             31         1.469095\n",
              "4080             19         1.469095\n",
              "4081              8         1.469095\n",
              "4082             -9         1.469095\n",
              "4083            -16         1.469095\n",
              "4084            -20         1.469095\n",
              "4085            -22         1.469095\n",
              "4086            -16         1.469095\n",
              "4087             -7         1.469095\n",
              "4088              3         1.469095\n",
              "4089              7         1.469095\n",
              "4090             10         1.469095\n",
              "4091             12         1.469095\n",
              "4092              8         1.469095\n",
              "4093              6         1.469095\n",
              "4094              3         1.469095\n",
              "4095            -13         1.468100\n",
              "4096            -15         1.468100\n",
              "4097             -7         1.468100\n",
              "4098             -7         1.468100\n",
              "4099             -3         1.468100\n",
              "\n",
              "[4100 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "YBkgDQ0by_Q9",
        "colab_type": "code",
        "outputId": "269b7b3e-0362-4d4e-b6a9-f73e8266722f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "#visualize 1% of samples data, every 100th datapoint\n",
        "n_samples = 4096*25+1\n",
        "n_step = 4096\n",
        "#train_ad_sample_df = train['acoustic_data'].values[::n_step]\n",
        "#train_ttf_sample_df = train['time_to_failure'].values[::n_step]\n",
        "train_ad_sample_df = train['acoustic_data'].values[:n_samples]\n",
        "train_ttf_sample_df = train['time_to_failure'].values[:n_samples]\n",
        "\n",
        "#function for plotting based on both features\n",
        "def plot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df, title=\"Acoustic data and time to failure: First n sampled data\"):\n",
        "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
        "    plt.title(title)\n",
        "    plt.plot(train_ad_sample_df, color='r')\n",
        "    ax1.set_ylabel('acoustic data', color='r')\n",
        "    plt.legend(['acoustic data'], loc=(0.01, 0.95))\n",
        "    ax2 = ax1.twinx()\n",
        "    plt.plot(train_ttf_sample_df, color='b')\n",
        "    ax2.set_ylabel('time to failure', color='b')\n",
        "    plt.legend(['time to failure'], loc=(0.01, 0.9))\n",
        "    plt.grid(True)\n",
        "\n",
        "plot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df)\n",
        "#del train_ad_sample_df\n",
        "#del train_ttf_sample_df"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxoAAAHnCAYAAAA/0yu8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3XlcFPX/B/DXzB4siCACKohXHnmh\niEaeSWqat6lpeZSaRfbTxDTv1K6vipbiUWlmpaZGZlaaKZjmkSd44oG3oiCHLKfAsru/P5AF5Mbd\nnV14PR8PHszOfGbmPcvsMu/5HCOo1Wo9iIiIiIiIjEiUOgAiIiIiIqp4mGgQEREREZHRMdEgIiIi\nIiKjY6JBRERERERGx0SDiIiIiIiMjokGEREREREZHRMNIiq1k2PG49Lni4yyrasrVuFwnwFG2ZYU\nIn/9DcGt25a6/N4WXniwN8SEERmPNj0dJ8eMx15Pb9z/c1eJ5S/MmYfT770PoOzvixTyxkv5HfDt\ngZvr1ptt/afdHxFZNrnUARARkHrrFg717AtHz5bo8OvPUodjkHTxEjJiYuDq2xUA8NwP6ySLJfLX\n3+DSuRNUNWtIFkNZPAgOQZX69WHfuBEAoGf4GbPtOy3yHtRhp+E+oF+51o87fATxR4+h64Fg2Lq5\nlVi+5eeflGs/phB//AROjhoDUakssKxa2zbw2fD9U8drbediRaFJSkLUn7tQd+TrUodCRKXEGg0i\nC3B36y+o+VIPJF+JQNKly1KHYxC57VfE/ntI6jCg12px+X+LkRETI3UopXZ1+UqkXLsuyb4f7NmL\nqFLURBQlKykZMpVNqZIMS+V7aD96hp/J9+Oz4fun3q41nosVRfx/x3Dnpy1Sh0FEZcBEg0hiusxM\n3Nu+Ax7Dh8LVtysif/6lQJno3XtwuM8ABLdqi8N9ByLmnwOGZak3b+HUuHew77kOCGnjg9PvvY/0\nB9kXQfHHT+Dvxs2R+TDBUP7muvU44Nsj9/V33+PfF3tir6c3DnTphquBK6HX63Fhzjzc2bQFd7f8\nbGgKc3zkm7j48WeGde/8tAUHu/dCcOu2ODpkOBJOF33X/t5vvxvKnpn8AbRpaQWO8Uj/VxDs1Q77\nO3XF5YWLoddqAQB7Pb2RlZSEY8NHInzex4ZjOzr0NYS08cE/7bvg/IzZyHpim3kVVz4t8h7+btwc\ncUf+w9FXX0dwq7Y41KsfHp48ZVg/7tARHHq5H4JbtcXJN99CZlxckfs62LMPUiKu4uzUD3FqvB8A\n4O/GzRG9e4/hfYz4MhDnps1AsFc7HPDtgbgj/+Hu1iDs7/wiQryfx5XFSw3b02Vm4vKiJYa/05H+\nryD2YOEJ4PXV3+BKwBeIPXgIe1t4IT36AfQ6HW5++x0O9uyDvS3b4OBLvRH5y6+Frn9740+4MGce\ntGmPsLeFF+7t+AO6zExc/OQz7O/8IoJbt83e/4F/Deucmz4boW9PKPQ9L+78y3nf72z5Gf+074Ib\na74FACScPoPjI95AiPfz2OfTERfmzs/3t33aZmh544389Tcc8O2B66u/QbBXOyScPoPM+Ic4Pckf\n+3w6Irh1W/w3eBjijx3P3nch52JepTmXnlTUZxAAHt2/j7B3/w/7fDohpI0PTr45Dqk3bxnWPeDb\nA7d+3IhT4/0M+0q+fAXXVn6FfT6dsM+nI279sDFf+Rtr1iFswkQEt2qL/Z19EbXzr0Lj0uv1uLFm\nXfZ54+mNgz374N6OPwzLtenpODd9Nva1a4/9nV/EnS3F18aWVL648+zejj9w1n8qUq5dx94WXkgI\nOw29Xo+rK1bh3xdfQnDrtjj4Um/c276j2BiIyLyYaBBJLHpPMESFHC6dOqL2KwNx/4+d0D56ZFie\nGH4R5z6ciSbTPkD3sON45p3xODPJH2mR96DLzMTJMW/Bto4Huh4IQZeQ3chKTcG5aTNKte+EsNO4\n+mUg2ny1Aj3Ph6Ht+jW4t+03xB44iJaffwKn59qhzuvD8dLZ0ALrPgjZh4gvlqPV0gB0Dz2OGj26\nIeydCYVe7Kfevo3zM2aj4f9NQPdTx1B78CBEBuVe6D66fx9np0xDw/f88NKZU3huw/eI3Pab4aKh\ny97sC6H2P/+EFp/MhzY9HaffnYhavXuhe+gxdNyxDQmnQnFzTeFNu0pb/sbXa9H6iwB0O3EEVRrU\nw+XPFgIAspJTcHrSZLj374fuJ4+isf/7uLOp6DurLzyOt/UXS9Bu3ZpCy0T+sg21Bw9Ct+NHULVJ\nE5yfMRsp12/ghX1/o/mCj3Bz3Xqk3LgJAIj4MhDx/x3FcxvWo0fYcdR7YxROT5hkSCjzavh/78J9\n4AC4vtAFPcPPQFWrJu5uDcLNdd+j1ZJF6HHmJJpM+wDhHy3AwxMFL37rjR6JFp99DJmdLXqGn0Ht\nQQNw87vvEXvgEDr+FoQeYSfgPmgAzkz+AJrk5CLfg7KI2fcPOu/+Ew3eGY/0mFiEjnsHtXr3Qrfj\nh9FxxzYkX7qMiKVfGsr3DD+Dmj17FLPFstGoE5GpVqP78f9Qzas1IpYFIislFV337UX30OOo/cpA\nnJs2A7qsrALnYlGKOpeeVNxnEAAuzJ4HQa6A78F9ePG/fyGv6oALs+fm28adTZvRZMpkvPjfQchs\nVQh9ewJkKhu8eHg/6o8bgysBS5GVnGIof/vHDaj3xih0P3UUDSf44ezU6UiLvFcgtjs/bcGdTZvR\nZuVyvHTmJJrNnoELsz9C4vkL2cf4zbdIOHkSHbb/gi57diIp/CIy4+KLfk9KKF/ceVZ70AA8854f\n7Bs1RM/wM3DyboOoP3fh9g8b0W79t+hx5hSafDAZ52fNzZeIEZG0mGgQSezu1iC4D+gPQSaDS9cu\nEG2UiNq127D8/vYdqObthRrdfCHK5XAf2B+eSxZClMsRe/AQMuMf4tnpUyGvUgU2zs5o+H/v4eGx\n48go5o57jqykJEAQILe3BwBUbdwYXf8NQY0Xu5a47r1t21Gz10uo1qY1RLkcDcaNRbN5c6HXaAqU\nffB3MOzqeKD24EEQFQq4dn0B1Z9/zrDc1t0d3Y4fRq3eLwMA7Bs+A0fPlkg8d77QfctUKvge/Af1\n3hwNQRShqlUT1Tu0N1wAlbe8x6tDYFe3DmQqFWr26omU6zcAILv2QKdHg/HjINooUa1Na9Ts9VKJ\n71FxHD094dyxA2Q2NnDt2gUZD2LQaOIEyGxsUPOl7gCAtFu3odfpEBm0Dc+8+zbs6nhAVCjg8eoQ\n2DduhKg/d5ZqX3e3BsFj+Kuo1roVRLkctXq9BKfn2iFqV+F3sp/U4O230On3X2Hj6gpBJoNbv77Q\npj1C6rUb5T7+vNz794PSqRoEQUDUzl2wqVkD9UaPhKhQwNbdHQ3fe9ekd6q1qalo8NZYiDZKCIKA\nrKQkiAoFRFsVRLkc9UaPhO+h/RDlpe/WWNS59KSSPoPe36xCq6WLIFOpILO1Rc2ePZB4Lv9569Kl\nExxaNIfcvgqcO7RHVloa6r81FqJSiZrdu0Gv0eBRVJShvHPnTnDu0B6iUok6I16DsroTYvb9UyC2\nu1uDUHf0SFR9tgkEmQyuvl1R40Vfw98ievceeAzNPk55lSpoMm0KdIV8/nOUVL6s55lb397o+m8I\nqjSoD0EQUPPlXhBkMiSFXywyBiIyL3YGJ5JQyvUbSDhxEs0XZN+hFOVyuA/oh8igbfAYOhgAkHbn\nLuw8PPKt59anNwAgevffULnVgrxKFcMyu3p1s9e7G1ni/p07dIDrC11wqFcfOLVrC5dOHeE+cABU\ntWqWuG7anbtw8/Q0vBZtlHDv37fQsukPomFbt26+efaNGyHt9h3D68igbbgbtA3p0Q8AnQ66rCzY\n1Ch6VKoHwSG4uf4HPLpzF3qtFnqtFtXaej9V+Zz3DshOTnQZGdnxR0fDplZNiDa5HYxzOnmXl8qt\nlmFaVKkgqlRQODpm79vWFgCgy8hAZnw8spKTcW7aTJz/cJZhHb1eh2r3vUq1r0d3I2HfqGG+eXb1\n6iLtzt1Sra9JSMClzxfh4dHj0CQnQxAEAIA2M6NU65fEtk7u+Z128xZSb97C3hb5j02v1SIz/iGU\nztVLtc0DXV4sMM9j+FA0nze3wHxBoch3zjfwexun352IA51fhHOnDnD17YpavXtBKEOiUdS59KSS\nPoPJl67gytIvkXzpMnQZGdDrddBrsvJtQ1Urty+NaKuCjasLBFF8/Dr3XMpRpUH93GMXBNi6uyOj\nkNqxtJu3cHV5IK4FrjTM0+t1cOnSBUDO57qOYZmyWjXYuLoU+Z6UVL6s55k2IwNXFi9F7P4D0KgT\ns+PTaKAt4r0mIvNjokEkobtbgwAAx4a+Zpinz9JCl5mJ5IirqNqkMSCK0Ot0ha6vy8wsctsChELn\n67W52xJtlGjz1QqkXL2GmH/2I3pPMK5/9Q18Nv0IR8+WxQdfTFwF49QAOm3+OPKsG/nrb7gauApe\ngV/C5YXOEBUKnHrrnSK3F3/sOM7PnIOWn38CtwH9ILOxQfi8j4u8a1za8oJMVkT8mYC26PjLI+dC\nMPd14X8vUaUCALRbvxbO7Z8v176KOk9yLuRKcsZ/GvQaDdr/sgW2dTyQGR+P/R1eKFcsec+/HKJC\nkTutUsGprTee37yhXNvP4XtoP5TVnUpVVlTk/1fo2KI5XvhnD+L/O4rYAwdx6dP/4c5PW/D8Tz+W\nev9FnUsF9l3MZ9Cufj2ceusduPXvizarAqF0qoaoXbtx1n9q/n09ce48eW49qbC/AQo5F0SVCk1n\nTTfc9HiSLjOzwOcaxXwuSipf1vPs4oLPoD59Gu2+Xwf7Rg0hiCL2ehZ9s4GIzI9Np4gkos3IwP0d\nv6PxVH90/GO74afTX7/DoUVzQ6dwu7p1CrQ5vrPlZyRHXIVd3TpIvx+Vr/11SsRVQBCym23Y2GTv\nKz23z0fa3dy72LqsLGiSkmDfuBGe8XsbHX79GQ4tWuDeb7+XGH92XDcNr/U6XXaNwf37BcqqatbA\no/tR+ealXLlqmE48cxaOrTxRo/uLEBUK6DQaJOdZ/qTEs+egcqsFj1eHGI6xuOYSZS1fMP6ayIiL\ny3fBnhJRdHzGpKhaFcrq1ZH8xGhkaZH3DB2GS2JXtw6SIyLyzUuJuAq7+nWLWCO/xDNn4TFsKOzq\n1oEgCEi6EF6q9Uo6/wpTpX49pFy7lq9JjSY5GZrExFLt0xg0SUkAANeuL6D5/LnosG0r1KFhSLp8\nxej7Ku4zmHrtBrKSk9Fg/DgonaoBQKnf++LkrcnS6/V4FHkvXw1bjir16xU47x7dv28YpEFVs2a+\nz3VGfDwyiumjUVL5sp5niWfPwq1fH1Rt0hiCKCLl6jXo0tOLXYeIzIuJBpFEonfvgS4jE3VHvo4q\n9erl+/EY/iru//4HtBkZ8Bg6GInnziNq51/QaTR4sDcElz9fBJmNDVy6vgC5Q1VEfLEM2vR0pD+I\nwbVVX6FGN18onavDrm5dCHI5onfvgV6rxcPjJxF3+Ighhpvr1uPEyDcNFx6P7t1HRkyMoWmFTGWD\nR3cjoUlKMlxc5PB4dShiQv5B/H9HocvKwu2NP+HG12sgr1q1wLG6+nZF2q3buP/HTugyMxGz7x8k\nhJ02LLf18EDa7dvIiItDRmwsLs7/BMrqTsh48MAQB5A9wlZWcgpsPTyQGf8QqTdvQZOYiIgvlkGv\n1yMzLq5AnDnbL0v5Jzl36gidJgu3vv8RusxMJJwKRUxIwTbteYk2Nki9fdsoHabrjnodN9etR+KF\ncOi1WsT8cwBH+gxA0sVLhZaXqWyQHh0NTVISdJmZqD10MCKDfkVi+EXoNBrc//1PqM+eQ+1XBpVq\n/7YeHlCfOQudRgP16bOI/PU3QBSREf2g2PVKOv8K49a/L/RaHSKWfomslFRkxj/E+Q9n4vzMOaWK\n1RiOvfo6ri5bgay0NOh1OqjPnoOoVMLW3b3Aufi0ivsMqtzdAFFEQmhY9o2JP3dBfeYsAGQ3MSyn\n+MOHkXAqFLrMTNzdvBWaxETU7N6tQLm6I19H5K/bEXf4P+iyspBw+gz+G/QqYv7ZDyD7cx35y3ak\nRd5DVkoqIpYug/g4uSxMSeVLOs9kNipkxj9E5sMEaNPTYetRG4nnL0CbkYHkq1dxddkKKJ2dDd8b\nRCQ9JhpEErm7NQi1eveCopALc/f+/aDTaBC9ew+qPtsEbVavwNXAlQjxfh7XVq6G14plsKtXF3I7\nO7T7bi1Srt/Agc6+ODpkOKo2aYxWSxYDAJTVnfDszA9xc916hHj74M5Pm9Fg/DjDfhqMHQOn59ri\n+GsjsbdlG5wY+QZq9uqJuiOym3J5DB2MhNAw/PtiT2QmJOSLscaLXdHso9k4P2su9nk/j6idf8F7\n7deFHo+jZ0s0/2Q+ri4LxL7nOuDejj9Qf8xow/I6I4bDoVkzHOzeC8eGjYRzp45oPGUyEs+dx+mJ\n/rBxcUGt3r1wfuYcXJg7DzV7vYRaL/fEf68MxeG+g6B0cUGLT+ZDo07E0VcLPsyrrOWfpKpZA14r\nvkTkr78hpG17XFu5Gg3eeavYdeqOfA3XV36FU2PfLnH7JXnG72249euD0LffRUgbH1xdFgjPgIVw\nbNG80PLuA/ohIyYWB17ohuSIq6g/9k3UfX04zr4/Bfue64hbP25E23XflNw87rHmC+bi4bHj2Ne2\nPSKWLUfTWdPhPrA/LsyZV+wwsyWdf4VRODig7dqvoD59Fv+074zDfQdC7uCAlgtzh1U29VPWvVYs\nQ+K589jfsStCvJ/Hre9/RJvV2U2XnjwXn1Zxn0FVzRpoOvNDXFkYgP0dXsDD4yfQZvUK2D/bBIf7\nDCh01LHSqD10MG6uW4997Trg+tdr0HrZkkJrNNxfGYhn3n0HF2bPRYjXczg/YzYa+09CzZeyR/xq\nMnUKHFt54r8Bg3Ho5ewHjtrl6YPxpJLKl3Se1ezZA6KtCgde6Ia4w0fw7IdTkfEgBvvadcCFmXPR\ncNJ7qDP8VVz/ag1u/bixyDiIyHwEtVpdurp3IiIismoHfHug3qgRJSZ8RETGwBoNIiIiIiIyOiYa\nRERERERkdGw6RURERERERscaDSIiIiIiMjomGkREREREZHSV7sngqsdP2SUiIiIiMqX0Sv4QSdZo\nEBERERGR0THRICIiIiKSyI0bNzBq1Ch07dq11Ots3rwZPj4+CA0NBQCEhYWhc+fOBX58fHwQFhYG\nAIiJicG0adPQq1cv9O7dG/Pnz0dqaqpJjikHEw0iIiIiIgkEBwdj4sSJqFOnTqnXiYqKwubNm/PN\n8/b2xuHDh/P9LFy4ELVr10aLFi0AADNnzoRKpUJQUBA2btyIBw8eYNGiRUY9nicx0SAiIiIikkBa\nWhrWrVuHTp06lXqdxYsXY9iwYcWWefToEZYsWYJp06bBxsYGERERuHDhAiZPngxHR0e4uLjAz88P\nISEhUKvVT3sYRWKiQUREREQkgYEDB8Ld3b3U5ffs2YOYmBiMGDGi2HIbN25E/fr1DQnMxYsXUb16\ndbi6uhrKNGvWDFqtFleuXClf8KXARIOIiIiIyMIlJSUhMDAQc+bMgVxe9MCxycnJ2Lp1K95++23D\nvISEBDg4OOQrp1KpoFQqWaNBRERERFSZBQYGolu3boY+F0XZvn07GjZsCE9PT8M8QRCg1+sLlC1s\nnjEx0aik4o6dQEZcHADg2Lh3yrz+Cb/3EHv0WJHLo/aGQJeZWe74iIiIiChbaGgoTp48iQkTJpRY\nNjg4GL6+vvnmOTk5ITExMd+81NRUaDQaODs7GzPUfJhoVFK3f/4FGXHxAID269caffvXvv0OOo3G\n6NslIiIiqmx27dqFhIQEDBo0CC+99BJeeuklAMC0adOwZMkSQ7n79+8jIiKiQOfyFi1aQK1WIyoq\nyjAvPDwcSqUSTZs2NVncle7J4JZMk5yMU5P8oU17BO2jR2j1yQI4tWmNmIOHcHHxUggyGWoP6IdG\n48ch9ugxXFq8FIJcDlu3WmizdDEif/8TyVci0PKj2chKTcW+Hi+j19FDiPjqG0Tt3gOIImr16Aan\n1q0QtXcvkiMi4LP2KxzoPQB9zoVCfSEcZ+fMgyCKqN7WGy3nzsoX39Wv1yDy9z9hW7s2slJSAACP\noqIQOnkqAECXpUHbL5ciPjQMCadP4+josei0dRPCFwYg4exZ6NIzUH/0SNR/fbjZ31siIiIia7J6\n9WokJydj5syZ8Pf3h5+fX77l/fv3x5w5c+Dj42OYd+nSJSiVStStWzdf2UaNGsHLywuBgYGYNWsW\nMjIysHbtWvTt2xf29vYmOwYmGkWQzZoF2a+/GnWb2iFDoF24sMjlGbGxqPfacLi/3BOxR/5DxNff\nwGfNVzg7Zx5e2LENymrVcOytd9Bg5AicnTUXHTdvgJ27O87OnY/IHX8AglDodq+tWYeXQ49BkMlw\na+NPqPFCFzg2b47Wny6AXe3ahnLn530Mr0WfwbFZM4T6T0Va5D3YeWQvz0xMwo0Nm9Bjfwh0WRoE\nd34RAJD+IAbP+k+Ca8cOuL01CDc2bILnvDm4tPRLdNj4PfQ6PezqeMBz/lxoH6UjuLMvEw0iIiIi\nAEOHDkV0dDS0Wi20Wi06d+4MAJg9ezbi4uIMHbUdHBwKdOYGsptE5Z0fFxcHBwcHiGLBRksLFy7E\n4sWLMXDgQMhkMnTv3h1Tpkwx0ZFlY6JhQWxcXHA/cBWurf0WuoxMyOzskBkfD5mNDWwet5/r8MN3\nyExQA4IAu8fDobl0bI/4Yyfg2LLwzkHufV7GkddHo86gAfB4ZWCR+0++cROOzZoBANou/yLfstRb\nt+DQpDFkKhvIYINqni2zY67hioh5n+DyF8uhSUyEY56ORwAgU9kgU63GwUFDISgUyHj4sHxvDhER\nEVEFs23btiKX9enTp9h1T5w4UWDe8OHDMXx44Td0nZ2dERAQULYAnxITjSJoFy4stvbBFK5/9z1s\na9VEu8AvkXD2HC58thCCTFZwRABBAPLM02dqAFGAkKdGQ6fJMkx7LfwMydeu497OXTg8bAS6/vlb\nofsXxMJrRLJ3ogfyZsc6HQDg8tJlqNG1CxqMHol7u/5CdMj+fKvFHT2OuCNH0fmXLRAVCvz5bMuS\n3gYiIiIiqgDYGdyCZDxMQJV69QAAUX/vhV6jgdLJCXqtFo+ioqHX63F0zFvZiYYgIO3ePQBA3PHj\ncGrVCnJ7e6THxAAA4k+eAgBokpJwefkKVG3UEE3934eymiOyklMgiCJ0Wm2+/Vdt3BgPT58BAIRN\nm4Hkq9cMy6rUq4eUq9ehy8yEJjkZ6vMXsmNOSECVenWh1+sRtTcEek32SFOCKEKXpUVGwkPYurtB\nVCiyl2u1HI2KiIiIqBJgomFB6g55Bde+/Q5HRrwBpzZeSI+Nxe2ff0Hrzz/BiXf/DwcHDYVrp45Q\nOjqgzeL/4dQkfxx69XXoNFmoPaAfXDt3RMr1Gzj06utIuX4dgihC4eCAjPiHONBvEA4PHwmnNm2g\ndKoGl/Y+OOn3f0i6EmHYv+eCj3Dh089xcPCrUDo6omrjRoZlSqdqqDN0MP4dNBSnP5yJaq1bAQAa\njHwd5+Z9jKOjx8JjQD/EHTuBmH8PwaX98zg0eBicWrVCys1bODT0NaTevo1aPbrhzOyPzP7eEhER\nEZF5CWq12rRP6rAwKpVK6hCIiIiIqBJIT0+XOgRJsUaDiIiIiIiMjp3BzSQhAbhzJ7uz9eMuFkVO\nF1im00FIVENwrv54vt6wPG85FxfAhEMhExERERGVGhMNM+nQQYlbt4oZ1alEtUos4eSkx/XrmbCz\ne4rdEBEREREZARMNM5k/PwuhoQL0+tyRafV6Ic80ipwWv/sOAKD17QZd/QaFljt0SMStWwLUajDR\nICIiIiLJsTO4FbB5HLPmp5+gGzKk0DJjxsixdasM165lwMPDnNERERERUWHYGZwqhJxn6T1+jh4R\nERERkaQkbzp148YNzJs3D3fv3sW///5rmJ+SkoIlS5bg1KlTyMzMROvWrTFz5ky4uLgAAGJiYhAQ\nEIDz589DFEX4+Phg+vTpqFKlilSHIikmGkRERERkSSSt0QgODsbEiRNRp06dAssWLVqEmJgY/PDD\nD/j1119hY2ODmTNnGpbPnDkTKpUKQUFB2LhxIx48eIBFixaZM3zzE4ruTM5Eg4iIiIgsiaSJRlpa\nGtatW4dOnTrlm69Wq7Fv3z5MmDABrq6ucHBwwPvvv49z584hIiICERERuHDhAiZPngxHR0e4uLjA\nz88PISEhUKvVEh2NtHISDX2l6nFDRERERJZK0kRj4MCBcHd3LzD/ypUr0Gq1ePbZZw3zatasCScn\nJ1y8eBEXL15E9erV4erqaljerFkzaLVaXLlyxSyxW5rcGo2nGUKXiIiIiMg4JO+jUZiEhATY2NjA\nxsYm33wHBweo1Wro9Xo4ODjkW6ZSqaBUKit9jQabThERERGRJbDIUacEQYC+kDZAOfNKWl4ZsekU\nEREREVkSi6zRcHJyQmZmJh49egRbW1vDfLVaDWdnZ+h0OiQmJuZbJzU1FRqNBs7OzuYO1yKwRoOI\niIiILIlF1mg8++yzkMlkuHTpkmHe3bt3kZSUhFatWqFFixZQq9WIiooyLA8PD4dSqUTTpk2lCNlo\n7v21GwBwO2gb7u/eU+r1RDG7KqOwRCPt3j0knD5bqu1kPHyIfd17IXxRQKHL02NicWbmHADAng5d\nkJWaWuoYiYiIiKjysMhEw9HRET179sTXX3+N2NhYqNVqrFy5Es8//zzq1auHRo0awcvLC4GBgUhM\nTERMTAzWrl2Lvn37wt7eXurwyy31biTu/f4nAKDesKFw790rf4Fi2kUVV6MRe+QoEs6WLtFIjriG\nKvXro8XM6YUuV9Vwhdeiz0u1LSIiIiKqvAS1Wi1Zq/6hQ4ciOjoaWq0WWq0WSqUSADB79mz4+vri\niy++wP79+6HX69G+fXvMmDED1apVAwDEx8dj8eLFOHHiBGQyGbp3744PPvgAKpWq2H2WtFxKR98c\nh4Qz5/DM2Deg1+lgU706qj7bBLcGvgIBQIJ7bTSZOR0P/j2IxAvhaDFnFtxf7on7u/9GyOzvEHlf\ngfbDWqDbyjmGbWbEx+NAv0FcZ3vvAAAgAElEQVQQ5XK0/GgO7Op44Nzc+YAoQm5fBW2/XAqlUzVD\n+f19BuDRvfuo9/oweAzoj7Nz50GUKwBRgM/Xq6FJScFJv/fg+9cf2NOhC7qH/I2zcxegdt+XUatH\nd0SH7MO9XX+j6QeTETp5CuR2VfDMmNGQV62KS4uXQlAoYOvuhjaL/wfx8d+biIiIqCJKT0+XOgRJ\nSdpHY9u2bcUu/+ijj/DRRx8VuszZ2RkBAYU37zGGWbNk+PVXmVG3OWSIFgsXaotc3ujdd3Dzhw1o\n6v8+Ln253DBfLVeg98MYRPm/j5OLlqDnf/8iIew0rn+/ATW6dMKVFatxsftvWL6qCtaFv4PfvgiD\n0LDd4+f7uULfaigE++o4ldETuomvQ3xlNmTPtIF271rsmf0DFAM/MDwLUPfyHGj/+RH3287A3b2H\nIPT9GPL6LZCx/UscWv47fMZ2L/XxJl64iF7HD0Pp5IT9L/dDpy2boHSqhgufL8K9XX+hziuDyvlO\nEhEREZGls8jO4JRftSwNZABUDg6wf6Y+5HZ2sHFxQVZyMpIiruLRvftoGf8G5jkKiL2YhO9Do3Ek\nQ2FYf6idDMk6GfasVuAb52t4d5oPAKCBvAuG2H2Jpd/nlm2ukKOXrYhlfytRV+aGEfb/gxKP4CRG\n40jGYEQ5C/AoZdxV6tWF0skJ6bGxSLl5C8ffmQAA0Kalwaa6k7HeHiIiIiKyQEw0irBwYfG1D+aU\n9xF8giz3T6bX6yEqFKjm2RLPr/gR27eLyMwUMBjAK/osQ5cOh8M6aFVadPPOQtWvgIXTsgAAynuP\nUO2kgM/6Zz3eHqC6q0W1c3p83DsLHr99hIdtJiClri/i964Fzj5CwsOCiYaQJ0CdJsswLSoUj38r\nYVurJrr8ssVYbwkRERERWTgmGhZEEETossqW3Ng3fAbJ167BXh+Ht992waUvlqH+iNdh61bLUOay\nFpDba9BovBZHjjXGiy+cRPW23ohYfRT6Oi3x7OTcfcYe1eFmhh6jZ2jxT/BDDJzuAVv3NOzc9w+u\nCW0L7Wwur1oV6TGxAID4k6cKLFdWcwQAJEVchUOTxrj+/Y9wae8Dx2bNynSsRERERGQ9mGhYkKqN\nGyLxwgWcX/Ap5A5VS7WO3NYWngs+wtE3xkG0sYFji+ZQ1aqZr0x17zYIm/IhbJyrw/Pj+dmdwQUB\nCkcHeH9RdD+XZ8a+iePj30WVunWh6P4mXri4AMnpfQuUqzN4EELfn4L7f/0NxxaFJw9tlizC6anT\nISoUUNWsifojXivV8RERERGRdZJ01CkpWPKoU0WxeRyz5qefoBsyRJIYgoMF9O+vxCefZGH6dMto\nUkZERERkySr7qFMW+RwNsjyGUan45HEiIiIiKgUmGlQqxT0QkIiIiIjoSUw0qFRYo0FEREREZcFE\ng0olp0ZDX6l69BARERFReTHRsCYSXuWz6RQRERERlQUTDSoVJhpEREREVBZMNKhUmGgQERERUVkw\n0aBSyekMzj4aRERERFQaTDSoVNgZnIiIiIjKgomGFZF9951k+2bTKSIiIiIqCyYaVkTcv1+6fTPR\nICIiIqIyENRqdaVqDKNSqaQOocxs8sSckZ4uSQxnzgho314JUdRDLs+eJwi5P3lfP830+PFazJ+v\nNc9BEREREZlQukTXbZaCiYYVsIREIz0dGDNGjuhoAXo9DD8Ainxd3LLCyl65IsDLS4+jRzXmOzAi\nIiIiE6nsiYZc6gDIOqhUwNatWSbdR40aSnY2JyIiIqog2EeDLIYgsA8IERERUUXBRIMshigy0SAi\nIiKqKJhokMVgokFERERUcTDRIIshinwgIBEREVFFwUSDLAb7aBARERFVHEw0yGKw6RQRERFRxcFE\ngywGEw0iIiKiioOJBlkMQQD0ekHqMIiIiIjICJhokMVgHw0iIiKiioOJBlkMNp0iIiIiqjiYaJDF\nYKJBREREVHEw0SCLwedoEBEREVUcTDTIYgiCnokGERERUQUhlzoAohyiCDx8CIwfn31aCkL2T97p\nkl4XvkxveF2vHjBxotawjIiIiIhMg4kGWYyGDfW4elXEpk0yk+6nXz8tGjQw6S6IiIiIKj0mGmQx\ntm3Lwr17WdDrc/tq5J3Ofi0UWFbS65zphQtl+O03GTQaAQDbaBERERGZEhMNshhyeXbTpuKVP0Fw\nccn+zZGtiIiIiEyPncGp0hAfn+1MNIiIiIhMj4kGVRpMNIiIiIjMx2KbToWFheH9998vMD8zMxPf\nfPMN3n33Xcjlcohibq7UtGlTrFu3zpxhkhURxexmV0w0iIiIiEzPYhMNb29vHD58ON+8Q4cO4csv\nv0SLFi0AACtXrkTbtm2lCI+sEGs0iIiIiMzHappOPXr0CEuWLMG0adNgY2MjdThkhZhoEBEREZmP\n1SQaGzduRP369dGpUyfDvK1bt2Lw4MHw9fXFlClTEBUVJWGEZOmYaBARERGZj1UkGsnJydi6dSve\nfvttw7yWLVvC09MTmzZtQlBQEHQ6Hfz9/ZGVlSVhpGTJmGgQERGRpblx4wZGjRqFrl27lnqdzZs3\nw8fHB6Ghofnmb9myBQMGDECXLl3w5ptv4ty5c4ZlPj4+6NixIzp37mz4GT9+vNGOozAW20cjr+3b\nt6Nhw4bw9PQ0zFu/fr1h2s7ODjNmzMCgQYMQHh6O1q1bSxEmWThByP7NRIOIiIgsQXBwMJYtW4bW\nrVvj7t27pVonKioKmzdvLjB/x44d2LJlCwICAlC/fn388ssvWLNmDVauXGkYPMnc/ZutokYjODgY\nvr6+xZZxc3ODTCZDXFyceYIiq8NEg4iIiCxJWloa1q1bl69rQEkWL16MYcOGFZi/YcMGjBs3Dk2b\nNoVKpcLo0aOxevXqfCO0mpvFJxr3799HREREvj/A5cuXsXTpUuj1uU+JvnPnDrRaLerUqSNFmGQF\n2HSKiIiILMnAgQPh7u5e6vJ79uxBTEwMRowYkW9+TEwMIiMjodfrMWrUKHTr1g3vvfcebt26la+c\nufs3W3yicenSJSiVStStW9cwz9nZGbt27cKaNWuQnp6O2NhYBAQEoHXr1mjSpImE0ZIlY6JBRERE\n1iopKQmBgYGYM2cO5PL8vR9iYmIAAH/99RcWLVqE7du3o1q1avjggw+g0WgASNO/2eL7aMTFxcHB\nwSFftY+rqyuWL1+OVatW4eeff4YgCOjSpQumTJkiYaRk6XJOoSlT5HB0zG1KJQjFT5dcTm+YtrUF\n5s/XomnT3No2IiIioqcVGBiIbt26GZ4nl1dOK5+RI0fCw8MDAODv749+/fohPDwcXl5ekvRvtvhE\nY/jw4Rg+fHiB+a1bt8a3334rQURkrdq108POTo9Ll7KzAr0e0OsFo++nZUs95szRGn27REREVDmF\nhobi5MmT2LJlS6HLnZ2dAQAODg6GeTVq1IBMJkNsbGyh65ijf7PFJxpExtK7tw4PH2YWuTw78cg/\nXdLrvNNHjogYOFABLXMMIiIiMqJdu3YhISEBgwYNyjd/2rRp6NOnD6ZMmQJ7e3tERETA29sbAPDg\nwQNotVq4ubnh8uXL2LlzJ6ZOnQrhcTMMc/RvZqJB9Fje5lDlYWeXnXmwDwgRERE9rdWrVyM5ORkz\nZ86Ev78//Pz88i3v378/5syZAx8fH8jlcgwZMgQbNmyAt7c33N3dsWLFCjRq1AjNmzdHfHw8du3a\nBXt7e4wZMwbJyclm6d/MRIPISHL6gOjZPYOIiIhKYejQoYiOjoZWq4VWq0Xnzp0BALNnz0ZcXBzU\najWA7CZReZtF5XBycjLM9/Pzg0ajwaRJk5CWloa2bdti2bJlEEVRsv7NglqtrlSXRSqVSuoQyswm\nT8wZ6ekSRkLFOXpUwIsvKvHhh1n49FO2nyIiIqrs0iv5dZvFD29LT+DtcouV0+yKfyIiIiIiJhpW\nRzh9WuoQqAh8TgcRERFRLiYa1iaz6FGTSFrso0FERESUi4kGkZGwRoOIiIgoFxMNIiNhokFERESU\ni4kGkZEw0SAiIiLKxUSDyEg46hQRERFRLiYaREaSW6PxFI8XJyIiIqogmGhYGbm/P2SLFkkdBhWC\nTaeIiIiIcjHRsDLimTOQL1ggdRhUCCYaRERERLnkUgdAVFHkJBrh4QLWrBENfTaEPC2pBCH3p7jX\nxS17/nkdGjQw/fEQERERPQ0mGkRGUrWqHoKgx9GjIo4eNV1lYbt2Ohw+rDHZ9omIiIiMgYkGkZHU\nqgXs369BZKQAvT539KnCpotbVtz0rFlyJCaa53iIiIiInoagVqsr1WCcKpVK6hDKzKaQmDPS0yWI\nhKTWsKESKpUe4eGs0SAiIrJ06ZX8eo2dwYmsiCBw+FwiIiKyDkw0iKxIdqIhdRREREREJWOiQWRF\nRJGJBhEREVkHJhpEVoSJBhEREVkLJhpWSrZ6tdQhkAREUW8YgYqIiIjIkjHRsFKyWbOkDoEkwBoN\nIiIishZMNIisCBMNIiIishZMNIisCBMNIiIishZMNIisCBMNIiIishZMNIisCBMNIiIishZMNIis\nCBMNIiIishZyqQMgotITReDRI2DGDBmA7CeFCwIKnc5RXJnCXru56TF2rC7fNoiIiIjKiokGkRVx\nc9Pj9GkRgYGm/eh26JCJZs34wA4iIiIqPyYaVkrIzJQ6BJLATz9l4fJlLfR6GB7clzP95OvilhVV\n9ttvZdi+XYZHj8x3TERERFQxMdEgsiK2tkCbNqaradi7N3vbfPo4ERERPS12Bicig5x+GexwTkRE\nRE+LiQYRGYiPvxGYaBAREdHTYqJBRAas0SAiIiJjYaJBRAas0SAiIiJjYaJBRAas0SAyPuHiRQih\noVKHQRZCCAkBoqKkDoPILCx61CkfHx/I5XKIYm4+1LRpU6xbtw4xMTEICAjA+fPnIYoifHx8MH36\ndFSpUkXCiImsW85HjaNOERmP0tsbAJCRni5xJCS569eh7NcPegcHZMbESB0NkclZdKIBACtXrkTb\ntm0LzJ85cybc3d0RFBQEjUaDuXPnYtGiRfj0008liJKoYmDTKSIiE4mPh3zVKgCAkJQkcTBE5mGV\nTaciIiJw4cIFTJ48GY6OjnBxcYGfnx9CQkKgVqulDo/IajHRICIyDcWQIZB9/bXUYRCZlcUnGlu3\nbsXgwYPh6+uLKVOmICoqChcvXkT16tXh6upqKNesWTNotVpcuXJFwmiJrBv7aBAZGT9M9Jh47JjU\nIRCZnUUnGi1btoSnpyc2bdqEoKAg6HQ6+Pv7Iy4uDg4ODvnKqlQqKJVK1mgQPQX20SAyLvGXX6QO\ngYhIMhbdR2P9+vWGaTs7O8yYMQODBg2CKIrQF3IlVNg8Iio9JhpExiVERkodAhGRZCy6RuNJbm5u\nkMlkkMlkSExMzLcsNTUVGo0Gzs7OEkVHZP1y+2gI0gZCREREVs9iazQuX76MnTt3YurUqRAeNxy/\nc+cOtFotWrVqhdWrVyMqKgpubm4AgPDwcCiVSjRt2lTKsImsWk4fjYgIAa6uguG1IOT+FPe6NGWV\nSj3q1jXP8RAREZF0LDbRcHZ2xq5du2Bvb48xY8YgOTkZAQEBaN26Ndq0aQMvLy8EBgZi1qxZyMjI\nwNq1a9G3b1/Y29tLHTqR1bKxyf49fbppvxqWL9fg3XfZSZaIiKgis9hEw9XVFcuXL8eqVavw888/\nQxAEdOnSBVOmTAEALFy4EIsXL8bAgQMhk8nQvXt3wzIiKp9hw7SIjQUePcp+rdfn9tcoOC0UWaao\ndeLigD//lOHWLTbNIiIiqugEtVpdqbp9qlQqqUMoM5siYuZTZsnahIUJ6NhRifffz0JAgFbqcIhM\nTvbFF5DPmQMAyEhMzK02pErnyf/l/B9eOaRX8r+zVXUGJyLrxlGtqFLjiU9ElQwTDSIyGz55nIiI\nqPJgomHFZJ9/LnUIRGWS++Rx9tGgSkLIc66XsUZD/PlnyAICjBwQEZH5MNGwYvJPP5U6BKIyYY0G\nUekp3nwT8nnzpA6DiKjcmGgQkdmwjwZVOsyqiagSY6JBRGbDGg2qbIQzZ6QOgYhIMkw0KhBxwwbI\n339f6jCIisREgyqdrKzcaVblEVElw0SjAlG88w5ka9cCGRlSh0JUKFHMvtBiokFERFTxMdGoiASO\n6EOWjTd2iSyH/K23IH77rdRhEFEFxESDiMyGTaeoUrPEDDszE7KffoJi0iSpIyGiCoiJRkXEGg2y\nUEw0qNLh9zERVWJMNIjIbJhoEBERVR5MNIjIbPgcjcIJwcFQNm4M3LoldShERMaTlARl8+YQN22S\nOhKSiFzqAMgEWFVPFirn1Ny5U0TLlgrDvJz5OdNPvi7rdL9+OsyapTXDERmHYsQICMnJkH31FbQB\nAVKHQ6bCDJsqGXH3bgg3bkAxfjwyRo2SOhySABMNIjKbGjWAF17Q4do1AampAvT63GuvwqZLel3Y\nsrQ0AVFRglUlGkRERBUREw0iMh69vtgaNbkc2LtXY9IQPD0VSElhrR5VICV8roiILBX7aFRAwoUL\nUodAlZDw77+wsbWFEBwsbRwCO5uThSpn0ymliwvkfn5GDobIDDIzpY6AJMZEowKS/fCD1CGYjlbL\nds4WSr54cfbvzz+XNA5RZKJBFYuQmgrZjz9KHQZRmQnR0VKHQBJjomHtoqKkjsB89HrYVKkC+eDB\nUkdCFsyqEw0m0RVP3iZPFtj8SQgNlToEIqrAmGhYOfG//6QOwXw02W37Zbt3SxwIWTKrTDQs8AKU\nKgcZhx0lktyNGzcwatQodO3atdTrbN68GT4+Pgh94mbBli1bMGDAAHTp0gVvvvkmzp07Z1gWExOD\nadOmoVevXujduzfmz5+P1NRUox1HYZhoWLvC7oBW1IsW3u2lUrDKRIMqB0v8Dquo/y+IrERwcDAm\nTpyIOnXqlHqdqKgobN68ucD8HTt2YMuWLQgICEBwcDB69OiBNWvWQPf4n+LMmTOhUqkQFBSEjRs3\n4sGDB1i0aJHRjqUwTDQqCi2H8qQKJjm5XBdmTDTIovBCnoiKkZaWhnXr1qFTp06lXmfx4sUYNmxY\ngfkbNmzAuHHj0LRpU6hUKowePRqrV6+GKIqIiIjAhQsXMHnyZDg6OsLFxQV+fn4ICQmBWq025iHl\nw0SjglD4+Bim9fzHRtYuJgY2rq6Qv/56mVdlokFERNZi4MCBcHd3L3X5PXv2ICYmBiNGjMg3PyYm\nBpGRkdDr9Rg1ahS6deuG9957D7du3QIAXLx4EdWrV4erq6thnWbNmkGr1eLKlStGOZbCMNGoIMTw\ncMO0kJIiYSQmZInNDsgkhMuXAQCyHTvKvK41JxpCfLy0AWi1QFyctDFUZJb4HcYbU0RWIykpCYGB\ngZgzZw7k8vyPwouJiQEA/PXXX1i0aBG2b9+OatWq4YMPPoBGo0FCQgIcHBzyraNSqaBUKlmjQWUj\n27BB6hCIJGOVicbjiz3Z5s2AhDcK5IMHw8bDA7h3T7IYiIiocIGBgejWrRtatGhRYJn+8Y2MkSNH\nwsPDA9WqVYO/vz8iIyMRHh4OQRAMZQpbz1SYaJD1sMS7gZTLQu6MWmWikZeENQqyPXsAAIIJq9GJ\nqBIx8YhGlUloaChOnjyJCRMmFLrc2dkZAPLVWtSoUQMymQyxsbFwcnJCYmJivnVSU1Oh0WgM65qC\nvOQiZNEs5OKOyFJY5ZPB836OrS54Kpalf0fzBg6ZkNzEIxpVJrt27UJCQgIGDRqUb/60adPQp08f\nTJkyBfb29oiIiIC3tzcA4MGDB9BqtXBzc4NKpYJarUZUVBTc3NwAAOHh4VAqlWjatKnJ4maiQdaD\n/xAtmzH/PmW5ONNoIFy7Bn2zZgAqQI0Gz/OKi6MDElEZrF69GsnJyZg5cyb8/f3h5+eXb3n//v0x\nZ84c+Pj4QC6XY8iQIdiwYQO8vb3h7u6OFStWoFGjRmjevDlEUYSXlxcCAwMxa9YsZGRkYO3atejb\nty/s7e1NdgxMNMh68AKMCiF/6y3IgoKQ+fff0Pv6QhQBvV6Ara0SQHbOkpO35EyX9Lqksi4ueuzb\np0GNGsY/HkGvB8/0CiTP95bi9deh+ftvCYMphKXXuBBVcEOHDkV0dDS0Wi20Wi06d+4MAJg9ezbi\n4uIMHbUdHBwKdOYGACcnJ8N8Pz8/aDQaTJo0CWlpaWjbti2WLVsGUczuKbFw4UIsXrwYAwcOhEwm\nQ/fu3TFlyhSTHh8TDSnpdBBOnYK+VSsI585B365d9u3YsqhMF995jzUyEkJaGvRNmkgXD1kEWVAQ\nAEA8dQpaX1+89ZYWOl3u6aLX5z918r4uz/SDBwKuXhVx+bKAGjWM9Plj06lKQTxwQOoQiMjCbNu2\nrchlffr0KXbdEydO5Hstl8vh7+8Pf3//Qss7OzsjICCg7EE+BSYaEpJ9/TXkU6caXmuWLYOuiE4+\nRapMiUYeNo0aAQAy0tMljoRM4inO69de0+G110x3sf7ppzJ8/rncdB89S0g0Kun3iklYeo2BpcdH\nRFaNo05JSPj333yvxYMHJYrEShj74icyEsK5c8bdJlV4OddlJssHLCHRIOPhqDtEVImxRsPaVaa7\nUUZONFgrYmTGPBct+LzOad1ozHxASEjIfSFVopGWJs1+KzjZ7t1Sh0BEJBnWaFiS8l5IJycbNw5L\nZarmHJmZptkuVUimSDTyycqCuGuX+e+E531+hwUnelYvKUnqCPJjMzkqgXD5MoQzZ6QOg6wUE40K\nQD5unNQhWDUZx/mmMijreA1lJVuzBoohQyCfNMm0O3qCYuJEs+6vsrIxxVBlRCak9PKCsn17qcMg\nK8VEw5KU486ScPYsxH37TBCMBTLRnTfx1CmTbLfSyWn+I1XTHzPdmc3to2Gau/7C6dMAAPGJPlym\nJuzfb9b9ERFRxcdEw5KU40JJvmSJCQKxUKa6kGTTAaMQw8Kyfz8x3F65WHDTHZM3nbKEzuD8TFQe\nFvxZIyLrx0RDQkJMjNQhEADh5EkIYWFQ9O/P/hrWzEwXTCZPNCyAoNFIHQKZCxMNIjIhix516sGD\nBwgMDERYWBiysrLg6ekJf39/1KtXz/C4dTFPg+mmTZti3bp1EkZcNuKxY/ln8C5i8Uz0/ghqNZQd\nOwIA5NOmIWvFCpPsh8rAgj8LOV85JgvRAo5dtno1dC+/LHUYZG5ZWYDcoi8LiMjKWHSNxrRp0wAA\nQUFB2L59O5RKJWbPnm1YvnLlShw+fNjwY01JBpWdcO+e6fdRnv4aOh1k06dD9umnxg/IWlXgIYNN\n/hyNnGFmzXynOW8thhgcbNZ9UynduWP8beZNbC0gya1McvpjWY0bN6SOgKyQxSYaKSkpaNKkCd5/\n/304ODjAwcEBw4YNw9WrV5FkacMDGgu/5IuleOklk+8jp59BmdbZtQvyFSsg//xzCGfPmiAq6yNb\nvvzpNmDBzTlM3XRKvH49e0Li7wPBGH1tyKiUXbsafZvi3r1G3yaVjrJDB6lDKBNlu3ZSh0BWyGIT\nDXt7e3z00UeoVauWYV5UVBSqVKmCKlWqAAC2bt2KwYMHw9fXF1OmTEFUVJRU4ZIZCGq11CEULjo6\nd9pSYzQz4fZt8+/UTBfmopi9n0ITjYcPIQsIABITc8vv3g1x506zxGZUPJctjmCK/3EPHuTZgfkT\nfHHzZghHjph9v1R2Ah/qSeVgsYnGk6Kjo7Fq1SqMGzcOMpkMLVu2hKenJzZt2oSgoCDodDr4+/sj\nKytL6lCpMrPgO/FkHMX10ZB/8AHk8+ZBnqeJp+KVV6AYOtRM0RFZF8W4cVB27y51GERkIlaRaFy7\ndg3jx4+Hr68vRo8eDQBYv3493njjDdjZ2aFGjRqYMWMGbt68ifDwcImjLT/Zzp2SN5eoNPg+UzkV\n10dDuHkz+/fjGh3hafo6SJ20Sr3/CkzcsQNiUFChy2SrV0P47z8zR/QY/+ZEZGQWP7zEqVOnMGPG\nDIwePRpjxowpspybmxtkMhni4uLMF5wJCGFh0LdtK3UY1kOvL9c/R+HwYRMEA/6jrgRyajS++kqG\nXbtEw59cEADZ9Y8gIA4Id4P+bTlkG2Mh4DsI0EPrJ4cgwPCTs44gADJ8DQF6CMhOgBXQYIJmOxpI\ncHxkeorXXgMAZAwbln9BfDzkU6dmLzPXgAr8ziIiE7LoROPixYuYPn06ZsyYgV69ehnmX758GTt3\n7sTUqVMhPP6SvHPnDrRaLerUqSNVuMaRmlr2dQq5Oy+bMQPajz8GVCojBFXxCCkpxttW3sEJKtM/\nba0WsnnzoBs+HPpWrfItkn3/PbRvvw29t3f5tp33fdRqAZms5FXOnYNs7lxoP/kkNxswgWee0UMQ\n9Dh6VMTRo08u7Zv9KwrARgAYm7vox+K2+m6BOdoUB3zxNIGS9ZHiOT55P2tS1vRGRwN5+mRSxSOb\nPx/aBQsq1/9JstxEQ6vV4tNPP8XYsWPzJRkA4OzsjF27dsHe3h5jxoxBcnIyAgIC0Lp1azRp0kSi\niKUjPHpUYJ48MBDw8IB20iQJIqpcZAEBUocgCXHnTsi/+AL44otC774qO3Ys/13ZPBc84u7d0PXr\nV+Iqsl9+yV61c2eTPgOie3c97t3LNIxCq9fn/iiGD4dw5gy0nbpAs24dlM2aPa6nEJAZHg69XjCU\nzbuu0qu1odwNPIMB+BOZUJjsGIgsjbJTJ2TmjLhGFZJ88WLo+vSB/vnnpQ6FzMhiE43z58/j+vXr\n+Oabb7BmzZp8y1asWIHly5dj1apV+PnnnyEIArp06YIpU6ZIFK2FyjPyjTGJf/0FISwM2rlzTbJ9\nayPkfZ8r052a5OTcaVPeCS1rLV95agUBIDYW8gULkDVzJlBCzWj16tk/T1LYREPEbehsG0PTALDB\nTcOyjIYAUPj7ZIPLudvA4+dZpEo8wksJ57K4Zw+EY8egnT/fTAGRqYkbN0I3dmzJBY0lz+At5nhO\nElkAjlxV6VhsouHl5X/zxwMAACAASURBVIUTJYzj/u2335opGjN68oIt79CDFkIxeDAAQOvvD9jb\nSxyNhTFhkx2Lk+dcFZ58yr0Vks+aBdmmTRBu3IBm927J4hCR3ctcJ/VYHSUkGoqBAwEA2okTAWdn\nc0REJqaYMAEZZkw0xL/+Mtu+yEJwIJZKpxJdFVknIe8zGqRw8ybkY8Zkt5+9excKL6/cZRbwhSGf\nOBFCSIjUYeSqTDUaef/+GRnG3bYU72POcyMePizX6uIff0A8fjx7+inOyZwO4ZInGqWk9PaGuGGD\n1GFYJfGPPyCbPj339alTEkYjgfLWPlYgCl9f4NYtAIBswQKIW7dKGs+ThEuXjLvBp7hukK1YAfGr\nr4wYDJmDxdZoUDbZggWS7l8xfjzEI0eyL/wSEyFevlzySmYk++47yL77znwjtFCuvP8wTJkYWEBC\nWxqKJ0cQKidrqdEwFHvwAIp33kHGG2+YOKCKJ+ec0U6bBtSoAcWrr0ocEZmbeOwYFH5+0OzZA/mi\nRQCAjMejklkCxSuvlH9lI393yx8n5RnvvWfU7ZJpWccts0pEPmNGvtfCU9zxkX/2GYSLF58qHuHk\nyezf169DNNKQsOK330JWUft3VJIaDdn//geFn1/uDAs6bvmMGZB98IHUYUDctat86z1ONPQQIB82\nDJCo7bqyT58iHn9ORmdJybQ5O2Sb8LjFbdsgf+89y3pvi5KSAlmeh3xalJya3sfk48c/1XuqGDs2\neyRBqjSYaFgY8cyZJ2Y83Z9IWd7hRR8THg+3KJ44kX8Y16egmDQJ8qVLjbIti2NBF9ymJP/kE6lD\nKJIQGQn5V1+Vf6hQI12YKIYMKdd6eWs0ZH/8AfmcOUaJpzyEEvrJUQXwxHdWvhsIpmbCJEAxahRk\n69cDUVEm24cxyb/8UuoQSkW2aVPpb2AW8vcVYmIqRJ8+Kj0mGpbOyBeu4rp1kL9bcMx+i3HnDhR9\n+z51TYxkNBqTbl7ctAnysWMlvUsnW7Wq9IULGXq5VPKe94IAZdWqsFGpynbcZfzsyHbuLFN5UynQ\ndMrE51SxWKNhFsLj773SUPTrVyABFHfvhnzIkPIl1098TsTDhyGEhZV9O+UgHjqUf4Yp7nRL9F0p\n/vgj5G+9ZR01KmX1tMeU53tFtmxZ9nc7hzausEqXaKSkQLh7F8LduxCvX0eVbt1MHBYZGDnRUEyc\nCNkPP+QbVtCSyOfMgbhvH+Rvvil1KOUiHjhg0u0rxo+HbMsWICbGpPspjnzatIIzizhPxd9/L99O\nnvhHJuRcbD9RjW8S5fnMGfFiokCiIWUtWUW8SLJA8lmzIO7bV6qyYkgIFE88W0rxyiuQ7doF8Z9/\njBLPU7XLLwPZDz/key08HkyhIlD4+UH2009AaVoCWHJN+NN8BxS1bp7jlc+aBQBQduhQ/v2QSZ0/\nL8LXtwqeey57lNGAABucOlXyQ3RzlJhoKAMD4dC8Oao+9//s3XeYE+XaBvB7Jm0pSwdZigoiLoeq\nyFoAASm6KEVBUOyAIBxBEFFkEURBZS10PdIsRxEQPAepiopgO6JgBWyIwifgitJhk2nfHymbZFMm\nZXaSzf27rr02ybQnbTLPW9uhcufOqHzFFVCCZgEmAxk0XKp13DhYpk41ZN8J8YxeJH77raGHEbZv\nN3T/hovww2SZPBmW8ePLMBhA2Lkz5OO2O+5IeN82/6QzVYcPTuIFealRp1L5IoSSI8aCH+8kreLr\nr8PWu3fJgiR9DoUUHFY9XkKco8glLwAd399UTuhDzXuhN95w64U6j6fquZ3wwAMVMG/eGZx1lvv9\nvP56CQUFWbq3j/rO2lavxvGff4Zy8cU4sWcPTi9cCLVZs/gjptgYdJFheeEFWJ94wpB9J0Ioo1Gt\nrNOmGbPjsroojHAca2EhrHPnlk0cHrYxY8rmQKl60W1AjYaGFHiuqXwBRLDdeSfEd94xO4yUZgma\n8LfMpeo5SyfBiKaboV4TKwdBTVVWK9CiRUlztyZNVFj0V2joaDpVuTJgt/vaCcs9e8LGSXaMVVwM\n27XXwpLEi2HroEGwJdgxPNls3buXvpDxu2/95z9hHTIE1ptvhiVUcx2DCDqbMITeOP4fFeuIEbDq\n7Yip4zjCJ5/EHYvpwj0/QYCtRw9YjEyS47m41rGNuHy5rl2Fazolrl8PW6tWZTqJZ0Cn/5MnYWvb\nFuLrr5fZ8TOFGKpz7IkTie1U02DLz4dl+vTE9lOG7FdemfR+QcKePbC1aFG250P/907Pb8LJk8bF\nYgDdLQJ0NJ3yPXT4MOwXXADs359AZGQEq1XDr78Kvrdt0yZrTD+TURMNrVo12FasgNKsGSqMHAn7\nnDnmTyJXzgmffgrx3Xfdpe5JKlG0vPkmxGR3sE6wpEb88MPSJ1j/TmKLF8Py2muwrFoFaywdkBNk\nu+mm+DdOoPrX8uKLsLz8sr6Vdbz2tqFD444lZblcELduhTXVmv3p+J7adPY7Cpdo2K6/HuKPP8JS\nhpPjiVu3ltx+5x2IO3e6h6ckw4lvv53YDmQZ4ubNsD72WHICKitJTqTFzZsh/vwzrHfdldT9Rjzm\nxo1+d6L/Jog//GBgNMlnHT06sR2EeU2E336DhRPypZxp04oxaFAlfPaZBWefXQWPPJKFwkL9A71E\n/Qac/te/IF9yCYoffxzqeedB/P13nF68OKGgKTLRv727wSUdFk9HrHhYBw9O+PiO2rUDfljEOEee\nsNesCUdWFsSgzoXxSGQYX+uUKbDXrw/8+WfCccRD+OYb320tjkTQ1rEjrPfck8yQAh0+DHv9+vG/\nT/4/UFEuSOznnOMehaeoCI6sLPfnY+XK6MdI9c7gfvfFl17yPTfBLymwTJsGe7NmSRuxSly+HLZB\ngyKu48jKgrBlS1KOF45l4kQ4srJgr1MHwo8/Rl75r79gb9AA4osvxnUs64gR7lmbTRL1+SVLijWP\nM6p/iFCWz9Nv9CxHjRoQ33gjps0dWfrbv8fDetddsHXrltA+vOcduFywNW8eus9nmNfcfsUV4Xds\nwPtkmT0b9saNQ/c3oahq1tTwyScnsWvXCXz33XF8/PFJtGypv+YxaqLhKCyEds45QMWKcI4bh+IZ\nM+Awu81jOec/qk/IKvVkHmvmzLi3tbz1VlJiiHtkIj/eiQ1tKTB0r/DXXxCNHio1TImQxTOzrDuQ\n2C+Yxc8/h2XRonijir7/NWsg/PVX/O+T3/MW//vfiKsKf//tnmjSbz3bLbfEd9xoDOgMfhA5WIee\nWHfgQmzcKGIDrsZGXIW3fzoPmzYJ2LRJwAd3r8QmdMO76Iotw1fg/fcFbN4s4MNpH2HL3rOx5c2/\nsXWrgA8/FPDRR+6/jz8W8MknAg4f1h+T3toY67hx8Txl3bxzDQjHj0ct+RTXr4dw+DBsI0bEdSzL\niy8afv6NePxEa3H1fv9TrA+BuGqV2SEkTPjrr4D7tltvNSmS0Cz//ndiE/D6ne+Eb7+FuGdP6D6f\nKZLEWh98EMKBA2U2bHN5c9ddFQEAtWppqFIl9u3DJhrWNWtQYdgw2F9/HRWGDy/5GzIE1nffjTvg\njCXLsNetC+u995b5oWMtHbG1bQvr9dfrW/nIkZKSjWPHAp6nZfp02CtVijq8X6lJCpPh99/dJdgv\nvAD83/+5by9YoHtzW/PmSQtF2LkzsDRdlktesxAcWVkQolSlO3JyYL31VjiysgIuSCxvvlmykt9F\nud73olTsu3a5Y4+xRK7UfjZtcu8nqJ20dcQI2Bs21N0u23HWWX471XGBFKXZgmXaNPfr4temWvz6\nawibN+uKBwBw8iQcVavqW/fAgdKPBQ3Za4cLFsjYhktwLdahz9YH0LevDT2xAfnYiGtfvhG9etnR\nq5cdPbAJPbAJ3fEueuxdiJ497cjPt+NKbEYXfICrbj8bPXrY0b27Hd26uf+6drXjyivt6NLsb9ir\nVIHlkUf0P1evMPOjCJ7sxZafD1uHDiHXsUyaBHu1ar59hPouWCZPhr1q1YglkJZ//SvmsC2FhbBX\nqFA2wyQnKNpoSaHOH7brroMQ64h9Yb5HwhdfuL+zQTPcWwoKYK9eHSguju04CcaT8G5/+cWQ/VqH\nDHGXlnuPs3On4Qm371gffJCUmkTrnXcGPIeox/WrKfWfUNTiP7no6dPu73msscya5f7/z3/CXq9e\n1LlVrEOGwH7uue47xcWwV68eGAclrEkTBcOHV8CSJXb8+982359eYbv5y926QatdG5avvoLcqVPJ\nAlGEc8KEhILOSIcPQzh6FJYXXoA8e7bZ0QTSNPeX2WoFNM3ddGvnTsg6Jk8SP/7Yd1vYtg1aq1a+\n5+l7fMcOaBGaIFiWLIGc5HaZ4po1AADbvfdC8jwP2+jRcA4bpm97/yZcioKYhlgAAkpyvDUE1tGj\n4erfv3SzKlUtdUEsvvIKlOBOnEEX4xbPxb/1/vuhhGrupGm+fXvbaUd8L0K836KnmaR11Ci4+vWL\nuw+KdcoUd8z//ndAfBZvsxanE6hQoWSZnsRDT2mZKIZez7N/7+hjwpdfBsb76KOQunSJvn8gplIy\nceNGqIMHB8QkfvppwDoVUIzV6INd+Ac0CFBaXwj1hhtgmTQJGgSoV10N5fL20DTA+sgUaBDcj9ep\nC2X4cGgafANJSCPvgVbV/WOvaZ7Dnj6Nl+aexh8nKkKAC9Ynn4QSKdkINbvvTz+Ffi0OHQJUFaJ/\noub//dE0WJ9+2r3uDz9ACzNUurWw0L3Orl3QLr44emzBF6dhPj/WyZMBuF9zNT8/9P40rfTnPMR3\nNCLv8U0YstOyYAHkuXPDf+51xmTx/E5ZJkyA6p1MUFVhfeYZAO5O1lpubuRzY/Drqef4qureJtR7\nmozXM8nvjeW11wL2LRrRtNz35UXAa+k9r1unT4fkf50WTpjX0JLAAA/+c0dZn3nG97sl7NwZubla\nlPfT4n0dT54EggtyvOcUVQ14/YUff4Rw5kxAHAFSrPYuXTidAiwWlJo749Zb9TXNDT+eWIUKUC69\nFCe3bgWCSk6yHn4YxenWwcxk9o4dzQ4hLIfnAk/p3RvCr7+WPF6pUtRtbf37+27be/WCK4GhFoVk\n1pQlqcrWeuutsLzxBuSRI6F4mm0kk7hkCWwjR8L1+eeBxw0+URYXwxGldCi4s6P4009wVKwI2X/k\noHD27oUjxLDVwrFj7v9Hj8JRsSIkvZ3V/fn/SPoJaCPsP9rYzTfDoqP5hO3eeyFlZ0de6dQp2ELU\nIjoqVgy4b+/RI+B+8MV/pP0HbxuR54fO3qwZhF9/hXrBBVD8m7t5XIP1uAae0f2+BpzvdIVj0gwA\ngHxldSj3Xgph1y7YH3nct41apwWkgqGA0wnHNHd7aeeoG4FGQa/Rn8exae4f+BFNfQ9Zb7ghbMj2\nWrVKPxjh++X/2lomTIB11iyoXbtCWrfOd64BAPull4bdh2+dDh3geustaGFeY1ubNoAgQPKrFbU8\n+yysEydCnjQp/I7DxG+79loIn30Gl3+7Mk2DvXJlqL16QdYxcpjwww+wt24NwH1OLWuWhQshP/ss\nHEHfDeGjj2Dv1g3Sa69B7dev5PEwtZzeggzva2WZNy+gWa+9bVsAgHPfPqBOnZD7sHXtCuHnn+Ha\ntw/Cli2wX3UVpGXLoPbtGzZ+69NPw/r003D615j8/Tcc9epBHjcu9AWkXnv2wOGprXYmoUbGct99\nvtvW4cNhefllKDpneI9GeP99aJ7Jkf2/N849e4A6dQLfXz2j3q1ZA9sNN8DlN2qo+N//Rnwv4hX8\nWQnFUbGirvfAcdZZkF56CapfwU+oaxPrXXcFFGSJb70F1YTvX3n03HP6O36HEjWlt376KSp16YLs\n1q3df82awZrI8J8ZSvAfsk2SYm7CUhYsb70F0a8zcVz7+M9/4t7WmqzqTlkObEPq6b8BwF0Sonf4\nREkqqTV47jn3+xbcuTZcZ9tQtUGa5l7ff1ZUzw+VuGxZyOP7nsPevVHDDagt8OMtxQ3J0zSlVH8b\nz+PBJf162+oHOHPG3Sk7iH9tGFwud8mVqupKMrysDz4Ycbnw9deh49ErSudBIZ7BCyTJl9DrHW1G\n8E9EPZ8tS3Di6/1c/9//hT0uAPdoRFBLOpsDsHhqAEMe2//746VzgjlvMwjxvff0N7UJ+k5ZIyT4\n4g8/QPz++4DnZp040b1sw4aw+wx3DhDfew9C8AAcxcUQVBWWSH3J/Pbv38wwWf3YYhWqI7m3qZnl\n0UdjGiTA20wm3IWj+Omn7s+D98/pdH9vFAXiJ59AKCpyH9fTxNPXVC9aDJLkO2eKH37ojsFTm+L7\nXsY42EGp99D/98B7fvY/fjBNCzgnWP1q4r0jBlqCmpr5xDgpo3XmzJC/OeJ77wGe19Qn1Oc56LlY\nPIl3QFPb4N9c7+/m6dPu7eMZatjpjJpkRBV8DpgwIeoobMG/f77O6UbMA5JhmjfPRosWpf/0it4Z\nfNo0FBcWQq1dG6eWLYPrlltQnEbjcqciR3Y2HGFKgNKd5fnnSz82Z46+jZNUremoXBk2/5Kmhx8u\nWVapUqnS7LD7CSoRdGRnBzxmHTOm1DpetlGjfLe9oz8JR47AkZ0N4bfffMsEz8WXJUT/EUd2NnD0\nKITvvoP9wgt1xRwLR1aWe0SU1atLXbA7atRwt88OM+N3TMepUSPqfhx168JRq5bu98ZLiDK6ly3E\nvCSO6tV17dv7+lgjDBMc6n2LGM+IEWE/M5HYr73Wd9s6cSKEjz8uVYrpHb7asnBh6R389hsc2dmw\nDhsGR6NGpRKNmONp3z7mbaLVyGHvXojLl5d+fXSU1jqysyF8+y3sNWr4HhP9xvp3ZGdD+Pln3fv0\nnycg2udF+OYbOLKzfYmfYZOBxsBb2xCK8Msv7nh1DhEt/PZbqX4a/mwDB8JetSoclSu7/6pWdZ8/\ngkqdvcms+P33sN51V9TvgSM7G7Zu3dz/Bw70PW4dNAiOGjVgz82FIzsbYqjBK/TMBn7iBOwNGsCe\nm+t+Hn36uGOSZVjGj3ffDupT5ahQAY4aNdxzLf3+e/Rj+G9buXJM64ubNpX6zQEA27BhcJx3XuDK\nQQVbwtdfu9/jWbNgmTvX/Tp5CjUs/jUae/YEvH62q66CrXdv9/uXnR3z+RiA/v5qACyPPx7y8VK1\ncYcOwaaz2bOXuHMn7ME1P2w6FZcNG05i/Xr33+rVp3DffU7cfbdT9/bR59HIzobSrh1gs0Ft1gzO\nggLY589PKGjKLJb1690nwkglyadO+TqSpjxPqVbUzqh//gmEKA0OGGPdQwgzOZf4v//pGwUqXCl2\n8HGOH3fXHvj3IYnUxIRgefVV9w1VdX+GVRXYt8/dxyQZI3TFMRSy5fnnQ9eaHT/uq0UAPKPfnDrl\nqyXyzsGRaKJhBMubb7pL24PJsvvCMVTNih9x9WoIIWrOfMv9RynzlkorirumRVECLk7DzlVy5Ahw\n+LB78ABZdp8HPMmmdeLE2GrLytLhw76RkHw1FE88oTtey1NPRVwu6OjPF7C/MLWvwQJqPb3bega8\n8NYKWidPLvUdEsLNGRVUSywcOQJh3z7g+PGSGdZPnYJ17lz38T/7zP3Yb78FfP6sU6dCTKWJi721\nhUeOAH//7evfYJ0wAdbx4yNuavPr3ydu3VqmM81b9TTrTUAiQ9VTibPP1nx/552nYvBgF95/X/9M\n7sLRo0cjFu1UuuYaFE+aBMecOZB69YKam4sKw4fjZFCb8nSRZfD41OEYPS52unAWF6f9a6Hk58Pi\n3ywjDSmDB8OyZAkAQKtfH0KMpXOZRrnlFgi7dweUkqez9vgI25AHCXazQzGNs7gY9n/8A8Ivv0DL\nyvLVLuql1a4dtUaN3JRevSI2z0vKMXr2hOxNQj7+GPauXSOv378/LCHm1VHPO883GIjavTvETZuS\nH2ySqa1aQf7Xv2C//HKzQ0lprs2boV12WZkft9ioEdrKyJYtgZ3Af/9dxFNPOfDll/rmeYuaaIg/\n/QThjz+g1a2LrPHjIRYVwXnPPZASmT3ZREw0zOX87Tc4zjnH7DCIMlpHbMUnuBxKhPFAyjvn77/D\nUb++2WFQErk++wzCoUPQRBH2Xr3MDqfMaHXrQmvVqkxrI9IRE434XHttSTNIQQCyszUMG+ZE5876\najKjJhrlDRMNIsp0nfABtqITNLDNMhFlBiYa5ghbnJXdqlXEjjMnQo3mQkREKU+EezQZDWCqQUSZ\ngZ3BY5KfH3mKgw0bIveb8wqbaJzyDANnf+klqHXqQL7iCkBRYP3gg9LD/xERUdrwJhoqRFgQxxCW\nRERpxvLKK5B1zN9DbgUFyamJCZtoqI0aAQDEr79Gsd9oHa42bVDRb6g5IiJKL0w0iCjTiKtXA35z\nn1Bk2dkaWrdWS3UGj1XUnoDin3/C+v77kC+5BBBFWLZtg+g/+RwREaUV/0SDiIgo2PLldrRuXYyn\nnirdx1gQgE6dEmw65XVm5kxkPfwwKnrGpVYuuABnCgtjDJeIiFJFSR8NtlkmogyhY/JPKvH44+6m\nU2vXlk4oVq/WP2Jh1DWVvDycijL1OxERpQ9vojEP98AOFwRoEOD+EU7GbQEaauBvXIN1EMEfdyJK\nAVEm/aTQ9u8XsHChA3/95S6YcrmArVut6NMn9ETDwTJ3EHUiogxVA+5ZsMfjaUOPsxUd0REfGXoM\nIiJdqlY1O4K0NHx4RXTvLmPjRivuusuF9euteOGF07q3Z6JBRJRhZuNe9MMqqBB9zae8dRH+t6Pd\nD7dsA/LxBgbgOKqU9VMjIgpJKCoyO4S0ZLUCY8c68e677kTj1ltdGDKkIjp31pds6Es0jh8Hqrh/\nMISiImh16sQdMBERmasGjqAvVhu2/79QE29gADubExGlueJi4PffBYgi8OuvAho21LBvn/5ze9Q1\n7QsXouLdd/vuVxwyBPYFC+KLloiIyj2OakVEVD6MHu3Eli1WjB7tRMeO2WjcuAouuUTWvX3UGg3b\nihU4tWGD7/6p//wHlXr2hGvYsPgiJiKico2JBhFRenvnHSt69JBhtQKDBkkAgL17j+PkSaBaNf37\nid50SlHcDbS8BIFDhBERUVgcPpeIKL0VFGTBYinG9OlZqFjxTKlL/06dFF37iZpoyPn5qNSjB5TL\nLgNUFdYtWyD17h1X0EREVP6xRoOIKL0NHuzCnDkO7NsnorAwcNK+pE7Y5xw/HnKHDrB88QUgCDjz\nzDNQ2rWLL2oiIir3mGgQEaW3ESNcGDHChYUL7bjrLlfc+wn7KyB+/TUAwLJlC+ByQWnVCkrLlsDp\n0+7HiIiIQvBO4MdEg4govSWSZAARajTsy5ahuHVrZD31VOmFgoBTnToldGAiIiqf2EeDiIiACIlG\n8RNPuP8/8ACUK64I3GjtWmOjIiKitMWmU0REBERINITffoP466+oMGkSzkyf7htpSpBlVHjoIZy4\n9toyCzKcoqIiFBYW4ttvv4UoisjLy8MDDzyASpUqmR0aEVHGYqJBRFQ+nDwJPPecAzt2WCAIQLt2\nCkaMcKJCBX3bh000xD/+gP3NNyHu24eswkK/BSJcgwcnGndSTJgwAfXq1cOKFSsgSRImTZqEJ598\nEo899pjZoRERZSwmGkRE5cO991ZAvXoa7rzTBU0DPvjAinvvrYAFC87o2j5soqHk5eFMXh6k7t0h\np0DtRbAff/wR3333HWbMmIGqVasCAIYPH46RI0di3LhxqBbLbCJERJQ03s7g7KNBRJTeiopELF5c\nMpTt1VfLuOYa/S2Hohc3ORywLV8OAKhw112ofNFFsL71VuyRJtmuXbtQo0YN1K5d2/dYs2bNoCgK\nfvjhBxMjIyLKbKzRICIqH06fdv95nToFOJ36t486j4ajsBCnly2DddMmQFFwcutWVBo4ELLJk/Yd\nOXIEVapUCXgsKysLdrsdR48eNSkqIiLyJhrHUBV/opavhkOA5vvzvx9pWbh1RaiwQt/MtEREFJ87\n7nAhLy8bbdq4z7dffWVBQUGx7u2jJhqoUAFazZqwvvMOpIEDgcqVAYsl7oCTRRAEaMHzoQMhHyMi\norJjhQwAuA8zcR9mGnIMASqexX0Yg9mG7J8o08lPPQXr+PFmh0Emu/VWCZ07y/jmG3dn8MJCBdWr\n67/Wjp5oOJ2wz5kD67vvovixxyDu2QMcP55IzElRvXp1HDt2LOCxU6dOQZIk1KxZ06SoiIioG97F\nYCzGMbj7z3nrJhK57X+/GFn4EFfgC1xcps+LKJNoZ51ldghJpXbsaHYIYf3yyy+YPHky9u/fjy06\nJ8VeunQpZs2aheeffx5t27YFAOTl5cFqtUIUS5qt5ubmYtGiRbqWh9KvX0WsWnUaDRvKvse6dKmE\nzZtPhd3GX9RE48ysWbC//DLOzJ8PZGXB+t57KH7kEV07N1Lz5s1x9OhRHDx4EDk5OQCAnTt3wm63\nIzc31+ToiFKfa+1a2FNgoAfnn3/C4dfXioylXH01LBs3GnqMWvgLizHUsP0fQA7q4wD7gJQRrU4d\nCEVFZocRkfOU+6LHEcfw9tKsWbCNGZPskFKC0qcPLKtXmx1GSlDuvtvsEELatGkTZs6cidatW2P/\n/v26tjl48CCWLl0actncuXN9iUc8y71WrLChsNCB/ftFtGiR7XtckoDatfXXaEQ9S2sVKsB5991Q\n69WD8OuvkHr0gHruuboPYJQmTZqgTZs2mD17No4dO4aioiIsWLAA11xzDSpXrmx2eBSCMmiQ2SGQ\nP4cDasuW0ESTL9ays6OvQ0mhNWwI2O1mh5GwZI5qpWVlRVwu33tvwH31kksSPqbZ1KBJeKORFi9O\n+JjSv/6V8D4isljiatYtP/QQ1D59oq83ebL7/xNPQOndG1qdOpCfeipgHU1InVHWpLlzAQDKww/7\nHov2G6zVqBH4QAo9n6Qw+7cujNOnT2PRokVo37697m1mzJiBAQMGGBgVMGCAhM8+O4nrr5ewfv1J\n3997753Eli0npZsl7wAAIABJREFUde8n6qteuXdvVO7Tx/0/Px/Z7dqh0q23JhR8sjzxxBNQVRV9\n+vTBTTfdhMaNG2Ps2LFmh5VxnGfOQG3Xznc/3MlM7d+/rEJKCudff5XJceRRoyIuV1u0MObAggBp\n2zZImzcbs/8YuD77zLRjO//8E2qPHuGXnzkT8Kc1blyG0SWX68cfAy4ejLxojufiVFqypNRjzjNn\n4CwuhvOXX3yPxTKqlfNM5LHeXUeOuN/X+vVDLldvvDHg/Ze2bCn1mSi1jd/5MBUEx6g1aRLT9lr3\n7gHPV/afW0vn8dU77oj6XoQjP/FE1P3rpdWsGbC+MmUKkJMTcR/KLbdAmTgRzjNnoIwdC3nFCrh+\n+w3KjTf61lFbt4aaItdG8siRUO+6y/25btGi5H1bsiTs83SeOQPXgQNxv0fJFi0O5eqrY99piiZO\nffr0Qb169XSv//bbb6OoqAiDwlxrLVu2DNdffz06d+6MsWPH4uDBgzEt92exAM8/fwZnn635/ho0\n0GLK6aOepU988w1OfP21+//u3Ti5dSvkFGnnVrNmTRQWFuKDDz7Ae++9h4kTJyIrSulUptJstpCP\nR7rA0k0QIE+f7j5OvXpQxo6FZrNBnjAh8FgxZOvlhXbOOVHXUYcNc68bpvmQXFgILYml0PJjj0Gr\nXh3ahReafuKVH3wQAKBdcEGZH1u99FJoZ58NVKoEOUwBhdKvn/s18vuTZs0q40hjo7ZvD7Vr19AL\nBQHqnXf67srTpsV1DPmf/4wex1VXxbVvL61OHfc5xPsZ9fus6k001Lw893O+/PKAx6WXX4ZWuzbk\nhx8ueW/DxZGbG/gZ8MYS/Jj/cTt31vEMy1BQjMp990Gz26F26xZ1U1+Njt/zVcIUGml2OxTv+UwU\noQwdCrVJk5DvoR5apUrQHA4oN9wAtVMnKD17hl4xlv2Kovs53H471GbNAvahXnQRlIEDIRcUBGyi\n+L8G/sf0rwEQRSijR4f9rTWa7Fdz4Su5D/68+j8WzG+5NHMmtEqVoF55ZcxxxPJbFfGawC9OyTPF\nQoAQtRPSm28G7r9Vq8D7nTrpji1VHT9+HLNnz0ZBQQGs1tK9H1q0aIGWLVvi1VdfxYoVK6CqKsaM\nGQNZlnUtN0LM9Uhqs2awfPWVEbGQgdT+/eEsLj0cmeQ3J0pw6ab3ByMSZfhwAIB2xRVwFhfD9csv\n0Fq2hOvECSiPPBJYKlG1atgqe2WoMe251aCSO+fvv4d8HeLh+ugjOP0Hlw61zg8/QAkaCjr45K2d\nf777tfNrm+n/Q6ddeSVcSRyAQRk/Hq6DBwFvW+YYRmpL1mvni+WBB9w3srLg2ro1qfuORvrgA3cJ\nvyhCC1NrJL/2WqnHtB49oFx/fVJicBYXJ+U19d+H9N57kNatC7uuf42MFmehkfLMM9FXqlUL6qWX\nRlzF9fbbvtvqP/4RcHHh2rcPin9/wDgSDWXkSACA9P77kPw6O6oDB8K1fz8U/wtKVQ29kwoVIh6j\n1DH79AGqV49pm7Lgf5GlNWkC1/HjYRMGL+n116HMmFF6QZjSV9fx45DnzHGfz06fhjxvHqTvvos/\n5ltugevYMaBBA0hvvw056EJSL2dxcclvjOdzJL/wAqQvvwxYT/rkE8gvvwzl4Yd9301ncTG0li1D\n79higZKf79uv1qIFXCdOxBVjIlwbN0IpKIDmbTaeYBMhdcQIuP76C4hjYB3X8eP6z2lh+tOorVsD\nKDk/qn36lNz2LPN/jq7PPnMv80tEpdWrIW3bFvA+puL3MlazZ8/GlVdeiebNm4dcvmTJEtx2222o\nWLEi6tSpgwcffBB79+7Fzp07dS03QtRPo2P69IC/CiNHQgga7YlioyRY0heXcD+icCcUytVXQ376\n6cAFek5WsZ7QwqwfrkQ5XkrfvlBuvx2aX/KkXnRRUk40al4e1Nxc94+PjpI0ZcoUaPXrQ6tfv1Sz\nMrVDh4D70ksvQTv7bCh33w21VSvIjz7qWyZ7LppCxnThhZCnTo0cx5AhUK65pvQCM4eErlixJAzv\nD0iK0CLUDSuTJvluq02b6t+nXwmUVqtWfIEB0Bo1KtXmXbn7bqjdu/vuS2+8Aa1Bg5I4PT9MWqNG\nUFu0gPz44+7H27SJO45g6j/+EeMGJecl7fLLoebnQzvnnICkwKdOHd9NvX00VO9FIAD12mvdr1uY\nJl3SkiVhm09F423ypWVnQ/G05U+U5vfdSAZ5zhwACKzFiFSTY7GErxnzo/TtC6CkT0BSJbHGVR47\nFlr9+pBefDFp+wQAZepUaPXrQ3722aTuVw2X3ATRKlWClpfnueM5l+v8XZZWrAAAyPfcE3N8Ufe9\ncmXE5cpNN0H2O4/q5j1niCLkp56C2qwZNL+aKWn+fKjnnVfqt7U82L59Oz7//HOMGDFC9zY5OTmw\nWCw4fPhwXMu93n7bigUL3LVVe/eKMV02RB/e1v/HVhCgtGiB4ng+HOSjDhwIefVqOCI085JWrYKt\nX78kHjR8ouH9AQLcncGEv/9239Fxko+581u4E+B558FZXBzxNYmFvGwZAMDq1/9B+uST2HYS9Nzi\nLXnWmjeHa88e332bp9RFvfRSSO++G7CueuONcHna/UrbtgUsU559FuKuXRA/+KDUMaRPPwUAWKdM\nCRuHPH9+XPEbyv81djjg+uor2JN44atlZUGI431zrV8PLUKzAe0f/wj4PNjr1Sv53kSg3HMPrJ6m\nV67vv9cdjzJ8OCwvvFAS3+7d7ht+o6jIQU261F694OrVy/ed8o24YrNB+uIL33rS//4HAEn57kk7\ndoTcj3rllZDWry+9zD/RqFgRqFYNrh9+CL1zv3OHr0ZDsABhfvBcX30FVK1a8kC1aiWvWwhaly5w\n7dkT1+ugDhoEp38hgl9NTbyUceNgfeyxhPfjpV1wQelzWITzt+uUvqEr5WXLkMxGF87iYthzciAc\nORI5vi++gP3i6MMbS/PmuW+cd17AeThZtFatDNmvtHIlHDqalLq+/rqkwMb7fdL5u6z27p30Wmrf\nvqOMaCg//TTgcMSx45LnqIwaBSWoj6M6ZAjUIUNi328aWLduHY4cOYK+nuTe6/7770fPnj3Rq1cv\nrF27FuPGjYPg+Qzs27cPiqKgYcOG+P777yMuD2fKlCzs2SNi/34Rw4a58MYbNvz5p4CnntL32Yma\n9jonTIBz1ChIV18N6eqr4brjDmgxdFohN+Xmm6E2b+4uDdczq3qyS5l17s+/FEJXEqGnRL9PH18f\njlCJRqSS41ipnTuX/LAkgeRJwiRP4pIM8rPPQm3TBnIcJYDyzJLJz0KNca5VqVL6sfr1S42OErD8\n4ouhtm/v/oxedFHg8R58EGqHDlAvugiuDRtijhcA5KlTobZpA/XCCwMelxYsKB3L+edD7dJFV2mU\nqmN4PmnTppJ9V60KackSqO3aQXrjjYjbaTF25o22P3/yU0+5m1voHIZTs1gg339/6H09+iiU667T\nfWyzyaNGBTbJ9DsvKTFMDOZNNOS8ywIeVy+6CPKjj0Lt3Bna+efHFaNrzRqoLVtCbdYs7tJv5Y47\n3N8Zv6apMYtyznatWRNwX23SxP09u+yyMFuEkECNgXLDDZAjFGyE3e7uuyHfc0/k19bz3EP9BknP\nPw+1UydoubmQH3gAyi23BCz3fsakN9+EevHFUG+4IeYYk02ePDn2zssNG0Lt1g3SnDmlzi8BI6H5\nfU6kt95y9zMZPTpyPPfdB8Wvr5YpatQIeQ7UGjeG7FeoEkxetAjqhRdC8V5TlHPz58/Hk08+CQAY\nM2YMVq5ciVdffdX3BwAFBQUYPnw4atasiXXr1uGFF15AcXEx/vzzTxQWFqJ169Zo2rRp1OXhfPyx\nBa++ehrZ2e7P2gMPOPHNN/qv26LWaFjXrkWFceOg1q8PqCrEoiKcmT0bsl8VPUUnxzACSyJNKsLy\nnLA1mw2CJIU/9uWXQ6ta1d08ThCgVakCIVL/gGhJgiBA9uvIFaqjnBI0fGQipOD5ATyJjVa3blz7\nU4cNg1NHXxV/msUCQVHCL8/N9ZUix8q/ilgZObJUDYZy992w+o0Io152WfRRpWw2SO+957vrLdH1\nlnSFfybRub77DlqTJlA8nb6to0bBsnAhtNq1od52W+kNLBZIGzZAfO01iB99FHHf0scfRy191tq1\nK1Vip+oZZjnGDp1aDAMdKKNGAVFGGvMXqWTZ18dFD4ObyMkTJ5Z+0Ht+8HwPFU/Ca/Emmf4xxTCX\niq9Go0ZNSK+/DttNNwEoqbWM6XUJonXvDinR37caNWKqQdUcDghOZ0yH0Lp3hzJgACwrVriTIk9/\nA+uoUYCnhjOqBAp55H//O77t/Gvewl3sBn1u/Kl33ukb0EDxa1rqpfgVqqjhOo+XMcXz3bCEOV+F\nnNtGFCGtXeu7Ky1ZAtvgwe71Z8yAdfZs9wK/75DWsaOuz53iaTZpqqAkUqtfX1fNkHbhhb4a/PKi\nf//+OHToEBRFgaIo6OApaJs4cSIOHz6Mo0ePAgCqVKmCKiEKE6tXr+57fNasWZg3bx6WL18OQRDQ\nsWNH32istWvXjrg8HO/H1vuWKQoQS9/xqImGY+5cnPz4Y9/Fr3DwICrefjsTDQNp9eolfTQg7+gy\n0jvvwNatW8QLYd+JK8poLIC7ej+mOJ55BpY4Ssa12rUh/PlnzNvJEyZA+P57X3v0sqDeeSeEPXsg\nP/SQocdRhg2DuHlzQDtXZfTogERDNnrs+ihKDQWr93MdZj356aeBoiLfZ1SeNAnWOEdOChCc3MdT\npa+HCf1hXBs2wFpYCNVzMR4P5cYbAUWBRUfNjTJkCDRP6Zj8/POw3nNPqWZd0htvQFy0CFqco8D4\n+mho7n4YarduUHSMhJWq5AULYFm8GKL/gAg6PivytGkQDh2C7Nc5X37oIQg//ghl2DDYbr454vbq\nddcBnovXVCL997+wTpoUUy1XKpFefBG2WGoM9JwXgs6J0sKFEDduBCI0eTGKa80aCIcPw3bnnVBu\nvx2Wl18GAMhB75f03HOwRehb6C/awBHl2coI/Vl6RkmWtwU1sW7dujUWLlwYdv1oy0O55BIZI0dW\nwKFDAubNs2PtWhs6dNBfBBm9j4bNFlDCruXklIsJn1Ka3Z5QSVMwtUsXwDPMqnbZZZA2boQ9UqJY\noQJw/Li7VDfCCdD13XdAiOY7EcU4B4F6+eWQ3n8fQJxtyHNyIMXbXjrOZE+rVg1ynM2MYlK9euka\nnFq1IL30Emx33OGOxYRhYwMEl0gmeKGtBHVaVJKVaAS/10YN+xtiOEKjaV26QOrSJaF9yC+9BOHD\nDyMnGp7n5t8XSGvaFNI775RaVe3VC2qvXnHH46vRUAUgKyug5Dcdabm5kN55J/Acp+e7cvbZpV/f\nevXc57wDB6JvH+OIWmVFa9cu/vN2ClBvvDF8bU28gmpZ1VtvNW3eDq1NG2i1a8N5003uAghPoqEE\n9SlSBw8GdCYaqTqZHgEPP+zE6tVWVKyo4cABESNHOtG7t/4qjegzg1eqBPvcuRC/+w7id9/BPnt2\nyRBqZAh5yRKoXbtCue46KFFKpPSQYuwELK1eDaVnz5hrK+KWhFJerWpVSDFU5SsJXOREZfLcFGrf\nvlD69Im7T4U0cybkMWOSHFWQFJ04yWjKfffFva3Lk1RKSR7dJupxPc3qtMsvh9K/f6mx6r0UvRcU\nCfD2v1IfcJecRhjjIr2E+j4k+uR0fseUEBersv+wwhS7WM9v1aoF3A01T43aq5f7vL5+fSKRmU49\n/3y4QhQ+xNovjspWly4yxoxxYsQIJy68UMH+/fo/41GL187MmwfH44+j4ooV7vb2eXk4k8TOthRI\nHj26pNnB669D+PxzWEKM5a9XPCNKaG3alIxXbtIFoXLNNbCsW6e7w6zrjz9i2r+WlwcEdaYsN7Ky\nAvrFxEqNYeg8L3nKFFijDK8bIIbkUm3aFOKPP8YcU6qR5syJa1x6L61zZ8NGiIl4XG//E4sFsqfz\nYUhBF0tGUIcOhXPoUGgSgMJylGiEqukqo2Z2oQZ3CTtvBOmmDBkCy+LFuoepDdg21Dw1DkdC5/Wk\nSuC6QH7jDfckmMFYo5Gyxo3LwtKldtSsWdJkVRCA777TN2dM1ERDq10brnvuQfF55wEAxK+/Nqaz\nMrkF/7gYcaGfwqXJ0pw5EA4fdo8kkpVlXN+KCE3TpFdegfDFF/E3K0jh19cwIS6AtUTmLPF7DeUV\nK2Bv0ybiyFnJ5D/3RHJ3bOJ8JSlMmj8fwv/9X0zbeD8e5eUl9R/koeTBMnpyoWY279GjbI4dRE6g\nE3+qkadMAY4dS9q8KiklkWvA8vKlzSCffmrF3r3HEe8I6NE7gz/2GMRDh3DG0/zGMWsW1HPOgZNV\nq8YILqIzolNqLH1syuKi2W9iKtVvhKdQszInixahpkQdMAAYMCD+nWdi08IQiZv/TOelxPC50ho0\nKJOSfLVdO4iffw4t1knn9CpnP7ABc+4kIJ4x772Fn19+KaBnT3fbdf+xKwJvaxGWRb596aUa7r03\nkXHXdAr1fdBZmxtWIv38Yhx1LVmUGEf4S2l16kSuAfSXRucG6aWXzA6ByliLFgokCcYlGtaPPsIp\nv05ZZ158EZWuvhqxDcRHugWdcLQWLZJ/iHbtoIwY4ZvR1Szy+PEQiooSHt42nnld1H79gCQOq+vP\n6Lbq8uOPQ/ObJTkVKCNGwGpU7VOcP8Ku7duTHEiC4mjn4/rqKwMC0S/cLNpmEwSgbVsV27eLeP99\n4wpD1qwxJtFQL7kEwsGDEPbtC7uOMnp0yAk45fHjoV1ySfSD1KkD+f77Yxp62XRpdMGdKaRFi2Ab\nOtR3Xw2eSNhigTxxIpvblWNXXy2jTZtsNG2qBrTyXLNG36Se0YdAcbncf95S8JMngQjzMFBihDJq\nOuU/8VskWk4OBM8YzsmmXXRRUiYbUwYOjH0jg0ZOU4YPN7xGI5EOxYYJMbZ3qM+u5mnHH2qywbDi\nvPjQmjePazvD+NXc6aVFmETJaGqnTlCTMBiFEQQB+OgjCYri/nh4PyKx3I623vXX27BtmzFJjLRl\nC4DwI+nJ06aFbboZPLJPJIqeEdkysakn6aY1ahT4QKi5sBJoHqZVqgTh1KmIrQzIXFOnZuGxx4pR\nv358neKiJhquO+9E5bw8KBdeCEFRYNmxA8UTJsR1MNIhiSU6umb2jkJauRKOJF+wuf73P4ivvZbQ\n8JZeyjXXQInn82hUx7MM+NGWVq+G8PXXsMbx46KMHw/h5EnI0Sas838d4/hOyE8/HfM2iZJmz4Yt\nQi2ZrokCU0m0z7LJpc+CYOxowXY7oGmCr+OjEaSVKyH89JMxO9fLxHOW68MPYe/Y0bTjkw7J/nwE\nnTekrVshLlmSsoUaBOTmKhg0KP4Khqinaem22yB36QLLjh2AIEB5/HFo2dlxH5Ai0xIYlcYQ550H\nZfBgWJYsgWa1QohlOsgwtDZtoLRpk4TgAPnZZ4GqVWPfMIknT00QfDVRWhyl1ulGveoq4Kqr4ko0\nUKWKvtq0BN+f4Pk29PA2wYuptsWPOnw4tMcfhxBuBLTyNv9QOW/m4i2LMDLRUK+91pgdpwmtXTto\njRpB2LvX/YBJfUOoDAU3D2/ePPQoW5QymjZVcffdFXDppXJA169bb9WXfOgq1hVOnYJWqxa0mjVh\n+eknVOas4IZRokwFH4tSzbDiJE+bBvn++yE/91xS9pcSknjl4P86m9nUpdyK8XOs3HZbXIeR586F\nPG4c5CefjGv7YNKCBYnvpAxLm11pPuldsnkTjVQaQleaMyf5OzW5Fta1bl3Jnbp1zQuEQivnBQoU\n3V9/CRBFYNs2Kz79tORPr6hrZj34IKybN0P84w8ojRtD3LsXrmjNHih+SWzfryZr9JwaNaBMmwYx\naKZWzYgRscqKQTUaZv9olxv+r2OMo+fEnezVqQNl+vT4tvUeu0kTX42GetttQDqNopOTE3BXa9gw\n4ura+edD+PxzIyMylRmJhnbOORB++y1szbbWqlXZBVNWGjc2OwLzZdLFfCY913LiuefOJLR91BoN\ny44dOLltG5SWLXFq82ac+u9/gTOJHZTcNINLb6StWw3bt/zEE0CUC5GUxoTAeMl6jWNtqmnieyvp\nHc4yFfm9bsqgQZALCyOuLi1bZnREpjIj0XBt3Aj5kUeg3nJLyOW6RpuKVQqcC6WFC+HauNHsMEwl\nzZpldgihpcDng8xx553uASmaN89Gixal//SKXvfhbVfscgGaBrVNG1gnTeLwtkmgNWgA4dAhQ/at\ntmhh6OhHyWzilZB4B3ZO5slTEEpKaXhSToqEJgU1s8QsqFYgXSkTJwLRJlysXx/KjTfCsmxZ6ZFp\nygFRdH+OyrTpVKNGkQe3KKfnF/XWW80OwVTaOee4O0OPGWN2KIbLhH6M5cWMGe75qzZsOFlq2enT\n+s9FURMN5fzzYV+0CPLll6NS375Qzj8fwrFjMYRKpsiU6sk4O+4mddQp/9e6nF4IlDWtSxezQ6AM\nl4p9NAzBc5bplIceAipVgrR4sXEThqYA+emn2VQujdSp4762GTu2AlatOh2wrEuXiti8OUnzaBTP\nnAnh6FFoVatCXbUKQlERTqVKaXa6MzIZyJREI178cU1tfH9MpbfUUTv/fACAWg77DmRMokGmUu6+\n2zcLfEoO8VqjRtJ2pfhN/Eepb8UKGwoLHdi/XwxoKiVJQO3a+q8xozedEgRonip06YYbYo+UzGFA\nopGMeTkSIa1aBVvwrKTxMuq58AI547neess35LIyYAAsK1ZACzP5WijyuHGweod7NOvz5BnqNxpl\n3DhodepA7d/f4IDKXsYkGjxnmUrWM6miibTc3OTtLN6mzmSKAQMk9Osn4Z57KuChh4p9j4sikJOT\nzESD0pJ29tlmh5B0apLm3gCQ3FGnWrSA8O23Sd9vutAaNDA7hADaOeeYe/wePXy31SuucCcal12m\nf/v69Y0IS8eB4yicyMqCetddyY8lBWRMohFEveACs0PILAb2pSRKlMUCPP+8waNOUXpKxqzbpZSn\ni+gkPhflgQcM2W+qc37/PZRrr4Vrx47QK5TxayFPmwbppZegJqvWKwnUO+6AtGQJpNde078RJy1L\nCRmTaAR9T6V33jEpECIqj1ijYaZ0uyj1jKhj9LC8YSWzOVgyazTOO6/ktsml6WXq3HMhr1zpvl1c\nHHndMqDVrQv1xhvNDiOQ1Qp10CCzo9An3c5HBsuYRMOP/Oij8Q+wQUQUAhON8sqIPhotWkBavRpq\n69ZJ33eq0OJoQ6pddBGkRYsAWYbWvr0BURFRWfPmXZddZofV6r7vfcx9Wwu6H3jbe1/EtxCgQYD7\nnCxAA/LctVZVqwIvvijB1NaH/vOncKAXIkoyJhopQKtYEcLp01Dz8swOJSr1qqvMDiElhZtgi4jS\nU36+is8/F6EogKIElt1oGqBpgu8x9/0wt+GpCfakGwCg/SbA6QSKiwV8+qmIG25IkWoTNtsjoiRj\nopEC1I4doYwZA+2ii+La3rl7NxzNmiU5qszh3LkTjubN3XfYfCR9lZchnQUBzp9+glBUZHYkGe3m\nm1XcfLMr4f04skpPPun8oxgLFogYPdpm/seW5zwiMhA7g5vJ7xdG69LFNyRmzELMypvUIelSlNqy\nZXJ25NfHgiglNGwIrW1bs6MgA3mv7yP1AVGuuabsAiEiMgATjVRgwIle69gx6fs0nelFf6SXa/t2\nXsAQRaCns7n86quQXnjB2ED4PSUiAzHRoPTFxCNlad6maBQ7ky78tDp13P8bNjTl+JnGm2hEPI1V\nqBDTHCyUXjSdE2OmiowaVZGShn00KH0wsSAyTq1acH39ddpd/KQr3cPnssah3HJ99ZXZIcTE9cUX\nZodAaYg1GpS21OuvT/o+lVGjkr5PMpa3HbtWjoddLivaBRcA2dlmh5ER9CYaWu3axgdD5qhSxewI\nYsNzA8WBNRqpgCX1MXN9+y20Jk0S2ofz0CHfRHPOY8cgbN0KrVu3ZISXeQz4DDv37wcslqjrycuW\nQd6/H2jcOOkxEBlFT2dwAEC1anBt2QItxKAfSQ2EiMgATDTMxBN83LTzz098J9Wqldx2OKB17574\nPil59Jbk2mxMMihtePvA6Oqj4d3mkkuMC4i/Q0RkIDadKodc27ebHQIRxYsXfuWa/PDDAGLoo2E0\nft6IyEBMNMxkUJMprVIlQ/ZLREQJ8mQYsdRoEBGlKyYaqSAJJUrOP/5IQiBEiVMM6KRPVG54zvcp\nU6NBRGSglO2joWkaXnnlFaxevRp//fUX6tWrh9tvvx1XX301AGDq1KnYsGEDrNbAp/DKK6+gcSa2\n1453VvF04mmzr3btanIgVIrdbnYE5QebsmQE79vMGg0iKs9SNtFYvnw5li1bhpkzZ6JJkybYtGkT\npk6dinPPPRe5ubkAgPz8fEyZMsXkSFNQeb1QqVABziNHgKwssyOhYBYLtEaNIOzda3YkaUmrXx/C\n77+bHQaVIdZoUCZQeveG5a23zA6DTJTSTadGjx6N3NxcWK1W5Ofno06dOvjyyy/NDiulqHl5ZodQ\ntipUKL+JVJqTPUm/MmSIyZGkN7VzZ7NDoDJQkmiYez5TPfPQEBmCv9cZL2VrNG688caA+6dPn8ax\nY8dQp04d32M///wzhg4dij179qBGjRoYOXIkumZYsxq1f3+zQyACAKg33ghn//6ANWVPKylPGTgQ\nMGq+BEop3kRj3ToRRUUl12OCEN/tUMssFqB3bwXnnRc+Du2CC5L3pIiCMdHIeGlxRaBpGh5//HHU\nq1cPnTp1AgA0aNAATqcTI0eORK1atbBu3TpMnDgRCxcuRKtWrUyOmChDMclIDJsFln+eC6+zznJ3\nzti8WcTmzcY1LtixQ8C//y0btn8q/9Ru3SCwNQnFybSrgu3bt2PEiBEhl7Vr1w7z588HALhcLjzy\nyCP44YcWm4ZgAAAgAElEQVQf8Nxzz/k6fw8Jap7Rr18/bNq0CWvWrGGiQUTpiT2DM0ZenoavvnLh\n77/d9zWt5O333o5233s78L8ATQNOnQJuvtmGM2fK5vlQ+SWtXRv/uYk1GhnPtESjbdu22LZtW8R1\nTp48ibFjxwIAFi9ejGr+MzmHUL9+fRw+fDhpMRpNfuwx2Hv2hPLgg2aHQkRm4o9xRsrNNSKxdO/z\nxAn3PXY2p6TgOYrilLKdwSVJwn333Yfq1atj/vz5AUmGqqqYOXMmdu3aFbDN3r170bBhw7IONW7a\nlVfCWVwM7bLLzA6FiIjKEY5qRUSpIGUTjaVLl+L06dOYPn067EFj9IuiiD/++ANPPvkkDhw4AKfT\nieXLl2P37t247rrrTIqYiChBbDpVrmk5OVDz88vkWJx5nFKBOnCg2SGQyVK25+bq1atx8OBBdOnS\nJeDx/Px8FBQUoKCgAHPmzMHQoUNx4sQJNGrUCPPmzUMjjthCROmGzRIygqsM55lJleFzKbNpZ51l\ndghkspRNNN58882Iy7Ozs1FQUFBG0aQH+ZlnIL75JtCggdmhEBGRiWJpOqX06gWtRQtjA6LMxEKU\njJeyiQbFTvnnP6H8859mh0FEsfL+GLOdCyVJLImG/MYbxgZDRBkrZftoEBFlDCYa5ZbaoYMpx/V+\npNgZnIjMxEQj3Yl8C4mIUpXavr0px2WiQUSpgFepaU65/XazQyCiZGGNRvljUht1QQAEQWOiQUSm\nYqKRxrQaNYDsbLPDIKJEscMkGUAUWaNBROZiokFERFQOiSIryYjIXEw00hlLQYnKF14VUhKxRoNM\nZ7OZHQGZjMPbpjNelBCVDyw0IAOIIvDTTwJuvbXkp977UXP34Qh/W+96rVurGDaM2QyFpl10EZTh\nw6H06WN2KGQSJhppTKtRw+wQiIgoReXmavjySxFvvGEx7BiCIOL2211wOAw7BKUzQYA8e7bZUZCJ\nmGikMXnWLLNDIKJkYi1l+WNibdXWrRL+/tv9sfJ+tGK57X/fS9ME3/0RI6z46CMRimL8cyGi9MRE\nI53l5JgdARElA5tOkQFsNuCss5K915KsIzvbfZv9QIgoHHYGJyIiophxQnsiioaJBhGRybRGjdz/\nWUtZ/lSoYHYEhhE9VxCs0SCicNh0Kp2xuQVRuSAtXgzLggVQxo41OxRKMq1BA7NDMAwTDSKKhokG\nEZHZ6taFMnmy2VGQAbRatcwOwTBMNOKncZguyhBsOkVERGQQrXt3s0MwjLdSnYlGbJRBgyBt3252\nGERlgokGERGRUcpxE1fWaMRHmTgRWpMmZodBVCbYdIqIiIhi5k00OOqUPq5vvoGwbRuTDMooTDSI\niIgoZmw6FRutaVNoTZuaHQZRmWLTKSIiIooZEw0iioY1GmlMq1rV7BCIiChDsY8GUXL88ssvmDx5\nMvbv348tW7bo2mbp0qWYNWsWnn/+ebRt2xYAkJeXB6vVClEsqUfIzc3FokWLAABFRUUoLCzEt99+\nC1EUkZeXhwceeACVKlVK/pPyYKKRzurXNzsCIiLKUEw0iBK3adMmzJw5E61bt8b+/ft1bXPw4EEs\nXbo05LK5c+f6Eo9gEyZMQL169bBixQpIkoRJkybhySefxGOPPRZ3/NGw6VSaUnNzzQ6BiIgyGDuD\nEyXu9OnTWLRoEdq3b697mxkzZmDAgAExHefHH3/Ed999h3vvvRdVq1ZFrVq1MHz4cLz77rs4evRo\nrGHrxhqNdMUzOxERmcibaLz7roi6dUv6bAhCyZ//ffdtLWDE39DrlNyuUgW48EKtPI8STBmuT58+\nAIAdO3boWv/tt99GUVERBg0ahHnz5pVavmzZMkyfPh1///03LrzwQjzwwAPIycnBrl27UKNGDdSu\nXdu3brNmzaAoCn744QdccsklyXlCQZhoEBERUcwqVXIXeI0aZTP0OKtWSbjmGrbPIjp+/Dhmz56N\np556ClZr6Uv4Fi1aoGXLlpg6dSpOnjyJ6dOnY8yYMXjttddw5MgRVKlSJWD9rKws2O121mgQERFR\nahk/XkGTJhpk2V3doGklf5Hu6133228FvPWWBUVFZfikiFLY7NmzceWVV6J58+Yhly9ZssR3u2LF\ninjwwQfRt29f7Ny5E4IgQAvRGibUY8nERIOIiIhilpMDjBxpXE3D0qUi3nrLws7mRAC2b9+Ozz//\nHK+//rrubXJycmCxWHD48GFUr14dx44dC1h+6tQpSJKEmjVrJjtcHyYa6Yp9NIiIqBzjqFZEJdat\nW4cjR46gb9++AY/ff//96NmzJ3r16oW1a9di3LhxEDydmvbt2wdFUdCwYUOIooijR4/i4MGDyMnJ\nAQDs3LkTdrsduQYOMMREg4iIiFKOyHExKcPNnz8fJ06cwIQJEzBmzBgMHz48YHmvXr1QUFCAvLw8\nOJ1OrFu3DpUrV8Ydd9yBEydOoLCwEK1bt0ZTz4z0bdq0wezZs/HQQw/B6XRiwYIFuOaaa1C5cmXD\nngMTDSIiIko5rNGgTNC/f38cOnQIiqJAURR06NABADBx4kQcPnzY11G7SpUqpTpzA0D16tV9j8+a\nNQvz5s3D8uXLIQgCOnbsiLFjx/rWfeKJJzBjxgz06dMHFosFXbt2DVhuBOHo0aMZ1QYnKyvL7BBi\n5ggRs9q0KaRvvjEhGiIiioX3HO4sLjY5kvSyapWIm2+2YeZMCSNGMNug9FSc4d97VkwSERFRyimp\n0eAkGkTpiolGmlIHDzY7BCIiIsNw5nGi9MdEI00pY8aYHQIREZFh2EeDKP0x0SAiIqKUw0SDKP0x\n0SAiIqKUw0SDKP0x0SAiIqKU45lzjH00iNIYEw0iIiJKOaLozjBYo0GUvlJ2wr6pU6diw4YNsFoD\nQ3zllVfQuHFjaJqGJUuWYP369fj777/RuHFj3HvvvWjVqpVJERMREVGyeGs0mGgQpa+UTTQAID8/\nH1OmTAm57D//+Q9WrlyJWbNm4ZxzzsGqVaswduxYrFy5EtWrVy/jSImIiCiZ2EeDKP2ldKIRyapV\nqzBgwABccMEFAICbb74ZK1euxMaNG3HTTTeZHB0RERElwptoTJ9uQWGhxVfDIQglf/73Iy0Lt64o\nAmPHKhg1SimbJ0WUYVI60fj5558xdOhQ7NmzBzVq1MDIkSPRtWtXOJ1O7NmzB/fcc0/A+rm5udi1\na5dJ0RIREVGyXHSRhquuUvDXXwI0Db4/IDm3NQ34+WcR69eLTDSIDJKyiUaDBg3gdDoxcuRI1KpV\nC+vWrcPEiROxcOFC5OTkQFVVZGdnB2xTpUoVHDhwwKSIiYiIKFmqVgVWr5YN27+qAhUrOjiqFZGB\nTEs0tm/fjhEjRoRc1q5dO8yfPz/gsX79+mHTpk1Ys2YNhg8fDgDQeHYgIiKiOLCzOZHxTEs02rZt\ni23btsW0Tf369XH48GFUqVIFFosFx44dC1h+9OhR1KxZM5lhEhERUTnk7q+hMdEgMlBKzqOhqipm\nzpxZqr/F3r170bBhQ9jtdpx//vnYvXu3b5mmadi1axeHtyUiIiJdRJE1GkRGSslEQxRF/PHHH3jy\nySdx4MABOJ1OLF++HLt378Z1110HAOjfvz9WrlyJ3bt3o7i4GK+88gokSUKPHj1Mjp6IiIjSgSAw\n0SAyUsp2Bi8oKMCcOXMwdOhQnDhxAo0aNcK8efPQqFEjAEDv3r1x5MgRjB8/HkePHsUFF1yAWbNm\noXLlyiZHTkREROlAFMHO4EQGEo4ePZpRX7GsrCyzQ4iZI0TMzuJiEyIhIqJYec/hPG+nnmrV7GjZ\nUsOHH0pmh0LlVHGGf+9TsukUERERkdHYR4PIWEw0iIiIKCMx0SAyFhMNIiIiykhMNIiMxUSDiIiI\nMhITDSJjMdEgIiKijMREg8hYKTu8LRERUXkgvfoqhL17zQ6DQhBFwOkEDh5033fPFq7vtt71HA7A\nYimb50OUaji8bRrg8LZERETJd+65dhw6JBh8DA3ffOOC3W7oYShFZfrwtqzRICIioow0daqM994T\nAybt07SSv0j39az71Vcifv1VwNGjQJ06ZfOciFIJazTSAGs0iIiI0s8tt1ixcqUFv/7qRN26ZkdD\nZsj0Gg12Bk9D8uOPmx0CERERReHtr6FlVJEuUQkmGmlIuekms0MgIiKiKLyJBke2okzFRCPNSHPn\nAjk5ZodBREREUYieqywmGpSpmGikGfWuu8wOgYiIiHRgokGZjokGERERkQG8iQb7aFCmYqJBRERE\nZAD20aBMx0SDiIiIyACs0aBMx0SDiIiIyAAlfTSMnX2cKFUx0SAiIiIyAGs0KNMx0SAiIiIyAPto\nUKZjokFERERkAA5vS5mOiQYRERGRAZhoUKZjokFERERkAFF0d85gHw3KVFazAyAiIiIqj7x9NIYP\nt6JSJfd972Ohbpe+r0Vdt2JF4LHHZJxzThk9KaIYMNEgIiIiMsDFF2uwWjXs2GFsA5JLL1UxciTb\nZ1HqYaJBREREZICbblJx000uX9MpTUPE23rX895et07E4ME2KArn6aDUxESDiIiIyED+zZ6SKTvb\n/Z+dzSlVsTM4ERERURrihICU6phoEBEREaUhDp9LqY6JBhEREVEa4szjlOqYaBARERGlIe88HUw0\nKFUx0SAiIiJKQ94aDfbRoFTFRIOIiIgoDbGPBqU6JhpEREREaYiJBqU6JhpEREREaYiJBqU6JhpE\nREREaYh9NCjVMdEgIiIiSkOs0aBUx0SDiIiIKA0x0aBUx0SDiIiIKA0x0aBUZzU7gHD69++PQ4cO\nBTymKAry8/MxefJkTJ06FRs2bIDVGvgUXnnlFTRu3LgsQyUiIiIqc95E4+uvRSxapPn6bAgCEr7t\nf799exX16hn/fKj8SdlEY+XKlQH3z5w5g4EDB+Kqq67yPZafn48pU6aUdWhEREREpqtWzf1/0yYR\nmzYZ10ile3cVa9ZIhu2fyq+UTTSCvfDCC2jevDkuueQSs0MhIiIiMl3Tpho2bHChqEiAppWMPuW9\nHXw/ltteY8daceyY8c+Fyqe0SDT279+PN998EytWrAh4/Oeff8bQoUOxZ88e1KhRAyNHjkTXrl1N\nipKIiIiobHXpogEwbnzbBx9kHxCKX1okGosXL0Z+fj7q1q3re6xBgwZwOp0YOXIkatWqhXXr1mHi\nxIlYuHAhWrVqZWK0REREROWDKDLRoPiZlmhs374dI0aMCLmsXbt2mD9/PgCgqKgIb7/9NpYvXx6w\nzpAhQwLu9+vXD5s2bcKaNWuYaBARERElgSAw0aD4mZZotG3bFtu2bYu63vvvv49zzz0XZ599dtR1\n69evj8OHDycjPCIiIqKMxxoNSkTKz6OxZcsWtG/fPuAxVVUxc+ZM7Nq1K+DxvXv3omHDhmUZHhER\nEVG5JYqAopgdBaWrlE80du/ejSZNmgQ8Jooi/vjjDzz55JM4cOAAnE4nli9fjt27d+O6664zKVIi\nIiKi8oU1Gsb75ZdfcMstt6BTp066t1m6dCny8vKwfft23cvz8vJw+eWXo0OHDr6/oUOHJhx/JCnd\nGfzUqVM4ffo0qnkHivZTUFCAOXPmYOjQoThx4gQaNWqEefPmoVGjRiZESkRERFT+MNEw1qZNmzBz\n5ky0bt0a+/fv17XNwYMHsXTp0riWz507F23bto0r1nikdKJRqVKlsP04srOzUVBQUMYREREREWUO\nJhrGOn36NBYtWoQdO3bgk08+0bXNjBkzMGDAAMybNy+u5WUp5ZtOEREREZE5mGgYq0+fPqhXr57u\n9d9++20UFRVh0KBBcS1ftmwZrr/+enTu3Bljx47FwYMH44pbLyYaRERERBSSO9EQzA6DABw/fhyz\nZ89GQUEBrNbSjZKiLW/RogVatmyJV199FStWrICqqhgzZgxkWTYs5pRuOkVERERE5uE8Gqlj9uzZ\nuPLKK9G8efO4li9ZssR3u2LFinjwwQfRt29f7Ny5E61btzYkZiYaRERERBQSm06lhu3bt+Pzzz/H\n66+/HtfyUHJycmCxWAydg46JBhERERGFJIrA4cPAxIkWAO4aDsGvJZX/fe/t4PuRlgkC0KiRhgED\nmM1Esm7dOhw5cgR9+/YNePz+++9Hz549cebMmYjLe/XqhbVr12LcuHEQPG/Cvn37oCiKoXPQMdEg\nIiIiopDq1tXw668inn3W2EvGDh2ciKFPdEaYP38+Tpw4gQkTJmDMmDEYPnx4wPJevXqhoKAAeXl5\nABBxudPpxLp161C5cmXccccdOHHiBAoLC9G6dWs0bdrUsOfARIOIiIiIQlqzRsL33wvQNEDT3I8l\n47b3/qxZVrz7rogzZ8rm+aSa/v3749ChQ1AUBYqioEOHDgCAiRMn4vDhwzh69CgAoEqVKqhSpUqp\n7atXr+57PNryWbNmYd68eVi+fDkEQUDHjh0xduz/t3f3MVXX/R/HXwfwcAo9gCCay4uVxU1MqMib\na22NNNQua9lUnHdRWhmtQEzNm9XWutbQyonILBfLtOnmDerSUMhZbWXezhAONwnTQalkcUA0PHI4\n1x/G6SJIrl+/c/h6Ds/H5uR83nDO5/t984Xz4nw/35PtrU2TJJnsdrvLq49wi7FYLEZP4f8s+Pc5\nuyIj5aivN3g2AAAAnvHSS0HauDFQ5eXXNHy40bPxvNbWVqOnYCgub+tD2h991OgpAAAAeEzHmg0u\noeufCBoAAAAwRMDvz0S5spV/ImgAAADAEAQN/0bQAAAAgCE6goarT60Y7jsIGgAAADAEr2j4N4IG\nAAAADBEQcOOlDIKGfyJoAAAAwBB/XHXK2HnAOwgaAAAAMARrNPwbQQMAAACGYI2GfyNoAAAAwBAE\nDf9G0AAAAIAhCBr+jaABAAAAQ7FGwz8FGT0BAAAA9E0dr2hUV5t0221/XIXKZOr68c1qf3z8R2Lp\nGL/tNumOO7y8IegWQQMAAACGCA6+8f+LL/bz6uN88sl1TZ/O+Vm9jaDhSzqiOQAAgB9IT3fqt9+k\n1tYbt12ujn+mP93uXL9Z7b9v//STSQcPBujsWZ5DGYGgAQAAAEPceaf07387vXb/JSUmHTxo9tr9\n4+ZYDO5DXP/4h9FTAAAAAP4nBA1fEhVl9AwAAACA/wlBAwAAAIDHETQAAADg13ifDmMQNAAAAOCX\nuGCnsQgaAAAAADyOoAEAAADA4wgaAAAAADyOoAEAAAC/xmJwYxA0AAAA4JdYDG4sggYAAAAAjyNo\nAAAAAPA4ggYAAAAAjyNoAAAAAPA4Q4PGtWvX9N5772n06NE6ePBgl/quXbs0ffp0paSkaPbs2fr6\n66/dNZfLpYKCAk2ZMkWPPvqo5s2bp9LS0t6cfq+5Xlio9vvuk3PmTKOnAgAA4DMSElx64IF2/fOf\n7UZPpU8yLGhcunRJ6enpcjgccnVzzbHDhw9rzZo1Wrx4sYqLi5Wenq5ly5aptrZW0o0QsmPHDr3z\nzjsqKirS2LFjlZ2drcbGxt7eFK9r/9e/dP3kSSky0uipAAAA+IwhQ6TDh68rJYXr2xrBsKDR1NSk\nefPmafny5d3Wd+7cqQkTJuihhx6S2WxWamqqkpKStHv3bnc9LS1NsbGxslgsmjVrlqxWq/bv39+b\nmwEAAACgG4YFjeHDhys1NfUv6xUVFYqNje00FhcXJ5vNpmvXrqmmpkZxcXHd1gEAAAAY65ZdDN7Y\n2Cir1dppLDQ0VHa7Xc3NzWpvb9eAAQM61a1Wq+x2e29OEwAAAEA3grx1xydOnFBGRka3tZEjRyo/\nP/+mX28ymbqs3ei4bfr9bR67W9sBAAAAwHheCxrJyck6evTo3/768PBwNTU1dRqz2+2KiIiQ1WpV\nYGDgX9YBAAAAGOuWPXUqISFBFRUVncbKy8uVmJgos9mse++9t1Pd5XLJZrMpMTGxt6cKAAAA4E9u\n2aAxdepUlZSU6OjRo3I4HCoqKlJlZaUmT57sru/YsUMVFRVqbW3Vpk2bdP36dY0fP97gmQMAAAAw\n2e12QxY6FBQU6OOPP5YkORwOBQUFKSAgQA888IDy8vIkSXv37lVBQYEaGhoUHR2trKwsjR492n0f\nn3zyibZv3y673a7Y2FgtXry4y5Wo/sxisXhvowAAAIDftba2Gj0FQxkWNIxC0AAAAEBv6OtB45Y9\ndQoAAACA7yJoAAAAAPA4ggYAAAAAjyNoAAAAAPA4ggYAAAAAjyNoAAAAAPA4ggYAAAAAjyNoAAAA\nAPA4ggYAAAAAjwsyegK9ra+/QyMAAADQG3hFAwAAAIDHETQAAAAAeBxBAwAAAIDHETQAAAAAeBxB\nAwAAAIDHETQAAAAAeBxBAwAAAIDH9bn30ehtDQ0NWrVqlU6fPq2AgACNGjVKS5YsUUhIiNFTg6SL\nFy8qNzdXJ0+eVFtbm0aMGKEFCxYoOjq6x97V1tZq9erVqqys1O23366UlBRlZmYqKOjGYXXq1Cmt\nW7dONTU1Cg8P16RJkzRv3jz3Y3/55ZcqKChQXV2dBg8erBkzZmjy5MmG7Ad/tmXLFq1Zs0br169X\ncnIyffUTW7du1datW9XY2Ki7775br732mhITE+mvDzt79qxyc3N1+vRpmUwmxcfHKysrS8OHD6ev\nPqi2tlZvvvmm6urq9NVXX7nHW1pa9O677+r48eNyOBxKSkrS0qVLFRkZKann50302rfwioaXLV26\nVBaLRdu2bdPmzZt18eJF5eTkGD0t/G7RokWSpG3btqmwsFBms1nLly+XdPPeORwOZWdnKyYmRnv2\n7FF+fr6OHDmiDRs2SJJ+/fVXZWdna9y4cSoqKlJOTo62b9+u3bt3S5Jqamq0YsUKPfPMMyouLtai\nRYu0evVqHTlyxIC94L/Onz+vLVu2dBqjr75v9+7d2rp1q1atWqWSkhI99thj+vDDD9Xe3k5/fZTL\n5VJ2draioqK0d+9effbZZ7rjjjuUnZ0tl8tFX31MSUmJXnnlFQ0bNqxLLScnRw0NDdq4caN27typ\n4OBgLV261F2n1/6FoOFF1dXVKisrU1ZWlkJDQxUZGan58+friy++kN1uN3p6fV5LS4tiYmKUmZkp\nq9Uqq9WqtLQ0/fDDD6qqqrpp77799ls1NzfrpZdeUkhIiIYNG6b09HQVFhaqvb1dBw4cUEREhGbM\nmCGLxaKYmBhNmzZN27dvlyTt2bNHDz74oFJTU2U2mzVy5Eilpqa66/CMlStXKi0tzX27p2OSvvqG\nTZs2ae7cuYqLi5PFYtGcOXOUn5+vM2fO0F8fZbfb9eOPP2rixImyWCyyWCx6/PHHdeHCBVVWVtJX\nH3P16lV99NFHevjhhzuN2+12HTx4UBkZGRo0aJCsVqsyMzNVWlqq6upqfkb7IYKGF9lsNg0cOFCD\nBg1yj8XHx8vpdKqqqsrAmUGS+vfvrzfeeENDhgxxj50/f14hISEqLy+/ae9sNpvuuusumc1mdz0u\nLk7Nzc2qr6+XzWZTbGxsp8eLi4tTTU2Nrl271m09Pj5eFRUVXtravufAgQNqaGjQzJkz3WM9HZP0\n9dbX0NCg+vp6uVwuzZ49W2PHjtXLL7+ss2fP0l8fFh4erhEjRmjPnj26fPmyWltbtW/fPiUlJamq\nqoq++pinnnpKQ4cO7TJeVVUlp9PZaX8PHjxY4eHhstlsHMN+iKDhRY2NjbJarZ3GLBaLzGYzr2jc\ngi5cuKB169Zp7ty5ampqumnv7Ha7BgwY0Kne8fk3q7e3t+vy5ctqbGzsts73hWc0NzcrNzdXK1as\ncJ+3K/V8TNLXW19DQ4Mk6fPPP1dOTo4KCwsVFhamhQsX6tKlS/TXh+Xk5KiiokLjxo3TI488ou+/\n/15vvfUWx60faWxsVHBwsIKDgzuNd+xveu1/CBpeZDKZ5HK5uox3NwZjnTlzRs8//7xSUlI0Z86c\nHnv3V/XuPre72yaTqcfPx9+Xm5ursWPHKiEhodM4ffV9Hftz1qxZuvPOOxUWFqYFCxaovr6e/vqw\ntrY2ZWdnKzk5WcXFxSouLtaoUaP06quv0lc/8nd7Sa99F0HDi8LDw9XU1NRp7MqVK7p+/boiIiIM\nmhX+7Pjx45o/f76mTp3qXpDWU++6q3f8RSQiIkIDBw7sUm9qalJgYKBCQ0P/8uv5vvj/O3HihI4d\nO6aMjIwuNfrq+zr25X//1TMqKkqBgYEKCgqivz7q2LFjOnPmjDIzMxUWFqawsDBlZWXpp59+UkhI\nCH31E+Hh4XI4HPrtt986jXfsb35G+x+ChhclJCTIbrfr/Pnz7rHy8nKZzWbFxcUZODN0sNlsWrJk\niZYsWaJnn33WPd5T7xISElRbW6vW1lZ3vaysTJGRkRo6dKjuu+8+VVZWdnqssrIyxcfHq1+/fkpI\nSOhyTmh5ebkSExO9s6F9yL59+9TY2KjJkycrNTVVqampkm5cYWz//v301cdFRUWpf//+qq6udo9d\nvHhRTqdTI0aMoL8+qq2trctfltva2tTe3i6n00lf/URsbKwCAwM77e+6ujo1NzcrMTGR371+iKDh\nRffcc4/uv/9+5ebmqqmpSQ0NDdqwYYMmTZqk/v37Gz29Ps/pdOrtt9/Wc889pwkTJnSq9dS7MWPG\naNCgQcrPz9eVK1d07tw5bd68WWlpaTKZTJowYYJaWlq0efNmtba2ymazadeuXZo+fbok6emnn1Zp\naan2798vh8Oh7777TocOHdK0adOM2BV+ZcGCBdqxY4c+/fRT9z9JWrFihVauXElffVxQUJCmTJmi\nTZs2qbq6Wi0tLVq7dq37mKW/vikpKUlhYWHKz89XS0uLrl69qg8++EDh4eF64okn6KufCA0N1fjx\n47V+/Xr9/PPPstvtysvL0+jRoxUdHc3vXj9kstvtnJzmRb/88otWrlypo0ePKjAwUOPGjdPChQtl\nsViMnlqfd+rUKb344ovq169fl/M2165dq+jo6Jv27ty5c1q1apVKS0sVEhKiJ598UhkZGQoIuJHf\ny3NqXjAAAAD5SURBVMrKtHr1alVXVyssLEwzZ87sdAWkb775Rnl5eaqrq9OQIUP0wgsvaOLEib23\nA/qQUaNGud+wr6djkr7e+tra2rRu3ToVFRXp6tWrSk5O1rJlyzR48GD668Oqq6uVl5enyspKuVwu\nxcXFKTMzUzExMfTVx0ydOlUXLlyQ0+mU0+l0XyVq+fLlSklJ0fvvv69Dhw7J5XJpzJgxev311xUW\nFiap5+dN9Nq3EDQAAAAAeBynTgEAAADwOIIGAAAAAI8jaAAAAADwOIIGAAAAAI8jaAAAAADwOIIG\nAAAAAI8jaAAAAADwOIIGAAAAAI8jaAAAAADwuP8AlNt3Mf0HzjUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "nosoupBSzMZV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://www.kaggleusercontent.com/kf/10364757/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..onsybg1U7xvMy-GjJdYtTA.RUVJoAZ7sxCnRNmM1WTjXMjmsl7raHI_FzIYc0vh0poV3hcKVMrr2GRPCwlr_urWqByjmahePNQlEFEsBBUO2meVZXzX-HjRQuU40ySqElezoc9HLF7nGBMCKPSYt-HPZAsGHnXBtkXV8qVOZlu5BWddQp7wthNJ5JkSftT41GM.vS5mBigypyc7eNri4zC2rQ/__results___files/__results___12_0.png)"
      ]
    },
    {
      "metadata": {
        "id": "pgPdreNUefZl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 4 - Feature Engineering and creating training dataset"
      ]
    },
    {
      "metadata": {
        "id": "kU8LP-sdaNJk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Run this if using 1-D CNN or LSTM"
      ]
    },
    {
      "metadata": {
        "id": "dM6-82ONoYGz",
        "colab_type": "code",
        "outputId": "47077632-f1a6-442c-b3ee-c6fbeaf59053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "cell_type": "code",
      "source": [
        "#Training data for 1D-CNN and LSTM\n",
        "sequence_length=  4095 \n",
        "n_sequences = 40000\n",
        "num_features = 1\n",
        "BATCH_SIZE = 128 # On TPU, this will be the per-core batch size. A Cloud TPU has 8 cores so tha global TPU batch size is 1024\n",
        "\n",
        "X_train = pd.read_csv('train.csv', nrows=sequence_length*n_sequences,usecols =['acoustic_data'], dtype={'acoustic_data': np.int16})#.values.reshape(-1,4095,num_features) #Read values and reshape\n",
        "\n",
        "Y_train = pd.read_csv('train.csv', nrows=sequence_length*n_sequences,usecols =['time_to_failure'], dtype={'time_to_failure': np.float64}).values.reshape(-1,4095,1) #Read values and reshape\n",
        "Y_train = np.float32(Y_train[:,-1,-1]) #We only need the last value at end of sequence\n",
        "print(X_train.shape,Y_train.shape)\n",
        "\n",
        "#Scale input data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_train_scaled = np.float32(X_train_scaled.reshape(-1,4095,num_features))\n",
        "n_samples = X_train_scaled.shape[0]\n",
        "print(X_train_scaled.shape,Y_train.shape)\n",
        "print(X_train_scaled.dtype,Y_train.dtype)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(163800000, 1) (40000,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int16 were all converted to float64 by StandardScaler.\n",
            "  return self.partial_fit(X, y)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: DataConversionWarning: Data with input dtype int16 were all converted to float64 by StandardScaler.\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(40000, 4095, 1) (40000,)\n",
            "float32 float32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZZHiPkitaTf9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Run this if using Densely connected DNN"
      ]
    },
    {
      "metadata": {
        "id": "0SquIAyb5sh9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#lets create a function to generate some statistical features based on the training data\n",
        "def gen_features(X):\n",
        "    strain = []\n",
        "    strain.append(X.mean())\n",
        "    strain.append(X.std())\n",
        "    strain.append(X.min())\n",
        "    strain.append(X.max())\n",
        "    strain.append(X.kurtosis())\n",
        "    strain.append(X.skew())\n",
        "    strain.append(np.quantile(X,0.01))\n",
        "    strain.append(np.quantile(X,0.05))\n",
        "    strain.append(np.quantile(X,0.95))\n",
        "    strain.append(np.quantile(X,0.99))\n",
        "    strain.append(np.abs(X).max())\n",
        "    strain.append(np.abs(X).mean())\n",
        "    strain.append(np.abs(X).std())\n",
        "    return pd.Series(strain)\n",
        "\n",
        "#Training data for DNN\n",
        "train = pd.read_csv('train.csv', iterator=True, chunksize=150000, dtype={'acoustic_data': np.int16})\n",
        "num_statistical_features = 13\n",
        "X_train_features = pd.DataFrame()\n",
        "Y_train_features = pd.Series()\n",
        "for df in train:\n",
        "    features = gen_features(df['acoustic_data'])\n",
        "    X_train_features = X_train_features.append(features, ignore_index=True)\n",
        "    Y_train_features = Y_train_features.append(pd.Series(df['time_to_failure'].values[-1]))\n",
        "#Scale input data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train_features)\n",
        "X_train_scaled = scaler.transform(X_train_features)\n",
        "X_train_scaled = np.float32(X_train_scaled.reshape(-1,num_statistical_features))\n",
        "Y_train = np.float32(Y_train_features.values) \n",
        "n_samples = X_train_scaled.shape[0]\n",
        "print(X_train_scaled.shape,Y_train.shape)\n",
        "print(X_train_scaled.dtype,Y_train.dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TbnovNZm_Os0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### tf.data.Dataset: parse files and prepare training and validation datasets\n",
        "Please read the [best practices for building](https://www.tensorflow.org/guide/performance/datasets) input pipelines with tf.data.Dataset"
      ]
    },
    {
      "metadata": {
        "id": "7PMXExl6_Kmr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_training_dataset(batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X_train_scaled,Y_train.reshape(-1,1)))\n",
        "    dataset = dataset.cache()  # this small dataset can be entirely cached in RAM, for TPU this is important to get good performance from such a small dataset\n",
        "    dataset = dataset.shuffle(1000, reshuffle_each_iteration=True)   # Shuffle, repeat, and batch the examples.\n",
        "    dataset = dataset.repeat() # Mandatory for Keras for now\n",
        "    dataset = dataset.batch(batch_size, drop_remainder=True) # drop_remainder is important on TPU, batch size must be fixed\n",
        "    dataset = dataset.prefetch(-1)  # fetch next batches while training on the current one (-1: autotune prefetch buffer size)\n",
        "    return dataset                    # Return the dataset\n",
        "\n",
        "# instantiate the datasets\n",
        "training_dataset = get_training_dataset(BATCH_SIZE)\n",
        "\n",
        "# For TPU, we will need a function that returns the dataset\n",
        "training_input_fn = lambda: get_training_dataset(BATCH_SIZE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G82q7Z5gcoYf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 5 - Create and train models"
      ]
    },
    {
      "metadata": {
        "id": "ZOPgh3OBYdWW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Create either DNN, CNN or LSTM model"
      ]
    },
    {
      "metadata": {
        "id": "KaedyeUux8pe",
        "colab_type": "code",
        "outputId": "cb6719fc-cb9d-45e8-f3c7-0a629fd9ee19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "cell_type": "code",
      "source": [
        "def create_earthquake_MLP_model():\n",
        "    #Model #1 - MLP\n",
        "    earthquake_MLP_model = Sequential() # Initialising the ANN\n",
        "    earthquake_MLP_model.add(BatchNormalization(input_shape  = (num_statistical_features,)))\n",
        "    earthquake_MLP_model.add(Dense(units = 20, activation = 'relu')) #, input_dim = 13\n",
        "    earthquake_MLP_model.add(BatchNormalization())\n",
        "    earthquake_MLP_model.add(Dense(units = 20, activation = 'relu'))\n",
        "    earthquake_MLP_model.add(Dense(units = 20, activation = 'relu'))\n",
        "    earthquake_MLP_model.add(Dense(units = 20, activation = 'relu'))\n",
        "    earthquake_MLP_model.add(Dense(units = 20, activation = 'relu'))\n",
        "    earthquake_MLP_model.add(Dense(units = 20, activation = 'relu'))\n",
        "\n",
        "    earthquake_MLP_model.add(Dense(units = 10, activation = 'relu'))\n",
        "    earthquake_MLP_model.add(Dense(units = 10, activation = 'relu'))\n",
        "    earthquake_MLP_model.add(Dense(units = 10, activation = 'relu'))\n",
        "    earthquake_MLP_model.add(Dense(units = 10, activation = 'relu'))\n",
        "    earthquake_MLP_model.add(Dense(units = 10, activation = 'relu'))\n",
        "    earthquake_MLP_model.add(Dense(units = 1, activation = 'linear'))\n",
        "    earthquake_MLP_model.compile(optimizer = 'adam', loss = 'mse', metrics = ['mse'])\n",
        "    earthquake_MLP_model.summary()\n",
        "    \n",
        "    return earthquake_MLP_model\n",
        "\n",
        "def create_earthquake_CNN_model():\n",
        "    #Model #2 - 1D CNN\n",
        "    print('Creating CNN model')\n",
        "    earthquake_CNN_model = Sequential()\n",
        "    earthquake_CNN_model.add(Conv1D(100, 10, activation='relu', input_shape=(sequence_length, num_features)))\n",
        "    earthquake_CNN_model.add(Conv1D(100, 10, activation='relu'))\n",
        "    earthquake_CNN_model.add(MaxPooling1D(3))\n",
        "    earthquake_CNN_model.add(Conv1D(100, 10, activation='relu'))\n",
        "    earthquake_CNN_model.add(Conv1D(100, 10, activation='relu'))\n",
        "    earthquake_CNN_model.add(MaxPooling1D(3))\n",
        "    earthquake_CNN_model.add(Conv1D(100, 10, activation='relu'))\n",
        "    earthquake_CNN_model.add(Conv1D(100, 10, activation='relu'))\n",
        "    earthquake_CNN_model.add(MaxPooling1D(3))\n",
        "    earthquake_CNN_model.add(Conv1D(160, 10, activation='relu'))\n",
        "    earthquake_CNN_model.add(Conv1D(160, 10, activation='relu'))\n",
        "    earthquake_CNN_model.add(GlobalAveragePooling1D())\n",
        "    earthquake_CNN_model.add(Dropout(0.5))\n",
        "    earthquake_CNN_model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    earthquake_CNN_model.compile(optimizer = 'adam', loss = 'mse', metrics = ['mse'])\n",
        "    print(earthquake_CNN_model.summary())\n",
        "    return earthquake_CNN_model\n",
        "\n",
        "def create_earthquake_LSTM_model():\n",
        "    #Model #2 - LSTM\n",
        "    print('Creating CNN model')\n",
        "    earthquake_LSTM_model = Sequential()\n",
        "    earthquake_LSTM_model.add(LSTM(40, input_shape=(sequence_length, num_features), return_sequences=True))\n",
        "    earthquake_LSTM_model.add(LSTM(20, return_sequences=False))\n",
        "\n",
        "    earthquake_LSTM_model.add(Dense(100, activation='relu'))\n",
        "    earthquake_LSTM_model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    earthquake_LSTM_model.compile(optimizer = 'adam', loss = 'mse', metrics = ['mse'])\n",
        "    print(earthquake_LSTM_model.summary())\n",
        "    \n",
        "    return earthquake_LSTM_model\n",
        "\n",
        "DECAY= False\n",
        "# set up learning rate decay\n",
        "if DECAY:\n",
        "    lr_decay = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 0.0001 + 0.02 * math.pow(0.5, 1+epoch), verbose=True)\n",
        "else:\n",
        "    lr_decay = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 0.001, verbose=True)\n",
        "\n",
        "#trained_model = create_earthquake_LSTM_model()\n",
        "#trained_model = create_earthquake_CNN_model()\n",
        "trained_model = create_earthquake_MLP_model()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization_v1_4 (Ba (None, 13)                52        \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 20)                280       \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_5 (Ba (None, 20)                80        \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 10)                210       \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 3,173\n",
            "Trainable params: 3,107\n",
            "Non-trainable params: 66\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q4m9rnV7YUXs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Train Model using TPU, GPU or CPU"
      ]
    },
    {
      "metadata": {
        "id": "iGN-tInE0dvI",
        "colab_type": "code",
        "outputId": "e029cce9-a937-4657-a879-22f4185314d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36232
        }
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 500\n",
        "steps_per_epoch = n_samples//BATCH_SIZE  # 60,000 items in this dataset\n",
        "print(f'Iterations per epoch:{steps_per_epoch}')\n",
        "tpu = None\n",
        "\n",
        "\n",
        "# Counting steps and batches on TPU: the tpu.keras_to_tpu_model API regards the batch size of the input dataset\n",
        "# as the per-core batch size. The effective batch size is 8x more because Cloud TPUs have 8 cores. It increments\n",
        "# the step by +8 everytime a global batch (8 per-core batches) is processed. Therefore batch size and steps_per_epoch\n",
        "# settings can stay as they are for TPU training. The training will just go faster.\n",
        "# Warning: this might change in the final version of the Keras/TPU API.\n",
        "\n",
        "try: # TPU detection\n",
        "    tpu = tf.contrib.cluster_resolver.TPUClusterResolver() # Picks up a connected TPU on Google's Colab, ML Engine, Kubernetes and Deep Learning VMs accessed through the 'ctpu up' utility\n",
        "    #tpu = tf.contrib.cluster_resolver.TPUClusterResolver('MY_TPU_NAME') # If auto-detection does not work, you can pass the name of the TPU explicitly (tip: on a VM created with \"ctpu up\" the TPU has the same name as the VM)\n",
        "    \n",
        "except ValueError:\n",
        "    print('Training on GPU/CPU')\n",
        "\n",
        "start_time = time.time()\n",
        "if tpu: # TPU training\n",
        "    strategy = tf.contrib.tpu.TPUDistributionStrategy(tpu)\n",
        "    trained_model = tf.contrib.tpu.keras_to_tpu_model(trained_model, strategy=strategy)\n",
        "    # Work in progress: reading directly from dataset object not yet implemented\n",
        "    # for Keras/TPU. Keras/TPU needs a function that returns a dataset.\n",
        "    print('Training using TPU!')\n",
        "    history = trained_model.fit(training_input_fn, steps_per_epoch=steps_per_epoch, epochs=EPOCHS, callbacks=[lr_decay]) # validation_data=validation_input_fn, validation_steps=1, callbacks=[lr_decay]\n",
        "else: # GPU/CPU training\n",
        "    print('Training using GPU!')\n",
        "    history = trained_model.fit(training_dataset, steps_per_epoch=steps_per_epoch, epochs=EPOCHS)  #,validation_data=validation_dataset, validation_steps=1, callbacks=[lr_decay]\n",
        "\n",
        "print(f'Time elapsed:{(time.time()-start_time)/60.0}')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iterations per epoch:32\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.10.198.58:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 15835482460749807878)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16843138236576594323)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 4788155912491166667)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 7187841300277422876)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 11237785272940479581)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 14937465178827435123)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 917705793928786398)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 13498715306293325498)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 18334010727286554157)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 7976463479729326337)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 3533604713632217905)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "Training using TPU!\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 1/500\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name=None), TensorSpec(shape=(128, 13), dtype=tf.float32, name=None), TensorSpec(shape=(128, 1), dtype=tf.float32, name=None)]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for batch_normalization_v1_4_input\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7fbe11652c88> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 11.240207195281982 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "INFO:tensorflow:CPU -> TPU lr: 0.0010000000474974513 {0.001}\n",
            "INFO:tensorflow:CPU -> TPU beta_1: 0.8999999761581421 {0.9}\n",
            "INFO:tensorflow:CPU -> TPU beta_2: 0.9990000128746033 {0.999}\n",
            "INFO:tensorflow:CPU -> TPU decay: 0.0 {0.0}\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "32/32 [==============================] - 44s 1s/step - loss: 45.2679 - mean_squared_error: 45.2679\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 2/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 40.0708 - mean_squared_error: 40.0709\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 3/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 14.8344 - mean_squared_error: 14.8344\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 4/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 8.7133 - mean_squared_error: 8.7133\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 5/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 8.0156 - mean_squared_error: 8.0156\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 6/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 7.9290 - mean_squared_error: 7.9290\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 7/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.7949 - mean_squared_error: 7.7949\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 8/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.7440 - mean_squared_error: 7.7440\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 9/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.6566 - mean_squared_error: 7.6566\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 10/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.6030 - mean_squared_error: 7.6030\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 11/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.6634 - mean_squared_error: 7.6634\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 12/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.6393 - mean_squared_error: 7.6393\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 13/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.6328 - mean_squared_error: 7.6328\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 14/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.5737 - mean_squared_error: 7.5737\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 15/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 7.5325 - mean_squared_error: 7.5325\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 16/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.5829 - mean_squared_error: 7.5829\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 17/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 7.5600 - mean_squared_error: 7.5600\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 18/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.4574 - mean_squared_error: 7.4574\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 19/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.5245 - mean_squared_error: 7.5245\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 20/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.4842 - mean_squared_error: 7.4842\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 21/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.4857 - mean_squared_error: 7.4857\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 22/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.4861 - mean_squared_error: 7.4861\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 23/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 7.4839 - mean_squared_error: 7.4839\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 24/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.4839 - mean_squared_error: 7.4839\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 25/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.4219 - mean_squared_error: 7.4219\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 26/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.4092 - mean_squared_error: 7.4092\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 27/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.4419 - mean_squared_error: 7.4419\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 28/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.4049 - mean_squared_error: 7.4049\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 29/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.4525 - mean_squared_error: 7.4525\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 30/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.4630 - mean_squared_error: 7.4630\n",
            "\n",
            "Epoch 00031: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 31/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.3177 - mean_squared_error: 7.3177\n",
            "\n",
            "Epoch 00032: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 32/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 7.4306 - mean_squared_error: 7.4306\n",
            "\n",
            "Epoch 00033: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 33/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.4170 - mean_squared_error: 7.4170\n",
            "\n",
            "Epoch 00034: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 34/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.4061 - mean_squared_error: 7.4061\n",
            "\n",
            "Epoch 00035: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 35/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.4214 - mean_squared_error: 7.4214\n",
            "\n",
            "Epoch 00036: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 36/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.3642 - mean_squared_error: 7.3642\n",
            "\n",
            "Epoch 00037: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 37/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.4030 - mean_squared_error: 7.4030\n",
            "\n",
            "Epoch 00038: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 38/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.3601 - mean_squared_error: 7.3601\n",
            "\n",
            "Epoch 00039: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 39/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.4020 - mean_squared_error: 7.4020\n",
            "\n",
            "Epoch 00040: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 40/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.3973 - mean_squared_error: 7.3973\n",
            "\n",
            "Epoch 00041: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 41/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.3542 - mean_squared_error: 7.3542\n",
            "\n",
            "Epoch 00042: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 42/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.3424 - mean_squared_error: 7.3424\n",
            "\n",
            "Epoch 00043: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 43/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.3401 - mean_squared_error: 7.3401\n",
            "\n",
            "Epoch 00044: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 44/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.3606 - mean_squared_error: 7.3606\n",
            "\n",
            "Epoch 00045: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 45/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.3685 - mean_squared_error: 7.3685\n",
            "\n",
            "Epoch 00046: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 46/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.3348 - mean_squared_error: 7.3348\n",
            "\n",
            "Epoch 00047: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 47/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.2563 - mean_squared_error: 7.2563\n",
            "\n",
            "Epoch 00048: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 48/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.3626 - mean_squared_error: 7.3626\n",
            "\n",
            "Epoch 00049: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 49/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.3077 - mean_squared_error: 7.3077\n",
            "\n",
            "Epoch 00050: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 50/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.3070 - mean_squared_error: 7.3070\n",
            "\n",
            "Epoch 00051: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 51/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.3412 - mean_squared_error: 7.3412\n",
            "\n",
            "Epoch 00052: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 52/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.2449 - mean_squared_error: 7.2449\n",
            "\n",
            "Epoch 00053: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 53/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.2987 - mean_squared_error: 7.2987\n",
            "\n",
            "Epoch 00054: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 54/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.2951 - mean_squared_error: 7.2951\n",
            "\n",
            "Epoch 00055: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 55/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.2830 - mean_squared_error: 7.2830\n",
            "\n",
            "Epoch 00056: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 56/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.3149 - mean_squared_error: 7.3149\n",
            "\n",
            "Epoch 00057: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 57/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.2488 - mean_squared_error: 7.2488\n",
            "\n",
            "Epoch 00058: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 58/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.2401 - mean_squared_error: 7.2401\n",
            "\n",
            "Epoch 00059: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 59/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.3186 - mean_squared_error: 7.3186\n",
            "\n",
            "Epoch 00060: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 60/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.2659 - mean_squared_error: 7.2659\n",
            "\n",
            "Epoch 00061: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 61/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.2480 - mean_squared_error: 7.2480\n",
            "\n",
            "Epoch 00062: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 62/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.2669 - mean_squared_error: 7.2669\n",
            "\n",
            "Epoch 00063: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 63/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1840 - mean_squared_error: 7.1840\n",
            "\n",
            "Epoch 00064: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 64/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.2815 - mean_squared_error: 7.2815\n",
            "\n",
            "Epoch 00065: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 65/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.2387 - mean_squared_error: 7.2387\n",
            "\n",
            "Epoch 00066: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 66/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.2675 - mean_squared_error: 7.2675\n",
            "\n",
            "Epoch 00067: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 67/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.2687 - mean_squared_error: 7.2687\n",
            "\n",
            "Epoch 00068: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 68/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1646 - mean_squared_error: 7.1646\n",
            "\n",
            "Epoch 00069: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 69/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.2548 - mean_squared_error: 7.2548\n",
            "\n",
            "Epoch 00070: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 70/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.2325 - mean_squared_error: 7.2325\n",
            "\n",
            "Epoch 00071: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 71/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.2167 - mean_squared_error: 7.2167\n",
            "\n",
            "Epoch 00072: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 72/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.2424 - mean_squared_error: 7.2424\n",
            "\n",
            "Epoch 00073: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 73/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1547 - mean_squared_error: 7.1547\n",
            "\n",
            "Epoch 00074: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 74/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1970 - mean_squared_error: 7.1970\n",
            "\n",
            "Epoch 00075: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 75/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.2012 - mean_squared_error: 7.2012\n",
            "\n",
            "Epoch 00076: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 76/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.2003 - mean_squared_error: 7.2003\n",
            "\n",
            "Epoch 00077: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 77/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.2111 - mean_squared_error: 7.2111\n",
            "\n",
            "Epoch 00078: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 78/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1863 - mean_squared_error: 7.1863\n",
            "\n",
            "Epoch 00079: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 79/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.1643 - mean_squared_error: 7.1643\n",
            "\n",
            "Epoch 00080: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 80/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1726 - mean_squared_error: 7.1726\n",
            "\n",
            "Epoch 00081: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 81/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.2306 - mean_squared_error: 7.2306\n",
            "\n",
            "Epoch 00082: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 82/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1990 - mean_squared_error: 7.1990\n",
            "\n",
            "Epoch 00083: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 83/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1804 - mean_squared_error: 7.1804\n",
            "\n",
            "Epoch 00084: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 84/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1386 - mean_squared_error: 7.1386\n",
            "\n",
            "Epoch 00085: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 85/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.2120 - mean_squared_error: 7.2120\n",
            "\n",
            "Epoch 00086: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 86/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1360 - mean_squared_error: 7.1360\n",
            "\n",
            "Epoch 00087: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 87/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1906 - mean_squared_error: 7.1906\n",
            "\n",
            "Epoch 00088: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 88/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1842 - mean_squared_error: 7.1842\n",
            "\n",
            "Epoch 00089: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 89/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 7.1272 - mean_squared_error: 7.1272\n",
            "\n",
            "Epoch 00090: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 90/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1501 - mean_squared_error: 7.1501\n",
            "\n",
            "Epoch 00091: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 91/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.1505 - mean_squared_error: 7.1505\n",
            "\n",
            "Epoch 00092: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 92/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1554 - mean_squared_error: 7.1554\n",
            "\n",
            "Epoch 00093: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 93/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.1804 - mean_squared_error: 7.1804\n",
            "\n",
            "Epoch 00094: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 94/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.0860 - mean_squared_error: 7.0860\n",
            "\n",
            "Epoch 00095: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 95/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1221 - mean_squared_error: 7.1221\n",
            "\n",
            "Epoch 00096: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 96/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1554 - mean_squared_error: 7.1554\n",
            "\n",
            "Epoch 00097: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 97/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.1192 - mean_squared_error: 7.1192\n",
            "\n",
            "Epoch 00098: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 98/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1305 - mean_squared_error: 7.1305\n",
            "\n",
            "Epoch 00099: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 99/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1100 - mean_squared_error: 7.1100\n",
            "\n",
            "Epoch 00100: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 100/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1022 - mean_squared_error: 7.1022\n",
            "\n",
            "Epoch 00101: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 101/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1350 - mean_squared_error: 7.1350\n",
            "\n",
            "Epoch 00102: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 102/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1321 - mean_squared_error: 7.1321\n",
            "\n",
            "Epoch 00103: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 103/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1285 - mean_squared_error: 7.1285\n",
            "\n",
            "Epoch 00104: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 104/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1442 - mean_squared_error: 7.1442\n",
            "\n",
            "Epoch 00105: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 105/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.0319 - mean_squared_error: 7.0319\n",
            "\n",
            "Epoch 00106: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 106/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.0877 - mean_squared_error: 7.0877\n",
            "\n",
            "Epoch 00107: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 107/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1084 - mean_squared_error: 7.1084\n",
            "\n",
            "Epoch 00108: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 108/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.0741 - mean_squared_error: 7.0741\n",
            "\n",
            "Epoch 00109: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 109/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1221 - mean_squared_error: 7.1221\n",
            "\n",
            "Epoch 00110: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 110/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.0979 - mean_squared_error: 7.0979\n",
            "\n",
            "Epoch 00111: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 111/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.0488 - mean_squared_error: 7.0488\n",
            "\n",
            "Epoch 00112: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 112/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.0992 - mean_squared_error: 7.0992\n",
            "\n",
            "Epoch 00113: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 113/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.0890 - mean_squared_error: 7.0891\n",
            "\n",
            "Epoch 00114: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 114/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.0809 - mean_squared_error: 7.0809\n",
            "\n",
            "Epoch 00115: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 115/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.0666 - mean_squared_error: 7.0666\n",
            "\n",
            "Epoch 00116: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 116/500\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 7.0406 - mean_squared_error: 7.0406\n",
            "\n",
            "Epoch 00117: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 117/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.0804 - mean_squared_error: 7.0804\n",
            "\n",
            "Epoch 00118: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 118/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1052 - mean_squared_error: 7.1052\n",
            "\n",
            "Epoch 00119: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 119/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.0557 - mean_squared_error: 7.0557\n",
            "\n",
            "Epoch 00120: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 120/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.0483 - mean_squared_error: 7.0483\n",
            "\n",
            "Epoch 00121: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 121/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.0198 - mean_squared_error: 7.0198\n",
            "\n",
            "Epoch 00122: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 122/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1123 - mean_squared_error: 7.1123\n",
            "\n",
            "Epoch 00123: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 123/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.0438 - mean_squared_error: 7.0438\n",
            "\n",
            "Epoch 00124: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 124/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.0647 - mean_squared_error: 7.0647\n",
            "\n",
            "Epoch 00125: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 125/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.0945 - mean_squared_error: 7.0945\n",
            "\n",
            "Epoch 00126: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 126/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9895 - mean_squared_error: 6.9895\n",
            "\n",
            "Epoch 00127: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 127/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 7.0318 - mean_squared_error: 7.0318\n",
            "\n",
            "Epoch 00128: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 128/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.0517 - mean_squared_error: 7.0517\n",
            "\n",
            "Epoch 00129: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 129/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.0009 - mean_squared_error: 7.0009\n",
            "\n",
            "Epoch 00130: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 130/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.1102 - mean_squared_error: 7.1102\n",
            "\n",
            "Epoch 00131: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 131/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 7.0097 - mean_squared_error: 7.0097\n",
            "\n",
            "Epoch 00132: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 132/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.0052 - mean_squared_error: 7.0052\n",
            "\n",
            "Epoch 00133: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 133/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.0385 - mean_squared_error: 7.0385\n",
            "\n",
            "Epoch 00134: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 134/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 7.0186 - mean_squared_error: 7.0186\n",
            "\n",
            "Epoch 00135: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 135/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 7.0707 - mean_squared_error: 7.0707\n",
            "\n",
            "Epoch 00136: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 136/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9940 - mean_squared_error: 6.9940\n",
            "\n",
            "Epoch 00137: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 137/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.9710 - mean_squared_error: 6.9710\n",
            "\n",
            "Epoch 00138: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 138/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 7.0837 - mean_squared_error: 7.0837\n",
            "\n",
            "Epoch 00139: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 139/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.9934 - mean_squared_error: 6.9934\n",
            "\n",
            "Epoch 00140: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 140/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 7.0283 - mean_squared_error: 7.0283\n",
            "\n",
            "Epoch 00141: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 141/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.9927 - mean_squared_error: 6.9927\n",
            "\n",
            "Epoch 00142: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 142/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.9817 - mean_squared_error: 6.9817\n",
            "\n",
            "Epoch 00143: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 143/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.9951 - mean_squared_error: 6.9951\n",
            "\n",
            "Epoch 00144: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 144/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 7.0147 - mean_squared_error: 7.0147\n",
            "\n",
            "Epoch 00145: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 145/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.9964 - mean_squared_error: 6.9964\n",
            "\n",
            "Epoch 00146: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 146/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 7.0061 - mean_squared_error: 7.0061\n",
            "\n",
            "Epoch 00147: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 147/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9592 - mean_squared_error: 6.9592\n",
            "\n",
            "Epoch 00148: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 148/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9688 - mean_squared_error: 6.9688\n",
            "\n",
            "Epoch 00149: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 149/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9555 - mean_squared_error: 6.9555\n",
            "\n",
            "Epoch 00150: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 150/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9826 - mean_squared_error: 6.9826\n",
            "\n",
            "Epoch 00151: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 151/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 7.0118 - mean_squared_error: 7.0118\n",
            "\n",
            "Epoch 00152: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 152/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.9841 - mean_squared_error: 6.9841\n",
            "\n",
            "Epoch 00153: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 153/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.9667 - mean_squared_error: 6.9667\n",
            "\n",
            "Epoch 00154: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 154/500\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 7.0232 - mean_squared_error: 7.0232\n",
            "\n",
            "Epoch 00155: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 155/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.9590 - mean_squared_error: 6.9590\n",
            "\n",
            "Epoch 00156: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 156/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.9458 - mean_squared_error: 6.9458\n",
            "\n",
            "Epoch 00157: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 157/500\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 6.9694 - mean_squared_error: 6.9694\n",
            "\n",
            "Epoch 00158: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 158/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.9284 - mean_squared_error: 6.9284\n",
            "\n",
            "Epoch 00159: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 159/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9864 - mean_squared_error: 6.9864\n",
            "\n",
            "Epoch 00160: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 160/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9787 - mean_squared_error: 6.9787\n",
            "\n",
            "Epoch 00161: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 161/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9565 - mean_squared_error: 6.9565\n",
            "\n",
            "Epoch 00162: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 162/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9828 - mean_squared_error: 6.9828\n",
            "\n",
            "Epoch 00163: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 163/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9115 - mean_squared_error: 6.9115\n",
            "\n",
            "Epoch 00164: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 164/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.9259 - mean_squared_error: 6.9259\n",
            "\n",
            "Epoch 00165: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 165/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9303 - mean_squared_error: 6.9303\n",
            "\n",
            "Epoch 00166: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 166/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.9734 - mean_squared_error: 6.9734\n",
            "\n",
            "Epoch 00167: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 167/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7.0157 - mean_squared_error: 7.0157\n",
            "\n",
            "Epoch 00168: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 168/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9321 - mean_squared_error: 6.9321\n",
            "\n",
            "Epoch 00169: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 169/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9143 - mean_squared_error: 6.9143\n",
            "\n",
            "Epoch 00170: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 170/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9468 - mean_squared_error: 6.9468\n",
            "\n",
            "Epoch 00171: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 171/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9514 - mean_squared_error: 6.9514\n",
            "\n",
            "Epoch 00172: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 172/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.9379 - mean_squared_error: 6.9379\n",
            "\n",
            "Epoch 00173: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 173/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9938 - mean_squared_error: 6.9938\n",
            "\n",
            "Epoch 00174: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 174/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9083 - mean_squared_error: 6.9083\n",
            "\n",
            "Epoch 00175: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 175/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9756 - mean_squared_error: 6.9756\n",
            "\n",
            "Epoch 00176: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 176/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9233 - mean_squared_error: 6.9233\n",
            "\n",
            "Epoch 00177: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 177/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.9694 - mean_squared_error: 6.9694\n",
            "\n",
            "Epoch 00178: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 178/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9533 - mean_squared_error: 6.9533\n",
            "\n",
            "Epoch 00179: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 179/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.8787 - mean_squared_error: 6.8787\n",
            "\n",
            "Epoch 00180: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 180/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.9027 - mean_squared_error: 6.9027\n",
            "\n",
            "Epoch 00181: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 181/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9218 - mean_squared_error: 6.9218\n",
            "\n",
            "Epoch 00182: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 182/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.9038 - mean_squared_error: 6.9038\n",
            "\n",
            "Epoch 00183: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 183/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9702 - mean_squared_error: 6.9702\n",
            "\n",
            "Epoch 00184: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 184/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8991 - mean_squared_error: 6.8991\n",
            "\n",
            "Epoch 00185: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 185/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8841 - mean_squared_error: 6.8841\n",
            "\n",
            "Epoch 00186: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 186/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9152 - mean_squared_error: 6.9152\n",
            "\n",
            "Epoch 00187: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 187/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9386 - mean_squared_error: 6.9386\n",
            "\n",
            "Epoch 00188: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 188/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9688 - mean_squared_error: 6.9688\n",
            "\n",
            "Epoch 00189: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 189/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9300 - mean_squared_error: 6.9300\n",
            "\n",
            "Epoch 00190: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 190/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8463 - mean_squared_error: 6.8463\n",
            "\n",
            "Epoch 00191: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 191/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9187 - mean_squared_error: 6.9187\n",
            "\n",
            "Epoch 00192: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 192/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9532 - mean_squared_error: 6.9532\n",
            "\n",
            "Epoch 00193: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 193/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8945 - mean_squared_error: 6.8945\n",
            "\n",
            "Epoch 00194: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 194/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9027 - mean_squared_error: 6.9027\n",
            "\n",
            "Epoch 00195: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 195/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8305 - mean_squared_error: 6.8305\n",
            "\n",
            "Epoch 00196: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 196/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9064 - mean_squared_error: 6.9064\n",
            "\n",
            "Epoch 00197: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 197/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9071 - mean_squared_error: 6.9071\n",
            "\n",
            "Epoch 00198: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 198/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8698 - mean_squared_error: 6.8698\n",
            "\n",
            "Epoch 00199: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 199/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.9191 - mean_squared_error: 6.9191\n",
            "\n",
            "Epoch 00200: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 200/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.8597 - mean_squared_error: 6.8597\n",
            "\n",
            "Epoch 00201: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 201/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8849 - mean_squared_error: 6.8849\n",
            "\n",
            "Epoch 00202: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 202/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8689 - mean_squared_error: 6.8689\n",
            "\n",
            "Epoch 00203: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 203/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8815 - mean_squared_error: 6.8815\n",
            "\n",
            "Epoch 00204: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 204/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.9131 - mean_squared_error: 6.9131\n",
            "\n",
            "Epoch 00205: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 205/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8566 - mean_squared_error: 6.8566\n",
            "\n",
            "Epoch 00206: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 206/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8642 - mean_squared_error: 6.8642\n",
            "\n",
            "Epoch 00207: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 207/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8627 - mean_squared_error: 6.8627\n",
            "\n",
            "Epoch 00208: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 208/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8715 - mean_squared_error: 6.8715\n",
            "\n",
            "Epoch 00209: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 209/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8543 - mean_squared_error: 6.8543\n",
            "\n",
            "Epoch 00210: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 210/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.8557 - mean_squared_error: 6.8557\n",
            "\n",
            "Epoch 00211: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 211/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8262 - mean_squared_error: 6.8262\n",
            "\n",
            "Epoch 00212: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 212/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8667 - mean_squared_error: 6.8667\n",
            "\n",
            "Epoch 00213: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 213/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.8902 - mean_squared_error: 6.8902\n",
            "\n",
            "Epoch 00214: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 214/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8707 - mean_squared_error: 6.8707\n",
            "\n",
            "Epoch 00215: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 215/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8539 - mean_squared_error: 6.8539\n",
            "\n",
            "Epoch 00216: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 216/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.8163 - mean_squared_error: 6.8163\n",
            "\n",
            "Epoch 00217: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 217/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8719 - mean_squared_error: 6.8719\n",
            "\n",
            "Epoch 00218: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 218/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8740 - mean_squared_error: 6.8740\n",
            "\n",
            "Epoch 00219: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 219/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8264 - mean_squared_error: 6.8264\n",
            "\n",
            "Epoch 00220: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 220/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8910 - mean_squared_error: 6.8910\n",
            "\n",
            "Epoch 00221: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 221/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.8595 - mean_squared_error: 6.8595\n",
            "\n",
            "Epoch 00222: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 222/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8189 - mean_squared_error: 6.8189\n",
            "\n",
            "Epoch 00223: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 223/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8705 - mean_squared_error: 6.8705\n",
            "\n",
            "Epoch 00224: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 224/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8774 - mean_squared_error: 6.8774\n",
            "\n",
            "Epoch 00225: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 225/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8853 - mean_squared_error: 6.8853\n",
            "\n",
            "Epoch 00226: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 226/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8298 - mean_squared_error: 6.8298\n",
            "\n",
            "Epoch 00227: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 227/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8325 - mean_squared_error: 6.8325\n",
            "\n",
            "Epoch 00228: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 228/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8507 - mean_squared_error: 6.8507\n",
            "\n",
            "Epoch 00229: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 229/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8146 - mean_squared_error: 6.8146\n",
            "\n",
            "Epoch 00230: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 230/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8647 - mean_squared_error: 6.8647\n",
            "\n",
            "Epoch 00231: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 231/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8539 - mean_squared_error: 6.8539\n",
            "\n",
            "Epoch 00232: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 232/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.7892 - mean_squared_error: 6.7892\n",
            "\n",
            "Epoch 00233: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 233/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.8442 - mean_squared_error: 6.8442\n",
            "\n",
            "Epoch 00234: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 234/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8485 - mean_squared_error: 6.8485\n",
            "\n",
            "Epoch 00235: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 235/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8162 - mean_squared_error: 6.8162\n",
            "\n",
            "Epoch 00236: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 236/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.8737 - mean_squared_error: 6.8737\n",
            "\n",
            "Epoch 00237: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 237/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8363 - mean_squared_error: 6.8363\n",
            "\n",
            "Epoch 00238: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 238/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8173 - mean_squared_error: 6.8173\n",
            "\n",
            "Epoch 00239: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 239/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7910 - mean_squared_error: 6.7910\n",
            "\n",
            "Epoch 00240: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 240/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8388 - mean_squared_error: 6.8388\n",
            "\n",
            "Epoch 00241: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 241/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8373 - mean_squared_error: 6.8373\n",
            "\n",
            "Epoch 00242: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 242/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8081 - mean_squared_error: 6.8081\n",
            "\n",
            "Epoch 00243: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 243/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7408 - mean_squared_error: 6.7408\n",
            "\n",
            "Epoch 00244: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 244/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8165 - mean_squared_error: 6.8165\n",
            "\n",
            "Epoch 00245: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 245/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.8013 - mean_squared_error: 6.8013\n",
            "\n",
            "Epoch 00246: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 246/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.8325 - mean_squared_error: 6.8325\n",
            "\n",
            "Epoch 00247: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 247/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7824 - mean_squared_error: 6.7824\n",
            "\n",
            "Epoch 00248: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 248/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.7543 - mean_squared_error: 6.7543\n",
            "\n",
            "Epoch 00249: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 249/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8144 - mean_squared_error: 6.8144\n",
            "\n",
            "Epoch 00250: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 250/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8189 - mean_squared_error: 6.8189\n",
            "\n",
            "Epoch 00251: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 251/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.8173 - mean_squared_error: 6.8173\n",
            "\n",
            "Epoch 00252: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 252/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8336 - mean_squared_error: 6.8336\n",
            "\n",
            "Epoch 00253: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 253/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.7547 - mean_squared_error: 6.7547\n",
            "\n",
            "Epoch 00254: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 254/500\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 6.7998 - mean_squared_error: 6.7998\n",
            "\n",
            "Epoch 00255: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 255/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.8095 - mean_squared_error: 6.8095\n",
            "\n",
            "Epoch 00256: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 256/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.7530 - mean_squared_error: 6.7530\n",
            "\n",
            "Epoch 00257: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 257/500\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 6.8003 - mean_squared_error: 6.8003\n",
            "\n",
            "Epoch 00258: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 258/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.7824 - mean_squared_error: 6.7824\n",
            "\n",
            "Epoch 00259: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 259/500\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 6.7556 - mean_squared_error: 6.7556\n",
            "\n",
            "Epoch 00260: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 260/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8104 - mean_squared_error: 6.8104\n",
            "\n",
            "Epoch 00261: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 261/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7829 - mean_squared_error: 6.7829\n",
            "\n",
            "Epoch 00262: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 262/500\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 6.7731 - mean_squared_error: 6.7731\n",
            "\n",
            "Epoch 00263: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 263/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7952 - mean_squared_error: 6.7952\n",
            "\n",
            "Epoch 00264: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 264/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.6995 - mean_squared_error: 6.6995\n",
            "\n",
            "Epoch 00265: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 265/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.8089 - mean_squared_error: 6.8089\n",
            "\n",
            "Epoch 00266: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 266/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7585 - mean_squared_error: 6.7585\n",
            "\n",
            "Epoch 00267: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 267/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7569 - mean_squared_error: 6.7569\n",
            "\n",
            "Epoch 00268: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 268/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7991 - mean_squared_error: 6.7991\n",
            "\n",
            "Epoch 00269: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 269/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7454 - mean_squared_error: 6.7454\n",
            "\n",
            "Epoch 00270: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 270/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7786 - mean_squared_error: 6.7786\n",
            "\n",
            "Epoch 00271: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 271/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7706 - mean_squared_error: 6.7706\n",
            "\n",
            "Epoch 00272: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 272/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7496 - mean_squared_error: 6.7496\n",
            "\n",
            "Epoch 00273: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 273/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7853 - mean_squared_error: 6.7853\n",
            "\n",
            "Epoch 00274: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 274/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7587 - mean_squared_error: 6.7587\n",
            "\n",
            "Epoch 00275: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 275/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.7637 - mean_squared_error: 6.7637\n",
            "\n",
            "Epoch 00276: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 276/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.7555 - mean_squared_error: 6.7555\n",
            "\n",
            "Epoch 00277: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 277/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.7131 - mean_squared_error: 6.7131\n",
            "\n",
            "Epoch 00278: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 278/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.7955 - mean_squared_error: 6.7955\n",
            "\n",
            "Epoch 00279: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 279/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.7594 - mean_squared_error: 6.7594\n",
            "\n",
            "Epoch 00280: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 280/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7054 - mean_squared_error: 6.7054\n",
            "\n",
            "Epoch 00281: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 281/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.7938 - mean_squared_error: 6.7938\n",
            "\n",
            "Epoch 00282: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 282/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.7221 - mean_squared_error: 6.7221\n",
            "\n",
            "Epoch 00283: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 283/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.7377 - mean_squared_error: 6.7377\n",
            "\n",
            "Epoch 00284: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 284/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7699 - mean_squared_error: 6.7699\n",
            "\n",
            "Epoch 00285: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 285/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.6874 - mean_squared_error: 6.6874\n",
            "\n",
            "Epoch 00286: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 286/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.7341 - mean_squared_error: 6.7341\n",
            "\n",
            "Epoch 00287: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 287/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7728 - mean_squared_error: 6.7728\n",
            "\n",
            "Epoch 00288: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 288/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.7237 - mean_squared_error: 6.7237\n",
            "\n",
            "Epoch 00289: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 289/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.7312 - mean_squared_error: 6.7312\n",
            "\n",
            "Epoch 00290: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 290/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7560 - mean_squared_error: 6.7560\n",
            "\n",
            "Epoch 00291: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 291/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6913 - mean_squared_error: 6.6913\n",
            "\n",
            "Epoch 00292: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 292/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.7883 - mean_squared_error: 6.7883\n",
            "\n",
            "Epoch 00293: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 293/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7167 - mean_squared_error: 6.7167\n",
            "\n",
            "Epoch 00294: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 294/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.7690 - mean_squared_error: 6.7690\n",
            "\n",
            "Epoch 00295: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 295/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.7257 - mean_squared_error: 6.7257\n",
            "\n",
            "Epoch 00296: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 296/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6892 - mean_squared_error: 6.6892\n",
            "\n",
            "Epoch 00297: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 297/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7009 - mean_squared_error: 6.7009\n",
            "\n",
            "Epoch 00298: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 298/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7095 - mean_squared_error: 6.7095\n",
            "\n",
            "Epoch 00299: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 299/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7163 - mean_squared_error: 6.7163\n",
            "\n",
            "Epoch 00300: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 300/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7694 - mean_squared_error: 6.7695\n",
            "\n",
            "Epoch 00301: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 301/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.6124 - mean_squared_error: 6.6124\n",
            "\n",
            "Epoch 00302: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 302/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.7675 - mean_squared_error: 6.7675\n",
            "\n",
            "Epoch 00303: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 303/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.7078 - mean_squared_error: 6.7078\n",
            "\n",
            "Epoch 00304: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 304/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.6818 - mean_squared_error: 6.6818\n",
            "\n",
            "Epoch 00305: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 305/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.6829 - mean_squared_error: 6.6829\n",
            "\n",
            "Epoch 00306: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 306/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6476 - mean_squared_error: 6.6476\n",
            "\n",
            "Epoch 00307: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 307/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7020 - mean_squared_error: 6.7020\n",
            "\n",
            "Epoch 00308: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 308/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6818 - mean_squared_error: 6.6818\n",
            "\n",
            "Epoch 00309: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 309/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6944 - mean_squared_error: 6.6944\n",
            "\n",
            "Epoch 00310: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 310/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7155 - mean_squared_error: 6.7155\n",
            "\n",
            "Epoch 00311: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 311/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.6514 - mean_squared_error: 6.6514\n",
            "\n",
            "Epoch 00312: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 312/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6675 - mean_squared_error: 6.6675\n",
            "\n",
            "Epoch 00313: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 313/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7353 - mean_squared_error: 6.7353\n",
            "\n",
            "Epoch 00314: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 314/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7467 - mean_squared_error: 6.7467\n",
            "\n",
            "Epoch 00315: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 315/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7080 - mean_squared_error: 6.7080\n",
            "\n",
            "Epoch 00316: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 316/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7068 - mean_squared_error: 6.7068\n",
            "\n",
            "Epoch 00317: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 317/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6705 - mean_squared_error: 6.6705\n",
            "\n",
            "Epoch 00318: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 318/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.7023 - mean_squared_error: 6.7023\n",
            "\n",
            "Epoch 00319: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 319/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6838 - mean_squared_error: 6.6838\n",
            "\n",
            "Epoch 00320: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 320/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6801 - mean_squared_error: 6.6801\n",
            "\n",
            "Epoch 00321: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 321/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7218 - mean_squared_error: 6.7218\n",
            "\n",
            "Epoch 00322: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 322/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6439 - mean_squared_error: 6.6439\n",
            "\n",
            "Epoch 00323: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 323/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6323 - mean_squared_error: 6.6323\n",
            "\n",
            "Epoch 00324: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 324/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6253 - mean_squared_error: 6.6253\n",
            "\n",
            "Epoch 00325: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 325/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.7183 - mean_squared_error: 6.7183\n",
            "\n",
            "Epoch 00326: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 326/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6876 - mean_squared_error: 6.6876\n",
            "\n",
            "Epoch 00327: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 327/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6807 - mean_squared_error: 6.6807\n",
            "\n",
            "Epoch 00328: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 328/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6145 - mean_squared_error: 6.6145\n",
            "\n",
            "Epoch 00329: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 329/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6403 - mean_squared_error: 6.6403\n",
            "\n",
            "Epoch 00330: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 330/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.6623 - mean_squared_error: 6.6623\n",
            "\n",
            "Epoch 00331: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 331/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6942 - mean_squared_error: 6.6942\n",
            "\n",
            "Epoch 00332: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 332/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.6368 - mean_squared_error: 6.6368\n",
            "\n",
            "Epoch 00333: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 333/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.6181 - mean_squared_error: 6.6181\n",
            "\n",
            "Epoch 00334: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 334/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6843 - mean_squared_error: 6.6843\n",
            "\n",
            "Epoch 00335: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 335/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6768 - mean_squared_error: 6.6768\n",
            "\n",
            "Epoch 00336: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 336/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6564 - mean_squared_error: 6.6564\n",
            "\n",
            "Epoch 00337: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 337/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6584 - mean_squared_error: 6.6584\n",
            "\n",
            "Epoch 00338: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 338/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5836 - mean_squared_error: 6.5836\n",
            "\n",
            "Epoch 00339: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 339/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6621 - mean_squared_error: 6.6621\n",
            "\n",
            "Epoch 00340: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 340/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6326 - mean_squared_error: 6.6326\n",
            "\n",
            "Epoch 00341: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 341/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6343 - mean_squared_error: 6.6343\n",
            "\n",
            "Epoch 00342: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 342/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6549 - mean_squared_error: 6.6549\n",
            "\n",
            "Epoch 00343: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 343/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6357 - mean_squared_error: 6.6357\n",
            "\n",
            "Epoch 00344: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 344/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6570 - mean_squared_error: 6.6570\n",
            "\n",
            "Epoch 00345: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 345/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6598 - mean_squared_error: 6.6598\n",
            "\n",
            "Epoch 00346: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 346/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6425 - mean_squared_error: 6.6425\n",
            "\n",
            "Epoch 00347: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 347/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6534 - mean_squared_error: 6.6534\n",
            "\n",
            "Epoch 00348: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 348/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6426 - mean_squared_error: 6.6426\n",
            "\n",
            "Epoch 00349: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 349/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6323 - mean_squared_error: 6.6323\n",
            "\n",
            "Epoch 00350: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 350/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6274 - mean_squared_error: 6.6274\n",
            "\n",
            "Epoch 00351: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 351/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6498 - mean_squared_error: 6.6498\n",
            "\n",
            "Epoch 00352: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 352/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6192 - mean_squared_error: 6.6192\n",
            "\n",
            "Epoch 00353: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 353/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6190 - mean_squared_error: 6.6190\n",
            "\n",
            "Epoch 00354: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 354/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5878 - mean_squared_error: 6.5878\n",
            "\n",
            "Epoch 00355: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 355/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6845 - mean_squared_error: 6.6845\n",
            "\n",
            "Epoch 00356: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 356/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6755 - mean_squared_error: 6.6755\n",
            "\n",
            "Epoch 00357: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 357/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6472 - mean_squared_error: 6.6472\n",
            "\n",
            "Epoch 00358: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 358/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.6134 - mean_squared_error: 6.6134\n",
            "\n",
            "Epoch 00359: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 359/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.6052 - mean_squared_error: 6.6052\n",
            "\n",
            "Epoch 00360: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 360/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.6527 - mean_squared_error: 6.6527\n",
            "\n",
            "Epoch 00361: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 361/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6562 - mean_squared_error: 6.6562\n",
            "\n",
            "Epoch 00362: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 362/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.6038 - mean_squared_error: 6.6038\n",
            "\n",
            "Epoch 00363: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 363/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6578 - mean_squared_error: 6.6578\n",
            "\n",
            "Epoch 00364: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 364/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5804 - mean_squared_error: 6.5804\n",
            "\n",
            "Epoch 00365: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 365/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.5747 - mean_squared_error: 6.5747\n",
            "\n",
            "Epoch 00366: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 366/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6258 - mean_squared_error: 6.6258\n",
            "\n",
            "Epoch 00367: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 367/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6321 - mean_squared_error: 6.6321\n",
            "\n",
            "Epoch 00368: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 368/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6238 - mean_squared_error: 6.6238\n",
            "\n",
            "Epoch 00369: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 369/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5938 - mean_squared_error: 6.5938\n",
            "\n",
            "Epoch 00370: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 370/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5908 - mean_squared_error: 6.5908\n",
            "\n",
            "Epoch 00371: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 371/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6026 - mean_squared_error: 6.6026\n",
            "\n",
            "Epoch 00372: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 372/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6406 - mean_squared_error: 6.6406\n",
            "\n",
            "Epoch 00373: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 373/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5881 - mean_squared_error: 6.5881\n",
            "\n",
            "Epoch 00374: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 374/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6286 - mean_squared_error: 6.6286\n",
            "\n",
            "Epoch 00375: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 375/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5870 - mean_squared_error: 6.5870\n",
            "\n",
            "Epoch 00376: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 376/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6055 - mean_squared_error: 6.6055\n",
            "\n",
            "Epoch 00377: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 377/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.6420 - mean_squared_error: 6.6420\n",
            "\n",
            "Epoch 00378: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 378/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.6464 - mean_squared_error: 6.6464\n",
            "\n",
            "Epoch 00379: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 379/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6443 - mean_squared_error: 6.6443\n",
            "\n",
            "Epoch 00380: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 380/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5723 - mean_squared_error: 6.5723\n",
            "\n",
            "Epoch 00381: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 381/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5131 - mean_squared_error: 6.5131\n",
            "\n",
            "Epoch 00382: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 382/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6448 - mean_squared_error: 6.6448\n",
            "\n",
            "Epoch 00383: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 383/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.5424 - mean_squared_error: 6.5424\n",
            "\n",
            "Epoch 00384: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 384/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6358 - mean_squared_error: 6.6358\n",
            "\n",
            "Epoch 00385: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 385/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.5832 - mean_squared_error: 6.5832\n",
            "\n",
            "Epoch 00386: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 386/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5429 - mean_squared_error: 6.5429\n",
            "\n",
            "Epoch 00387: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 387/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.6314 - mean_squared_error: 6.6314\n",
            "\n",
            "Epoch 00388: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 388/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.5804 - mean_squared_error: 6.5804\n",
            "\n",
            "Epoch 00389: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 389/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5654 - mean_squared_error: 6.5654\n",
            "\n",
            "Epoch 00390: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 390/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6290 - mean_squared_error: 6.6290\n",
            "\n",
            "Epoch 00391: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 391/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.5300 - mean_squared_error: 6.5300\n",
            "\n",
            "Epoch 00392: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 392/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5871 - mean_squared_error: 6.5871\n",
            "\n",
            "Epoch 00393: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 393/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5741 - mean_squared_error: 6.5741\n",
            "\n",
            "Epoch 00394: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 394/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5418 - mean_squared_error: 6.5418\n",
            "\n",
            "Epoch 00395: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 395/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6070 - mean_squared_error: 6.6070\n",
            "\n",
            "Epoch 00396: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 396/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5190 - mean_squared_error: 6.5190\n",
            "\n",
            "Epoch 00397: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 397/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5681 - mean_squared_error: 6.5681\n",
            "\n",
            "Epoch 00398: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 398/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.6056 - mean_squared_error: 6.6056\n",
            "\n",
            "Epoch 00399: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 399/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5538 - mean_squared_error: 6.5538\n",
            "\n",
            "Epoch 00400: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 400/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5899 - mean_squared_error: 6.5899\n",
            "\n",
            "Epoch 00401: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 401/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5454 - mean_squared_error: 6.5454\n",
            "\n",
            "Epoch 00402: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 402/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4883 - mean_squared_error: 6.4883\n",
            "\n",
            "Epoch 00403: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 403/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.5627 - mean_squared_error: 6.5627\n",
            "\n",
            "Epoch 00404: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 404/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5851 - mean_squared_error: 6.5851\n",
            "\n",
            "Epoch 00405: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 405/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5091 - mean_squared_error: 6.5091\n",
            "\n",
            "Epoch 00406: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 406/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.5410 - mean_squared_error: 6.5410\n",
            "\n",
            "Epoch 00407: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 407/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.5071 - mean_squared_error: 6.5071\n",
            "\n",
            "Epoch 00408: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 408/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5686 - mean_squared_error: 6.5686\n",
            "\n",
            "Epoch 00409: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 409/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5875 - mean_squared_error: 6.5875\n",
            "\n",
            "Epoch 00410: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 410/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5397 - mean_squared_error: 6.5397\n",
            "\n",
            "Epoch 00411: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 411/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5545 - mean_squared_error: 6.5545\n",
            "\n",
            "Epoch 00412: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 412/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4437 - mean_squared_error: 6.4437\n",
            "\n",
            "Epoch 00413: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 413/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.5213 - mean_squared_error: 6.5213\n",
            "\n",
            "Epoch 00414: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 414/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.5280 - mean_squared_error: 6.5280\n",
            "\n",
            "Epoch 00415: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 415/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.5439 - mean_squared_error: 6.5439\n",
            "\n",
            "Epoch 00416: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 416/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5577 - mean_squared_error: 6.5577\n",
            "\n",
            "Epoch 00417: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 417/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5194 - mean_squared_error: 6.5194\n",
            "\n",
            "Epoch 00418: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 418/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.5154 - mean_squared_error: 6.5154\n",
            "\n",
            "Epoch 00419: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 419/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5586 - mean_squared_error: 6.5586\n",
            "\n",
            "Epoch 00420: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 420/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5367 - mean_squared_error: 6.5367\n",
            "\n",
            "Epoch 00421: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 421/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4944 - mean_squared_error: 6.4944\n",
            "\n",
            "Epoch 00422: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 422/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5042 - mean_squared_error: 6.5042\n",
            "\n",
            "Epoch 00423: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 423/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4714 - mean_squared_error: 6.4714\n",
            "\n",
            "Epoch 00424: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 424/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.5980 - mean_squared_error: 6.5980\n",
            "\n",
            "Epoch 00425: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 425/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5107 - mean_squared_error: 6.5107\n",
            "\n",
            "Epoch 00426: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 426/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5375 - mean_squared_error: 6.5375\n",
            "\n",
            "Epoch 00427: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 427/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4949 - mean_squared_error: 6.4949\n",
            "\n",
            "Epoch 00428: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 428/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.5640 - mean_squared_error: 6.5640\n",
            "\n",
            "Epoch 00429: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 429/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5020 - mean_squared_error: 6.5020\n",
            "\n",
            "Epoch 00430: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 430/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.4949 - mean_squared_error: 6.4949\n",
            "\n",
            "Epoch 00431: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 431/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5430 - mean_squared_error: 6.5430\n",
            "\n",
            "Epoch 00432: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 432/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5030 - mean_squared_error: 6.5030\n",
            "\n",
            "Epoch 00433: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 433/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.4745 - mean_squared_error: 6.4745\n",
            "\n",
            "Epoch 00434: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 434/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.4866 - mean_squared_error: 6.4866\n",
            "\n",
            "Epoch 00435: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 435/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5229 - mean_squared_error: 6.5229\n",
            "\n",
            "Epoch 00436: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 436/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5189 - mean_squared_error: 6.5189\n",
            "\n",
            "Epoch 00437: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 437/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4849 - mean_squared_error: 6.4848\n",
            "\n",
            "Epoch 00438: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 438/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.4556 - mean_squared_error: 6.4556\n",
            "\n",
            "Epoch 00439: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 439/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4287 - mean_squared_error: 6.4287\n",
            "\n",
            "Epoch 00440: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 440/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5165 - mean_squared_error: 6.5165\n",
            "\n",
            "Epoch 00441: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 441/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4906 - mean_squared_error: 6.4906\n",
            "\n",
            "Epoch 00442: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 442/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5139 - mean_squared_error: 6.5139\n",
            "\n",
            "Epoch 00443: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 443/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5494 - mean_squared_error: 6.5494\n",
            "\n",
            "Epoch 00444: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 444/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4672 - mean_squared_error: 6.4672\n",
            "\n",
            "Epoch 00445: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 445/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5498 - mean_squared_error: 6.5498\n",
            "\n",
            "Epoch 00446: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 446/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5133 - mean_squared_error: 6.5133\n",
            "\n",
            "Epoch 00447: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 447/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4624 - mean_squared_error: 6.4624\n",
            "\n",
            "Epoch 00448: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 448/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4917 - mean_squared_error: 6.4917\n",
            "\n",
            "Epoch 00449: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 449/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4894 - mean_squared_error: 6.4894\n",
            "\n",
            "Epoch 00450: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 450/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.4888 - mean_squared_error: 6.4888\n",
            "\n",
            "Epoch 00451: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 451/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5059 - mean_squared_error: 6.5059\n",
            "\n",
            "Epoch 00452: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 452/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.4824 - mean_squared_error: 6.4824\n",
            "\n",
            "Epoch 00453: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 453/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4662 - mean_squared_error: 6.4662\n",
            "\n",
            "Epoch 00454: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 454/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4741 - mean_squared_error: 6.4741\n",
            "\n",
            "Epoch 00455: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 455/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4708 - mean_squared_error: 6.4708\n",
            "\n",
            "Epoch 00456: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 456/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4734 - mean_squared_error: 6.4734\n",
            "\n",
            "Epoch 00457: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 457/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4749 - mean_squared_error: 6.4749\n",
            "\n",
            "Epoch 00458: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 458/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4454 - mean_squared_error: 6.4454\n",
            "\n",
            "Epoch 00459: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 459/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4680 - mean_squared_error: 6.4680\n",
            "\n",
            "Epoch 00460: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 460/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4007 - mean_squared_error: 6.4007\n",
            "\n",
            "Epoch 00461: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 461/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.5209 - mean_squared_error: 6.5209\n",
            "\n",
            "Epoch 00462: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 462/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4682 - mean_squared_error: 6.4682\n",
            "\n",
            "Epoch 00463: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 463/500\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 6.4387 - mean_squared_error: 6.4387\n",
            "\n",
            "Epoch 00464: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 464/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4506 - mean_squared_error: 6.4506\n",
            "\n",
            "Epoch 00465: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 465/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4401 - mean_squared_error: 6.4401\n",
            "\n",
            "Epoch 00466: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 466/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.4441 - mean_squared_error: 6.4441\n",
            "\n",
            "Epoch 00467: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 467/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4318 - mean_squared_error: 6.4318\n",
            "\n",
            "Epoch 00468: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 468/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4392 - mean_squared_error: 6.4392\n",
            "\n",
            "Epoch 00469: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 469/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4783 - mean_squared_error: 6.4783\n",
            "\n",
            "Epoch 00470: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 470/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4421 - mean_squared_error: 6.4421\n",
            "\n",
            "Epoch 00471: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 471/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5053 - mean_squared_error: 6.5053\n",
            "\n",
            "Epoch 00472: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 472/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4659 - mean_squared_error: 6.4659\n",
            "\n",
            "Epoch 00473: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 473/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.5210 - mean_squared_error: 6.5210\n",
            "\n",
            "Epoch 00474: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 474/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4470 - mean_squared_error: 6.4470\n",
            "\n",
            "Epoch 00475: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 475/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4928 - mean_squared_error: 6.4928\n",
            "\n",
            "Epoch 00476: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 476/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4770 - mean_squared_error: 6.4770\n",
            "\n",
            "Epoch 00477: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 477/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4565 - mean_squared_error: 6.4565\n",
            "\n",
            "Epoch 00478: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 478/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.4471 - mean_squared_error: 6.4471\n",
            "\n",
            "Epoch 00479: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 479/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4832 - mean_squared_error: 6.4832\n",
            "\n",
            "Epoch 00480: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 480/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4813 - mean_squared_error: 6.4813\n",
            "\n",
            "Epoch 00481: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 481/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.4022 - mean_squared_error: 6.4022\n",
            "\n",
            "Epoch 00482: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 482/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4473 - mean_squared_error: 6.4473\n",
            "\n",
            "Epoch 00483: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 483/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4650 - mean_squared_error: 6.4650\n",
            "\n",
            "Epoch 00484: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 484/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4534 - mean_squared_error: 6.4534\n",
            "\n",
            "Epoch 00485: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 485/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4692 - mean_squared_error: 6.4692\n",
            "\n",
            "Epoch 00486: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 486/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4268 - mean_squared_error: 6.4268\n",
            "\n",
            "Epoch 00487: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 487/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4644 - mean_squared_error: 6.4644\n",
            "\n",
            "Epoch 00488: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 488/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4449 - mean_squared_error: 6.4449\n",
            "\n",
            "Epoch 00489: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 489/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4568 - mean_squared_error: 6.4568\n",
            "\n",
            "Epoch 00490: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 490/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4766 - mean_squared_error: 6.4766\n",
            "\n",
            "Epoch 00491: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 491/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4369 - mean_squared_error: 6.4369\n",
            "\n",
            "Epoch 00492: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 492/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.3815 - mean_squared_error: 6.3815\n",
            "\n",
            "Epoch 00493: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 493/500\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 6.4064 - mean_squared_error: 6.4064\n",
            "\n",
            "Epoch 00494: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 494/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4322 - mean_squared_error: 6.4322\n",
            "\n",
            "Epoch 00495: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 495/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.3953 - mean_squared_error: 6.3953\n",
            "\n",
            "Epoch 00496: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 496/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4332 - mean_squared_error: 6.4332\n",
            "\n",
            "Epoch 00497: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 497/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.3561 - mean_squared_error: 6.3561\n",
            "\n",
            "Epoch 00498: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 498/500\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 6.4781 - mean_squared_error: 6.4781\n",
            "\n",
            "Epoch 00499: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 499/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4271 - mean_squared_error: 6.4271\n",
            "\n",
            "Epoch 00500: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 500/500\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6.4415 - mean_squared_error: 6.4415\n",
            "Time elapsed:4.201942658424377\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "41Jk9Qwwakmw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Training history"
      ]
    },
    {
      "metadata": {
        "id": "GbB898bDNbl_",
        "colab_type": "code",
        "outputId": "6c9b12f1-7db5-4883-b228-6ac7b3ff3e42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "cell_type": "code",
      "source": [
        "print(history.history.keys())\n",
        "display_training_curves(history.history['loss'], history.history['mean_squared_error'], 'loss', 212)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'mean_squared_error', 'lr'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfEAAADKCAYAAAC1+HmnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}